{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.decomposition import FactorAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/joe/MLP/features_sample_head0.csv\")\n",
    "df = df.astype({'Class_P0': 'bool', \n",
    "                'head1_AU28': 'bool',\n",
    "                'head1_AU45': 'bool',\n",
    "                'head1_AU26': 'bool',\n",
    "                'head1_AU25': 'bool',\n",
    "                'head1_AU01': 'bool',\n",
    "                'head1_AU05': 'bool',\n",
    "                'head1_AU20': 'bool',\n",
    "                'head1_AU06': 'bool',\n",
    "                'head1_AU23': 'bool',\n",
    "                'head1_AU07': 'bool',\n",
    "                'head1_AU02': 'bool', \n",
    "                'head1_AU09': 'bool',\n",
    "                'head1_AU17': 'bool',\n",
    "                'head1_AU10': 'bool',\n",
    "                'head1_AU04': 'bool',\n",
    "                'head1_AU12': 'bool',\n",
    "                'head1_AU14': 'bool',\n",
    "                'head1_AU15': 'bool'})\n",
    "le = LabelEncoder()\n",
    "# df['head0_pose'] = le.fit_transform(df['head0_pose'])\n",
    "df['head1_pose'] = le.fit_transform(df['head1_pose'])\n",
    "# df['head2_pose'] = le.fit_transform(df['head2_pose'])\n",
    "# df['head0_emo'] = le.fit_transform(df['head0_emo'])\n",
    "df['head1_emo'] = le.fit_transform(df['head1_emo'])\n",
    "# df['head2_emo'] = le.fit_transform(df['head2_emo'])\n",
    "\n",
    "y = df['Class_P0']\n",
    "x = df.drop(['Class_P0'], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size= 0.25, random_state=27)\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imp = imp.fit(x_train)\n",
    "\n",
    "X_train_imp = imp.transform(x_train)\n",
    "X_test_imp = imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "head1_head0_trans_x    float64\n",
       "head1_head0_trans_y    float64\n",
       "head1_head0_trans_z    float64\n",
       "head1_head0_ori_x      float64\n",
       "head1_head0_ori_y      float64\n",
       "head1_head0_ori_z      float64\n",
       "head1_head0_ori_w      float64\n",
       "head1_head1_trans_x    float64\n",
       "head1_head1_trans_y    float64\n",
       "head1_head1_trans_z    float64\n",
       "head1_head1_ori_x      float64\n",
       "head1_head1_ori_y      float64\n",
       "head1_head1_ori_z      float64\n",
       "head1_head1_ori_w      float64\n",
       "head1_head2_trans_x    float64\n",
       "head1_head2_trans_y    float64\n",
       "head1_head2_trans_z    float64\n",
       "head1_head2_ori_x      float64\n",
       "head1_head2_ori_y      float64\n",
       "head1_head2_ori_z      float64\n",
       "head1_head2_ori_w      float64\n",
       "head1_emo                int64\n",
       "head1_AU28                bool\n",
       "head1_AU45                bool\n",
       "head1_AU26                bool\n",
       "head1_AU25                bool\n",
       "head1_AU01                bool\n",
       "head1_AU05                bool\n",
       "head1_AU20                bool\n",
       "head1_AU06                bool\n",
       "head1_AU23                bool\n",
       "head1_AU07                bool\n",
       "head1_AU02                bool\n",
       "head1_AU09                bool\n",
       "head1_AU17                bool\n",
       "head1_AU10                bool\n",
       "head1_AU04                bool\n",
       "head1_AU12                bool\n",
       "head1_AU14                bool\n",
       "head1_AU15                bool\n",
       "head1_pose               int64\n",
       "ClassP1                   bool\n",
       "Class_P0                  bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=300, alpha=0.0001, activation='tanh',\n",
    "                     solver='adam', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection (Skip as it Takes time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfsl = sfs(clf, k_features=10,forward=True)\n",
    "# print(sfsl.fit(X_test_imp, y_test))\n",
    "# print(sfsl.subsets_)\n",
    "# #'feature_names': head1_head0_trans_x\thead1_head0_trans_y\thead1_head0_ori_w\thead1_head1_trans_x\thead1_head1_ori_x\thead1_head1_ori_y\thead1_head2_trans_z\thead1_AU07\thead1_AU09\thead1_AU12\tClassP1\tClass_P0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factore analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.229251</td>\n",
       "      <td>0.038701</td>\n",
       "      <td>-0.046333</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.050264</td>\n",
       "      <td>0.010320</td>\n",
       "      <td>-0.002822</td>\n",
       "      <td>-0.191301</td>\n",
       "      <td>0.034447</td>\n",
       "      <td>0.024862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117154</td>\n",
       "      <td>0.347586</td>\n",
       "      <td>0.280505</td>\n",
       "      <td>0.250070</td>\n",
       "      <td>0.290424</td>\n",
       "      <td>0.214873</td>\n",
       "      <td>0.229300</td>\n",
       "      <td>0.118473</td>\n",
       "      <td>0.130554</td>\n",
       "      <td>-0.024460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.415345</td>\n",
       "      <td>0.085748</td>\n",
       "      <td>-0.055347</td>\n",
       "      <td>-0.025910</td>\n",
       "      <td>0.143865</td>\n",
       "      <td>0.013290</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.220275</td>\n",
       "      <td>0.047710</td>\n",
       "      <td>0.013608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152616</td>\n",
       "      <td>-0.070172</td>\n",
       "      <td>0.032864</td>\n",
       "      <td>-0.087452</td>\n",
       "      <td>0.066551</td>\n",
       "      <td>-0.196603</td>\n",
       "      <td>-0.214244</td>\n",
       "      <td>-0.050584</td>\n",
       "      <td>0.136623</td>\n",
       "      <td>-0.070867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.024351</td>\n",
       "      <td>-0.025448</td>\n",
       "      <td>0.029083</td>\n",
       "      <td>0.028915</td>\n",
       "      <td>-0.015896</td>\n",
       "      <td>-0.005951</td>\n",
       "      <td>-0.008911</td>\n",
       "      <td>-0.014607</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>-0.007299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020240</td>\n",
       "      <td>0.011874</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>-0.018747</td>\n",
       "      <td>0.005866</td>\n",
       "      <td>-0.012577</td>\n",
       "      <td>0.005568</td>\n",
       "      <td>-0.002583</td>\n",
       "      <td>1.222158</td>\n",
       "      <td>0.009548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.009116</td>\n",
       "      <td>-0.059364</td>\n",
       "      <td>0.080977</td>\n",
       "      <td>0.082947</td>\n",
       "      <td>0.017058</td>\n",
       "      <td>0.045087</td>\n",
       "      <td>-0.013547</td>\n",
       "      <td>0.076063</td>\n",
       "      <td>-0.040903</td>\n",
       "      <td>0.061806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113758</td>\n",
       "      <td>-0.003354</td>\n",
       "      <td>0.045265</td>\n",
       "      <td>-0.061180</td>\n",
       "      <td>0.128599</td>\n",
       "      <td>-0.113433</td>\n",
       "      <td>-0.056639</td>\n",
       "      <td>0.027372</td>\n",
       "      <td>-0.304575</td>\n",
       "      <td>0.020607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.050452</td>\n",
       "      <td>-0.004069</td>\n",
       "      <td>0.051529</td>\n",
       "      <td>-0.016139</td>\n",
       "      <td>-0.035310</td>\n",
       "      <td>-0.042531</td>\n",
       "      <td>-0.018821</td>\n",
       "      <td>-0.088688</td>\n",
       "      <td>0.069031</td>\n",
       "      <td>-0.019669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006450</td>\n",
       "      <td>0.016460</td>\n",
       "      <td>0.033926</td>\n",
       "      <td>0.015020</td>\n",
       "      <td>-0.004515</td>\n",
       "      <td>0.014543</td>\n",
       "      <td>0.011249</td>\n",
       "      <td>-0.025281</td>\n",
       "      <td>-0.335516</td>\n",
       "      <td>-0.082344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.058259</td>\n",
       "      <td>-0.025028</td>\n",
       "      <td>0.076832</td>\n",
       "      <td>0.016357</td>\n",
       "      <td>-0.004936</td>\n",
       "      <td>-0.007937</td>\n",
       "      <td>-0.010299</td>\n",
       "      <td>0.192462</td>\n",
       "      <td>-0.043300</td>\n",
       "      <td>0.108299</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018351</td>\n",
       "      <td>-0.018562</td>\n",
       "      <td>-0.046130</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>-0.045559</td>\n",
       "      <td>0.021136</td>\n",
       "      <td>0.015408</td>\n",
       "      <td>-0.064972</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>-0.121897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.106090</td>\n",
       "      <td>-0.020296</td>\n",
       "      <td>-0.030895</td>\n",
       "      <td>-0.018969</td>\n",
       "      <td>-0.052276</td>\n",
       "      <td>-0.045708</td>\n",
       "      <td>-0.013706</td>\n",
       "      <td>0.123368</td>\n",
       "      <td>0.020303</td>\n",
       "      <td>0.008030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103146</td>\n",
       "      <td>-0.012361</td>\n",
       "      <td>0.058884</td>\n",
       "      <td>-0.080483</td>\n",
       "      <td>0.052753</td>\n",
       "      <td>-0.069762</td>\n",
       "      <td>-0.059316</td>\n",
       "      <td>-0.015329</td>\n",
       "      <td>0.100993</td>\n",
       "      <td>0.035221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.020805</td>\n",
       "      <td>-0.015838</td>\n",
       "      <td>0.114933</td>\n",
       "      <td>0.009028</td>\n",
       "      <td>0.012269</td>\n",
       "      <td>-0.018553</td>\n",
       "      <td>-0.008200</td>\n",
       "      <td>-0.152733</td>\n",
       "      <td>0.034303</td>\n",
       "      <td>0.080936</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055928</td>\n",
       "      <td>-0.017759</td>\n",
       "      <td>-0.056438</td>\n",
       "      <td>0.043313</td>\n",
       "      <td>-0.012665</td>\n",
       "      <td>0.031459</td>\n",
       "      <td>0.028979</td>\n",
       "      <td>-0.011972</td>\n",
       "      <td>0.065460</td>\n",
       "      <td>-0.100900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.079977</td>\n",
       "      <td>-0.051714</td>\n",
       "      <td>-0.057123</td>\n",
       "      <td>0.010678</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>-0.006847</td>\n",
       "      <td>-0.014012</td>\n",
       "      <td>-0.048217</td>\n",
       "      <td>0.009261</td>\n",
       "      <td>-0.094823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029331</td>\n",
       "      <td>0.009678</td>\n",
       "      <td>-0.005946</td>\n",
       "      <td>0.059176</td>\n",
       "      <td>0.010087</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>0.043262</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>-0.003692</td>\n",
       "      <td>-0.007884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.112749</td>\n",
       "      <td>-0.000791</td>\n",
       "      <td>-0.023267</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.030570</td>\n",
       "      <td>-0.014973</td>\n",
       "      <td>-0.006051</td>\n",
       "      <td>0.035198</td>\n",
       "      <td>-0.008247</td>\n",
       "      <td>-0.002117</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025256</td>\n",
       "      <td>0.048789</td>\n",
       "      <td>-0.071363</td>\n",
       "      <td>0.046809</td>\n",
       "      <td>0.013171</td>\n",
       "      <td>0.037264</td>\n",
       "      <td>0.029472</td>\n",
       "      <td>-0.019275</td>\n",
       "      <td>-0.051045</td>\n",
       "      <td>0.028845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.002944</td>\n",
       "      <td>-0.044904</td>\n",
       "      <td>-0.054600</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>-0.004204</td>\n",
       "      <td>-0.071725</td>\n",
       "      <td>-0.032927</td>\n",
       "      <td>-0.055176</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012223</td>\n",
       "      <td>-0.021019</td>\n",
       "      <td>0.034458</td>\n",
       "      <td>-0.035556</td>\n",
       "      <td>-0.018441</td>\n",
       "      <td>-0.024375</td>\n",
       "      <td>0.011456</td>\n",
       "      <td>-0.006321</td>\n",
       "      <td>-0.018581</td>\n",
       "      <td>-0.044477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.037822</td>\n",
       "      <td>-0.003788</td>\n",
       "      <td>0.033072</td>\n",
       "      <td>-0.014076</td>\n",
       "      <td>0.025508</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>-0.011544</td>\n",
       "      <td>0.026696</td>\n",
       "      <td>-0.009575</td>\n",
       "      <td>0.001975</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035466</td>\n",
       "      <td>-0.009291</td>\n",
       "      <td>0.027295</td>\n",
       "      <td>-0.021266</td>\n",
       "      <td>0.015721</td>\n",
       "      <td>-0.005716</td>\n",
       "      <td>-0.022716</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>-0.002063</td>\n",
       "      <td>0.045269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.003378</td>\n",
       "      <td>-0.001755</td>\n",
       "      <td>0.014068</td>\n",
       "      <td>-0.004757</td>\n",
       "      <td>-0.007025</td>\n",
       "      <td>-0.000889</td>\n",
       "      <td>-0.000820</td>\n",
       "      <td>-0.011257</td>\n",
       "      <td>-0.001164</td>\n",
       "      <td>-0.012652</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001616</td>\n",
       "      <td>-0.006498</td>\n",
       "      <td>-0.002586</td>\n",
       "      <td>-0.019769</td>\n",
       "      <td>0.031439</td>\n",
       "      <td>0.019750</td>\n",
       "      <td>0.011958</td>\n",
       "      <td>-0.073313</td>\n",
       "      <td>0.006417</td>\n",
       "      <td>-0.037538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.004068</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>-0.001923</td>\n",
       "      <td>0.008627</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.006341</td>\n",
       "      <td>0.003304</td>\n",
       "      <td>-0.004394</td>\n",
       "      <td>0.006997</td>\n",
       "      <td>0.006345</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004178</td>\n",
       "      <td>-0.006921</td>\n",
       "      <td>0.010231</td>\n",
       "      <td>-0.007589</td>\n",
       "      <td>0.012796</td>\n",
       "      <td>-0.004698</td>\n",
       "      <td>-0.000903</td>\n",
       "      <td>0.006199</td>\n",
       "      <td>-0.002825</td>\n",
       "      <td>-0.019778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0  -0.229251  0.038701 -0.046333  0.002087  0.050264  0.010320 -0.002822   \n",
       "1  -0.415345  0.085748 -0.055347 -0.025910  0.143865  0.013290 -0.000004   \n",
       "2   0.024351 -0.025448  0.029083  0.028915 -0.015896 -0.005951 -0.008911   \n",
       "3  -0.009116 -0.059364  0.080977  0.082947  0.017058  0.045087 -0.013547   \n",
       "4   0.050452 -0.004069  0.051529 -0.016139 -0.035310 -0.042531 -0.018821   \n",
       "5   0.058259 -0.025028  0.076832  0.016357 -0.004936 -0.007937 -0.010299   \n",
       "6   0.106090 -0.020296 -0.030895 -0.018969 -0.052276 -0.045708 -0.013706   \n",
       "7  -0.020805 -0.015838  0.114933  0.009028  0.012269 -0.018553 -0.008200   \n",
       "8  -0.079977 -0.051714 -0.057123  0.010678  0.001891 -0.006847 -0.014012   \n",
       "9  -0.112749 -0.000791 -0.023267  0.001419  0.030570 -0.014973 -0.006051   \n",
       "10  0.002944 -0.044904 -0.054600  0.003822  0.000312 -0.000698 -0.004204   \n",
       "11 -0.037822 -0.003788  0.033072 -0.014076  0.025508  0.001868 -0.011544   \n",
       "12  0.003378 -0.001755  0.014068 -0.004757 -0.007025 -0.000889 -0.000820   \n",
       "13 -0.004068  0.004273 -0.001923  0.008627 -0.000003 -0.006341  0.003304   \n",
       "14 -0.000000 -0.000000 -0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "15 -0.000000  0.000000 -0.000000  0.000000  0.000000 -0.000000  0.000000   \n",
       "16 -0.000000  0.000000 -0.000000 -0.000000 -0.000000 -0.000000  0.000000   \n",
       "17 -0.000000  0.000000 -0.000000  0.000000  0.000000 -0.000000  0.000000   \n",
       "18 -0.000000  0.000000 -0.000000 -0.000000  0.000000 -0.000000 -0.000000   \n",
       "19  0.000000 -0.000000  0.000000  0.000000 -0.000000 -0.000000  0.000000   \n",
       "20  0.000000 -0.000000 -0.000000 -0.000000 -0.000000 -0.000000  0.000000   \n",
       "21 -0.000000 -0.000000  0.000000  0.000000 -0.000000 -0.000000 -0.000000   \n",
       "22 -0.000000 -0.000000  0.000000 -0.000000  0.000000  0.000000  0.000000   \n",
       "23 -0.000000  0.000000 -0.000000  0.000000 -0.000000  0.000000 -0.000000   \n",
       "24 -0.000000  0.000000  0.000000 -0.000000  0.000000 -0.000000 -0.000000   \n",
       "25  0.000000 -0.000000  0.000000 -0.000000 -0.000000  0.000000 -0.000000   \n",
       "26  0.000000  0.000000 -0.000000 -0.000000  0.000000 -0.000000  0.000000   \n",
       "27 -0.000000  0.000000  0.000000  0.000000  0.000000  0.000000 -0.000000   \n",
       "28  0.000000  0.000000 -0.000000  0.000000 -0.000000  0.000000  0.000000   \n",
       "29  0.000000  0.000000 -0.000000  0.000000 -0.000000  0.000000 -0.000000   \n",
       "30 -0.000000  0.000000 -0.000000 -0.000000 -0.000000  0.000000 -0.000000   \n",
       "31  0.000000  0.000000 -0.000000  0.000000  0.000000  0.000000 -0.000000   \n",
       "32 -0.000000 -0.000000 -0.000000 -0.000000 -0.000000 -0.000000  0.000000   \n",
       "33 -0.000000  0.000000  0.000000 -0.000000 -0.000000 -0.000000 -0.000000   \n",
       "34  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000 -0.000000   \n",
       "35  0.000000  0.000000  0.000000  0.000000  0.000000 -0.000000  0.000000   \n",
       "36  0.000000  0.000000 -0.000000  0.000000  0.000000 -0.000000 -0.000000   \n",
       "37  0.000000 -0.000000 -0.000000 -0.000000  0.000000  0.000000 -0.000000   \n",
       "38 -0.000000  0.000000  0.000000  0.000000 -0.000000 -0.000000  0.000000   \n",
       "39 -0.000000 -0.000000 -0.000000  0.000000  0.000000 -0.000000  0.000000   \n",
       "40 -0.000000 -0.000000  0.000000 -0.000000  0.000000  0.000000  0.000000   \n",
       "41 -0.000000 -0.000000 -0.000000  0.000000 -0.000000 -0.000000 -0.000000   \n",
       "\n",
       "          7         8         9   ...        32        33        34        35  \\\n",
       "0  -0.191301  0.034447  0.024862  ...  0.117154  0.347586  0.280505  0.250070   \n",
       "1  -0.220275  0.047710  0.013608  ...  0.152616 -0.070172  0.032864 -0.087452   \n",
       "2  -0.014607  0.001063 -0.007299  ...  0.020240  0.011874  0.001021 -0.018747   \n",
       "3   0.076063 -0.040903  0.061806  ...  0.113758 -0.003354  0.045265 -0.061180   \n",
       "4  -0.088688  0.069031 -0.019669  ...  0.006450  0.016460  0.033926  0.015020   \n",
       "5   0.192462 -0.043300  0.108299  ... -0.018351 -0.018562 -0.046130  0.001137   \n",
       "6   0.123368  0.020303  0.008030  ...  0.103146 -0.012361  0.058884 -0.080483   \n",
       "7  -0.152733  0.034303  0.080936  ... -0.055928 -0.017759 -0.056438  0.043313   \n",
       "8  -0.048217  0.009261 -0.094823  ... -0.029331  0.009678 -0.005946  0.059176   \n",
       "9   0.035198 -0.008247 -0.002117  ... -0.025256  0.048789 -0.071363  0.046809   \n",
       "10 -0.071725 -0.032927 -0.055176  ... -0.012223 -0.021019  0.034458 -0.035556   \n",
       "11  0.026696 -0.009575  0.001975  ... -0.035466 -0.009291  0.027295 -0.021266   \n",
       "12 -0.011257 -0.001164 -0.012652  ... -0.001616 -0.006498 -0.002586 -0.019769   \n",
       "13 -0.004394  0.006997  0.006345  ... -0.004178 -0.006921  0.010231 -0.007589   \n",
       "14  0.000000 -0.000000  0.000000  ... -0.000000  0.000000  0.000000  0.000000   \n",
       "15  0.000000 -0.000000  0.000000  ... -0.000000 -0.000000  0.000000  0.000000   \n",
       "16  0.000000 -0.000000  0.000000  ...  0.000000  0.000000 -0.000000  0.000000   \n",
       "17 -0.000000  0.000000 -0.000000  ...  0.000000  0.000000 -0.000000 -0.000000   \n",
       "18 -0.000000 -0.000000 -0.000000  ...  0.000000 -0.000000  0.000000  0.000000   \n",
       "19 -0.000000  0.000000  0.000000  ... -0.000000  0.000000  0.000000 -0.000000   \n",
       "20  0.000000 -0.000000  0.000000  ...  0.000000  0.000000 -0.000000  0.000000   \n",
       "21 -0.000000 -0.000000  0.000000  ...  0.000000 -0.000000  0.000000 -0.000000   \n",
       "22  0.000000  0.000000  0.000000  ... -0.000000 -0.000000  0.000000  0.000000   \n",
       "23  0.000000  0.000000  0.000000  ... -0.000000  0.000000  0.000000  0.000000   \n",
       "24  0.000000  0.000000  0.000000  ...  0.000000  0.000000 -0.000000 -0.000000   \n",
       "25 -0.000000  0.000000  0.000000  ... -0.000000  0.000000  0.000000 -0.000000   \n",
       "26  0.000000 -0.000000  0.000000  ... -0.000000  0.000000  0.000000 -0.000000   \n",
       "27  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000 -0.000000   \n",
       "28 -0.000000  0.000000  0.000000  ... -0.000000  0.000000 -0.000000 -0.000000   \n",
       "29 -0.000000  0.000000 -0.000000  ...  0.000000  0.000000 -0.000000  0.000000   \n",
       "30  0.000000  0.000000  0.000000  ... -0.000000  0.000000  0.000000 -0.000000   \n",
       "31  0.000000  0.000000 -0.000000  ... -0.000000  0.000000 -0.000000 -0.000000   \n",
       "32 -0.000000 -0.000000  0.000000  ... -0.000000  0.000000  0.000000 -0.000000   \n",
       "33 -0.000000 -0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "34  0.000000  0.000000 -0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "35 -0.000000 -0.000000 -0.000000  ... -0.000000  0.000000  0.000000 -0.000000   \n",
       "36  0.000000  0.000000 -0.000000  ... -0.000000 -0.000000  0.000000  0.000000   \n",
       "37  0.000000  0.000000 -0.000000  ...  0.000000 -0.000000  0.000000 -0.000000   \n",
       "38  0.000000 -0.000000 -0.000000  ...  0.000000  0.000000 -0.000000  0.000000   \n",
       "39  0.000000  0.000000 -0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "40  0.000000  0.000000 -0.000000  ... -0.000000  0.000000  0.000000 -0.000000   \n",
       "41  0.000000 -0.000000  0.000000  ... -0.000000 -0.000000  0.000000  0.000000   \n",
       "\n",
       "          36        37        38        39        40        41  \n",
       "0   0.290424  0.214873  0.229300  0.118473  0.130554 -0.024460  \n",
       "1   0.066551 -0.196603 -0.214244 -0.050584  0.136623 -0.070867  \n",
       "2   0.005866 -0.012577  0.005568 -0.002583  1.222158  0.009548  \n",
       "3   0.128599 -0.113433 -0.056639  0.027372 -0.304575  0.020607  \n",
       "4  -0.004515  0.014543  0.011249 -0.025281 -0.335516 -0.082344  \n",
       "5  -0.045559  0.021136  0.015408 -0.064972 -0.000156 -0.121897  \n",
       "6   0.052753 -0.069762 -0.059316 -0.015329  0.100993  0.035221  \n",
       "7  -0.012665  0.031459  0.028979 -0.011972  0.065460 -0.100900  \n",
       "8   0.010087  0.002142  0.043262  0.005024 -0.003692 -0.007884  \n",
       "9   0.013171  0.037264  0.029472 -0.019275 -0.051045  0.028845  \n",
       "10 -0.018441 -0.024375  0.011456 -0.006321 -0.018581 -0.044477  \n",
       "11  0.015721 -0.005716 -0.022716  0.003787 -0.002063  0.045269  \n",
       "12  0.031439  0.019750  0.011958 -0.073313  0.006417 -0.037538  \n",
       "13  0.012796 -0.004698 -0.000903  0.006199 -0.002825 -0.019778  \n",
       "14 -0.000000  0.000000 -0.000000 -0.000000 -0.000000  0.000000  \n",
       "15  0.000000  0.000000  0.000000 -0.000000  0.000000  0.000000  \n",
       "16  0.000000 -0.000000 -0.000000  0.000000  0.000000 -0.000000  \n",
       "17  0.000000 -0.000000 -0.000000 -0.000000 -0.000000 -0.000000  \n",
       "18 -0.000000  0.000000 -0.000000  0.000000  0.000000  0.000000  \n",
       "19  0.000000 -0.000000  0.000000 -0.000000 -0.000000  0.000000  \n",
       "20  0.000000 -0.000000 -0.000000  0.000000 -0.000000  0.000000  \n",
       "21 -0.000000  0.000000  0.000000  0.000000 -0.000000 -0.000000  \n",
       "22 -0.000000 -0.000000 -0.000000 -0.000000  0.000000 -0.000000  \n",
       "23 -0.000000 -0.000000 -0.000000 -0.000000  0.000000 -0.000000  \n",
       "24 -0.000000  0.000000 -0.000000  0.000000  0.000000  0.000000  \n",
       "25  0.000000 -0.000000 -0.000000  0.000000  0.000000  0.000000  \n",
       "26 -0.000000 -0.000000 -0.000000 -0.000000 -0.000000 -0.000000  \n",
       "27 -0.000000  0.000000  0.000000 -0.000000 -0.000000 -0.000000  \n",
       "28 -0.000000  0.000000 -0.000000 -0.000000 -0.000000  0.000000  \n",
       "29 -0.000000  0.000000 -0.000000 -0.000000 -0.000000  0.000000  \n",
       "30  0.000000 -0.000000  0.000000 -0.000000 -0.000000  0.000000  \n",
       "31 -0.000000  0.000000  0.000000 -0.000000  0.000000 -0.000000  \n",
       "32  0.000000  0.000000 -0.000000  0.000000 -0.000000  0.000000  \n",
       "33 -0.000000 -0.000000 -0.000000 -0.000000 -0.000000  0.000000  \n",
       "34  0.000000  0.000000 -0.000000 -0.000000 -0.000000 -0.000000  \n",
       "35 -0.000000  0.000000 -0.000000  0.000000 -0.000000 -0.000000  \n",
       "36 -0.000000 -0.000000  0.000000 -0.000000 -0.000000 -0.000000  \n",
       "37 -0.000000  0.000000  0.000000  0.000000  0.000000 -0.000000  \n",
       "38  0.000000  0.000000 -0.000000 -0.000000 -0.000000  0.000000  \n",
       "39 -0.000000  0.000000 -0.000000  0.000000 -0.000000  0.000000  \n",
       "40 -0.000000 -0.000000 -0.000000 -0.000000  0.000000 -0.000000  \n",
       "41 -0.000000 -0.000000 -0.000000  0.000000  0.000000 -0.000000  \n",
       "\n",
       "[42 rows x 42 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factore = FactorAnalysis().fit(X_train_imp)\n",
    "pd.DataFrame(factore.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65731749\n",
      "Iteration 2, loss = 0.60232364\n",
      "Iteration 3, loss = 0.55444914\n",
      "Iteration 4, loss = 0.54724275\n",
      "Iteration 5, loss = 0.52977980\n",
      "Iteration 6, loss = 0.51550800\n",
      "Iteration 7, loss = 0.51014091\n",
      "Iteration 8, loss = 0.49673520\n",
      "Iteration 9, loss = 0.49283121\n",
      "Iteration 10, loss = 0.48342040\n",
      "Iteration 11, loss = 0.47456023\n",
      "Iteration 12, loss = 0.46857123\n",
      "Iteration 13, loss = 0.46273774\n",
      "Iteration 14, loss = 0.45368652\n",
      "Iteration 15, loss = 0.44764017\n",
      "Iteration 16, loss = 0.43897724\n",
      "Iteration 17, loss = 0.43482439\n",
      "Iteration 18, loss = 0.42486878\n",
      "Iteration 19, loss = 0.42022122\n",
      "Iteration 20, loss = 0.41612885\n",
      "Iteration 21, loss = 0.40816771\n",
      "Iteration 22, loss = 0.40062184\n",
      "Iteration 23, loss = 0.39526265\n",
      "Iteration 24, loss = 0.39684879\n",
      "Iteration 25, loss = 0.38551632\n",
      "Iteration 26, loss = 0.37897331\n",
      "Iteration 27, loss = 0.37268364\n",
      "Iteration 28, loss = 0.36539491\n",
      "Iteration 29, loss = 0.35903854\n",
      "Iteration 30, loss = 0.35386784\n",
      "Iteration 31, loss = 0.34830873\n",
      "Iteration 32, loss = 0.34268440\n",
      "Iteration 33, loss = 0.33925757\n",
      "Iteration 34, loss = 0.33727877\n",
      "Iteration 35, loss = 0.32625128\n",
      "Iteration 36, loss = 0.32351811\n",
      "Iteration 37, loss = 0.31872641\n",
      "Iteration 38, loss = 0.30852996\n",
      "Iteration 39, loss = 0.30435120\n",
      "Iteration 40, loss = 0.29907664\n",
      "Iteration 41, loss = 0.29624212\n",
      "Iteration 42, loss = 0.29400295\n",
      "Iteration 43, loss = 0.28281278\n",
      "Iteration 44, loss = 0.28283511\n",
      "Iteration 45, loss = 0.27709766\n",
      "Iteration 46, loss = 0.27057682\n",
      "Iteration 47, loss = 0.26962703\n",
      "Iteration 48, loss = 0.26501242\n",
      "Iteration 49, loss = 0.26005447\n",
      "Iteration 50, loss = 0.25439913\n",
      "Iteration 51, loss = 0.25313100\n",
      "Iteration 52, loss = 0.24783048\n",
      "Iteration 53, loss = 0.24477445\n",
      "Iteration 54, loss = 0.23924951\n",
      "Iteration 55, loss = 0.23466719\n",
      "Iteration 56, loss = 0.23488687\n",
      "Iteration 57, loss = 0.23307260\n",
      "Iteration 58, loss = 0.22357021\n",
      "Iteration 59, loss = 0.22170178\n",
      "Iteration 60, loss = 0.21761820\n",
      "Iteration 61, loss = 0.21416754\n",
      "Iteration 62, loss = 0.21108392\n",
      "Iteration 63, loss = 0.20738877\n",
      "Iteration 64, loss = 0.20418036\n",
      "Iteration 65, loss = 0.20000111\n",
      "Iteration 66, loss = 0.19862129\n",
      "Iteration 67, loss = 0.19570497\n",
      "Iteration 68, loss = 0.19093648\n",
      "Iteration 69, loss = 0.18720463\n",
      "Iteration 70, loss = 0.18292280\n",
      "Iteration 71, loss = 0.18280150\n",
      "Iteration 72, loss = 0.18023756\n",
      "Iteration 73, loss = 0.17896801\n",
      "Iteration 74, loss = 0.17958799\n",
      "Iteration 75, loss = 0.17652420\n",
      "Iteration 76, loss = 0.16475585\n",
      "Iteration 77, loss = 0.16710400\n",
      "Iteration 78, loss = 0.16293083\n",
      "Iteration 79, loss = 0.15990235\n",
      "Iteration 80, loss = 0.16197048\n",
      "Iteration 81, loss = 0.15522635\n",
      "Iteration 82, loss = 0.14933357\n",
      "Iteration 83, loss = 0.14802825\n",
      "Iteration 84, loss = 0.14615416\n",
      "Iteration 85, loss = 0.14313830\n",
      "Iteration 86, loss = 0.14374930\n",
      "Iteration 87, loss = 0.13472392\n",
      "Iteration 88, loss = 0.13382429\n",
      "Iteration 89, loss = 0.13150279\n",
      "Iteration 90, loss = 0.12457273\n",
      "Iteration 91, loss = 0.12725459\n",
      "Iteration 92, loss = 0.12358184\n",
      "Iteration 93, loss = 0.11704265\n",
      "Iteration 94, loss = 0.11476954\n",
      "Iteration 95, loss = 0.11191685\n",
      "Iteration 96, loss = 0.10925219\n",
      "Iteration 97, loss = 0.11155057\n",
      "Iteration 98, loss = 0.11544655\n",
      "Iteration 99, loss = 0.10988495\n",
      "Iteration 100, loss = 0.10302439\n",
      "Iteration 101, loss = 0.09746396\n",
      "Iteration 102, loss = 0.09750821\n",
      "Iteration 103, loss = 0.09456253\n",
      "Iteration 104, loss = 0.09031635\n",
      "Iteration 105, loss = 0.08880567\n",
      "Iteration 106, loss = 0.09394158\n",
      "Iteration 107, loss = 0.09476527\n",
      "Iteration 108, loss = 0.09392703\n",
      "Iteration 109, loss = 0.08671078\n",
      "Iteration 110, loss = 0.09490423\n",
      "Iteration 111, loss = 0.08767601\n",
      "Iteration 112, loss = 0.07805021\n",
      "Iteration 113, loss = 0.07926981\n",
      "Iteration 114, loss = 0.07092337\n",
      "Iteration 115, loss = 0.06805247\n",
      "Iteration 116, loss = 0.06751725\n",
      "Iteration 117, loss = 0.06778750\n",
      "Iteration 118, loss = 0.06470785\n",
      "Iteration 119, loss = 0.06651296\n",
      "Iteration 120, loss = 0.06655459\n",
      "Iteration 121, loss = 0.06116420\n",
      "Iteration 122, loss = 0.05712315\n",
      "Iteration 123, loss = 0.05674129\n",
      "Iteration 124, loss = 0.05486266\n",
      "Iteration 125, loss = 0.05365272\n",
      "Iteration 126, loss = 0.05087977\n",
      "Iteration 127, loss = 0.05254123\n",
      "Iteration 128, loss = 0.05089607\n",
      "Iteration 129, loss = 0.04716316\n",
      "Iteration 130, loss = 0.04760401\n",
      "Iteration 131, loss = 0.04822223\n",
      "Iteration 132, loss = 0.05554355\n",
      "Iteration 133, loss = 0.06153312\n",
      "Iteration 134, loss = 0.05166217\n",
      "Iteration 135, loss = 0.04898164\n",
      "Iteration 136, loss = 0.04484350\n",
      "Iteration 137, loss = 0.04157957\n",
      "Iteration 138, loss = 0.04895705\n",
      "Iteration 139, loss = 0.04447995\n",
      "Iteration 140, loss = 0.03710808\n",
      "Iteration 141, loss = 0.03425677\n",
      "Iteration 142, loss = 0.03482478\n",
      "Iteration 143, loss = 0.03490899\n",
      "Iteration 144, loss = 0.03399694\n",
      "Iteration 145, loss = 0.03325290\n",
      "Iteration 146, loss = 0.03318738\n",
      "Iteration 147, loss = 0.03585138\n",
      "Iteration 148, loss = 0.03528733\n",
      "Iteration 149, loss = 0.03693046\n",
      "Iteration 150, loss = 0.03402206\n",
      "Iteration 151, loss = 0.02958612\n",
      "Iteration 152, loss = 0.02715834\n",
      "Iteration 153, loss = 0.02715590\n",
      "Iteration 154, loss = 0.02755914\n",
      "Iteration 155, loss = 0.02532805\n",
      "Iteration 156, loss = 0.02790980\n",
      "Iteration 157, loss = 0.02413721\n",
      "Iteration 158, loss = 0.02587179\n",
      "Iteration 159, loss = 0.02220818\n",
      "Iteration 160, loss = 0.02286899\n",
      "Iteration 161, loss = 0.02427098\n",
      "Iteration 162, loss = 0.02402170\n",
      "Iteration 163, loss = 0.02136763\n",
      "Iteration 164, loss = 0.02584035\n",
      "Iteration 165, loss = 0.02489109\n",
      "Iteration 166, loss = 0.02136183\n",
      "Iteration 167, loss = 0.02467953\n",
      "Iteration 168, loss = 0.02079890\n",
      "Iteration 169, loss = 0.01915359\n",
      "Iteration 170, loss = 0.02051242\n",
      "Iteration 171, loss = 0.01850615\n",
      "Iteration 172, loss = 0.01951798\n",
      "Iteration 173, loss = 0.01797533\n",
      "Iteration 174, loss = 0.01605975\n",
      "Iteration 175, loss = 0.01566659\n",
      "Iteration 176, loss = 0.01636243\n",
      "Iteration 177, loss = 0.01578977\n",
      "Iteration 178, loss = 0.01620347\n",
      "Iteration 179, loss = 0.01566750\n",
      "Iteration 180, loss = 0.01471180\n",
      "Iteration 181, loss = 0.01476719\n",
      "Iteration 182, loss = 0.01432647\n",
      "Iteration 183, loss = 0.01388515\n",
      "Iteration 184, loss = 0.01362723\n",
      "Iteration 185, loss = 0.01340603\n",
      "Iteration 186, loss = 0.01354263\n",
      "Iteration 187, loss = 0.01330901\n",
      "Iteration 188, loss = 0.01286692\n",
      "Iteration 189, loss = 0.01215889\n",
      "Iteration 190, loss = 0.01270510\n",
      "Iteration 191, loss = 0.01285090\n",
      "Iteration 192, loss = 0.01184139\n",
      "Iteration 193, loss = 0.01228474\n",
      "Iteration 194, loss = 0.01295128\n",
      "Iteration 195, loss = 0.01114226\n",
      "Iteration 196, loss = 0.01106978\n",
      "Iteration 197, loss = 0.01179803\n",
      "Iteration 198, loss = 0.01225852\n",
      "Iteration 199, loss = 0.01216511\n",
      "Iteration 200, loss = 0.01015241\n",
      "Iteration 201, loss = 0.01102843\n",
      "Iteration 202, loss = 0.00999490\n",
      "Iteration 203, loss = 0.01045128\n",
      "Iteration 204, loss = 0.01054120\n",
      "Iteration 205, loss = 0.01096707\n",
      "Iteration 206, loss = 0.00937416\n",
      "Iteration 207, loss = 0.01035142\n",
      "Iteration 208, loss = 0.01022196\n",
      "Iteration 209, loss = 0.00960550\n",
      "Iteration 210, loss = 0.00973611\n",
      "Iteration 211, loss = 0.01038936\n",
      "Iteration 212, loss = 0.00919197\n",
      "Iteration 213, loss = 0.00949325\n",
      "Iteration 214, loss = 0.00864929\n",
      "Iteration 215, loss = 0.00884557\n",
      "Iteration 216, loss = 0.00833984\n",
      "Iteration 217, loss = 0.00798801\n",
      "Iteration 218, loss = 0.00893859\n",
      "Iteration 219, loss = 0.00793227\n",
      "Iteration 220, loss = 0.00851920\n",
      "Iteration 221, loss = 0.00766585\n",
      "Iteration 222, loss = 0.00933934\n",
      "Iteration 223, loss = 0.00753755\n",
      "Iteration 224, loss = 0.00820348\n",
      "Iteration 225, loss = 0.00749782\n",
      "Iteration 226, loss = 0.00787788\n",
      "Iteration 227, loss = 0.00807471\n",
      "Iteration 228, loss = 0.00740518\n",
      "Iteration 229, loss = 0.00711790\n",
      "Iteration 230, loss = 0.00707038\n",
      "Iteration 231, loss = 0.00789861\n",
      "Iteration 232, loss = 0.00738321\n",
      "Iteration 233, loss = 0.00903761\n",
      "Iteration 234, loss = 0.00660431\n",
      "Iteration 235, loss = 0.00919245\n",
      "Iteration 236, loss = 0.01044415\n",
      "Iteration 237, loss = 0.00753724\n",
      "Iteration 238, loss = 0.00708558\n",
      "Iteration 239, loss = 0.00901081\n",
      "Iteration 240, loss = 0.01046152\n",
      "Iteration 241, loss = 0.00720517\n",
      "Iteration 242, loss = 0.01019758\n",
      "Iteration 243, loss = 0.00982481\n",
      "Iteration 244, loss = 0.00867288\n",
      "Iteration 245, loss = 0.00590895\n",
      "Iteration 246, loss = 0.00735095\n",
      "Iteration 247, loss = 0.00685745\n",
      "Iteration 248, loss = 0.00603351\n",
      "Iteration 249, loss = 0.00635566\n",
      "Iteration 250, loss = 0.00705133\n",
      "Iteration 251, loss = 0.00593426\n",
      "Iteration 252, loss = 0.00735800\n",
      "Iteration 253, loss = 0.00652351\n",
      "Iteration 254, loss = 0.00696214\n",
      "Iteration 255, loss = 0.00684989\n",
      "Iteration 256, loss = 0.00553197\n",
      "Iteration 257, loss = 0.00738418\n",
      "Iteration 258, loss = 0.00824730\n",
      "Iteration 259, loss = 0.00602555\n",
      "Iteration 260, loss = 0.00897207\n",
      "Iteration 261, loss = 0.00587507\n",
      "Iteration 262, loss = 0.00789317\n",
      "Iteration 263, loss = 0.01056375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 264, loss = 0.00824331\n",
      "Iteration 265, loss = 0.00684413\n",
      "Iteration 266, loss = 0.00782077\n",
      "Iteration 267, loss = 0.00590398\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train_imp, y_train)\n",
    "y_pred = clf.predict(X_test_imp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8163265306122449\n",
      "[[ 47  21]\n",
      " [ 15 113]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      0.69      0.72        68\n",
      "        True       0.84      0.88      0.86       128\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       196\n",
      "   macro avg       0.80      0.79      0.79       196\n",
      "weighted avg       0.81      0.82      0.81       196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/joe/MLP/features_sample_head1.csv\")\n",
    "df = df.astype({'Class_P0': 'bool', \n",
    "                'head0_AU28': 'bool',\n",
    "                'head0_AU45': 'bool',\n",
    "                'head0_AU26': 'bool',\n",
    "                'head0_AU25': 'bool',\n",
    "                'head0_AU01': 'bool',\n",
    "                'head0_AU05': 'bool',\n",
    "                'head0_AU20': 'bool',\n",
    "                'head0_AU06': 'bool',\n",
    "                'head0_AU23': 'bool',\n",
    "                'head0_AU07': 'bool',\n",
    "                'head0_AU02': 'bool', \n",
    "                'head0_AU09': 'bool',\n",
    "                'head0_AU17': 'bool',\n",
    "                'head0_AU10': 'bool',\n",
    "                'head0_AU04': 'bool',\n",
    "                'head0_AU12': 'bool',\n",
    "                'head0_AU14': 'bool',\n",
    "                'head0_AU15': 'bool',\n",
    "                'ClassP1':'bool'})\n",
    "\n",
    "le = LabelEncoder()\n",
    "# df['head0_pose'] = le.fit_transform(df['head0_pose'])\n",
    "df['head0_pose'] = le.fit_transform(df['head0_pose'])\n",
    "# df['head2_pose'] = le.fit_transform(df['head2_pose'])\n",
    "# df['head0_emo'] = le.fit_transform(df['head0_emo'])\n",
    "df['head0_emo'] = le.fit_transform(df['head0_emo'])\n",
    "# df['head2_emo'] = le.fit_transform(df['head2_emo'])\n",
    "\n",
    "y = df['ClassP1']\n",
    "x = df.drop(['ClassP1'], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size= 0.25, random_state=27)\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imp = imp.fit(x_train)\n",
    "\n",
    "X_train_imp = imp.transform(x_train)\n",
    "X_test_imp = imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=300, alpha=0.0001, activation='tanh',\n",
    "                     solver='adam', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63571733\n",
      "Iteration 2, loss = 0.55531078\n",
      "Iteration 3, loss = 0.53410567\n",
      "Iteration 4, loss = 0.48759481\n",
      "Iteration 5, loss = 0.48208121\n",
      "Iteration 6, loss = 0.45857526\n",
      "Iteration 7, loss = 0.44192361\n",
      "Iteration 8, loss = 0.42835007\n",
      "Iteration 9, loss = 0.40878332\n",
      "Iteration 10, loss = 0.40356159\n",
      "Iteration 11, loss = 0.38624829\n",
      "Iteration 12, loss = 0.38067414\n",
      "Iteration 13, loss = 0.37102095\n",
      "Iteration 14, loss = 0.36119164\n",
      "Iteration 15, loss = 0.35474081\n",
      "Iteration 16, loss = 0.34767750\n",
      "Iteration 17, loss = 0.34240659\n",
      "Iteration 18, loss = 0.33808606\n",
      "Iteration 19, loss = 0.33291765\n",
      "Iteration 20, loss = 0.32823991\n",
      "Iteration 21, loss = 0.32311748\n",
      "Iteration 22, loss = 0.31958365\n",
      "Iteration 23, loss = 0.31494877\n",
      "Iteration 24, loss = 0.31166798\n",
      "Iteration 25, loss = 0.30583975\n",
      "Iteration 26, loss = 0.30492560\n",
      "Iteration 27, loss = 0.30254457\n",
      "Iteration 28, loss = 0.29621326\n",
      "Iteration 29, loss = 0.29246492\n",
      "Iteration 30, loss = 0.28768846\n",
      "Iteration 31, loss = 0.28299911\n",
      "Iteration 32, loss = 0.28106630\n",
      "Iteration 33, loss = 0.27808553\n",
      "Iteration 34, loss = 0.27089018\n",
      "Iteration 35, loss = 0.26995110\n",
      "Iteration 36, loss = 0.26886524\n",
      "Iteration 37, loss = 0.25839569\n",
      "Iteration 38, loss = 0.25992631\n",
      "Iteration 39, loss = 0.25770306\n",
      "Iteration 40, loss = 0.24616264\n",
      "Iteration 41, loss = 0.24565168\n",
      "Iteration 42, loss = 0.24494456\n",
      "Iteration 43, loss = 0.23425436\n",
      "Iteration 44, loss = 0.23710331\n",
      "Iteration 45, loss = 0.23475085\n",
      "Iteration 46, loss = 0.22347086\n",
      "Iteration 47, loss = 0.22558252\n",
      "Iteration 48, loss = 0.21562843\n",
      "Iteration 49, loss = 0.21192906\n",
      "Iteration 50, loss = 0.21025151\n",
      "Iteration 51, loss = 0.20319272\n",
      "Iteration 52, loss = 0.20448020\n",
      "Iteration 53, loss = 0.19892998\n",
      "Iteration 54, loss = 0.19339500\n",
      "Iteration 55, loss = 0.19079302\n",
      "Iteration 56, loss = 0.18616165\n",
      "Iteration 57, loss = 0.18284992\n",
      "Iteration 58, loss = 0.17837824\n",
      "Iteration 59, loss = 0.17605629\n",
      "Iteration 60, loss = 0.17560115\n",
      "Iteration 61, loss = 0.17301986\n",
      "Iteration 62, loss = 0.16705339\n",
      "Iteration 63, loss = 0.16555871\n",
      "Iteration 64, loss = 0.15739540\n",
      "Iteration 65, loss = 0.15732341\n",
      "Iteration 66, loss = 0.15125924\n",
      "Iteration 67, loss = 0.14922489\n",
      "Iteration 68, loss = 0.14482780\n",
      "Iteration 69, loss = 0.14249762\n",
      "Iteration 70, loss = 0.13915229\n",
      "Iteration 71, loss = 0.13687252\n",
      "Iteration 72, loss = 0.13448127\n",
      "Iteration 73, loss = 0.12849907\n",
      "Iteration 74, loss = 0.12799952\n",
      "Iteration 75, loss = 0.12503435\n",
      "Iteration 76, loss = 0.12281533\n",
      "Iteration 77, loss = 0.11646279\n",
      "Iteration 78, loss = 0.11667062\n",
      "Iteration 79, loss = 0.11125191\n",
      "Iteration 80, loss = 0.10834527\n",
      "Iteration 81, loss = 0.10597394\n",
      "Iteration 82, loss = 0.10547128\n",
      "Iteration 83, loss = 0.11300597\n",
      "Iteration 84, loss = 0.11000147\n",
      "Iteration 85, loss = 0.10615922\n",
      "Iteration 86, loss = 0.10673968\n",
      "Iteration 87, loss = 0.08988695\n",
      "Iteration 88, loss = 0.09863245\n",
      "Iteration 89, loss = 0.09852594\n",
      "Iteration 90, loss = 0.09177134\n",
      "Iteration 91, loss = 0.08952020\n",
      "Iteration 92, loss = 0.08250963\n",
      "Iteration 93, loss = 0.08139835\n",
      "Iteration 94, loss = 0.07559796\n",
      "Iteration 95, loss = 0.07380525\n",
      "Iteration 96, loss = 0.06866057\n",
      "Iteration 97, loss = 0.06908183\n",
      "Iteration 98, loss = 0.06613535\n",
      "Iteration 99, loss = 0.06437509\n",
      "Iteration 100, loss = 0.06331254\n",
      "Iteration 101, loss = 0.05958481\n",
      "Iteration 102, loss = 0.05943004\n",
      "Iteration 103, loss = 0.05989454\n",
      "Iteration 104, loss = 0.05462236\n",
      "Iteration 105, loss = 0.05448760\n",
      "Iteration 106, loss = 0.05032072\n",
      "Iteration 107, loss = 0.04980639\n",
      "Iteration 108, loss = 0.04773449\n",
      "Iteration 109, loss = 0.04578170\n",
      "Iteration 110, loss = 0.04548511\n",
      "Iteration 111, loss = 0.04395384\n",
      "Iteration 112, loss = 0.04248877\n",
      "Iteration 113, loss = 0.04326132\n",
      "Iteration 114, loss = 0.03920697\n",
      "Iteration 115, loss = 0.03823607\n",
      "Iteration 116, loss = 0.03676688\n",
      "Iteration 117, loss = 0.03806541\n",
      "Iteration 118, loss = 0.03480546\n",
      "Iteration 119, loss = 0.03224781\n",
      "Iteration 120, loss = 0.03222068\n",
      "Iteration 121, loss = 0.03071826\n",
      "Iteration 122, loss = 0.02980396\n",
      "Iteration 123, loss = 0.02937027\n",
      "Iteration 124, loss = 0.02888922\n",
      "Iteration 125, loss = 0.02682323\n",
      "Iteration 126, loss = 0.02608794\n",
      "Iteration 127, loss = 0.02513626\n",
      "Iteration 128, loss = 0.02490783\n",
      "Iteration 129, loss = 0.02337280\n",
      "Iteration 130, loss = 0.02293120\n",
      "Iteration 131, loss = 0.02179383\n",
      "Iteration 132, loss = 0.02110842\n",
      "Iteration 133, loss = 0.02006608\n",
      "Iteration 134, loss = 0.02014623\n",
      "Iteration 135, loss = 0.01930030\n",
      "Iteration 136, loss = 0.01828627\n",
      "Iteration 137, loss = 0.01791940\n",
      "Iteration 138, loss = 0.01721754\n",
      "Iteration 139, loss = 0.01660943\n",
      "Iteration 140, loss = 0.01613298\n",
      "Iteration 141, loss = 0.01566907\n",
      "Iteration 142, loss = 0.01546457\n",
      "Iteration 143, loss = 0.01472846\n",
      "Iteration 144, loss = 0.01471502\n",
      "Iteration 145, loss = 0.01378090\n",
      "Iteration 146, loss = 0.01361783\n",
      "Iteration 147, loss = 0.01285296\n",
      "Iteration 148, loss = 0.01282429\n",
      "Iteration 149, loss = 0.01239746\n",
      "Iteration 150, loss = 0.01198832\n",
      "Iteration 151, loss = 0.01170962\n",
      "Iteration 152, loss = 0.01128331\n",
      "Iteration 153, loss = 0.01127038\n",
      "Iteration 154, loss = 0.01121189\n",
      "Iteration 155, loss = 0.01052250\n",
      "Iteration 156, loss = 0.01049357\n",
      "Iteration 157, loss = 0.01048412\n",
      "Iteration 158, loss = 0.00930789\n",
      "Iteration 159, loss = 0.00967446\n",
      "Iteration 160, loss = 0.00945018\n",
      "Iteration 161, loss = 0.00892835\n",
      "Iteration 162, loss = 0.00887678\n",
      "Iteration 163, loss = 0.00850540\n",
      "Iteration 164, loss = 0.00884951\n",
      "Iteration 165, loss = 0.00839185\n",
      "Iteration 166, loss = 0.00820367\n",
      "Iteration 167, loss = 0.00810007\n",
      "Iteration 168, loss = 0.00731728\n",
      "Iteration 169, loss = 0.00809412\n",
      "Iteration 170, loss = 0.00742233\n",
      "Iteration 171, loss = 0.00698817\n",
      "Iteration 172, loss = 0.00698865\n",
      "Iteration 173, loss = 0.00660712\n",
      "Iteration 174, loss = 0.00659193\n",
      "Iteration 175, loss = 0.00631194\n",
      "Iteration 176, loss = 0.00614134\n",
      "Iteration 177, loss = 0.00595369\n",
      "Iteration 178, loss = 0.00577749\n",
      "Iteration 179, loss = 0.00566211\n",
      "Iteration 180, loss = 0.00558909\n",
      "Iteration 181, loss = 0.00548048\n",
      "Iteration 182, loss = 0.00534894\n",
      "Iteration 183, loss = 0.00521849\n",
      "Iteration 184, loss = 0.00511317\n",
      "Iteration 185, loss = 0.00499500\n",
      "Iteration 186, loss = 0.00499776\n",
      "Iteration 187, loss = 0.00477955\n",
      "Iteration 188, loss = 0.00476960\n",
      "Iteration 189, loss = 0.00460562\n",
      "Iteration 190, loss = 0.00457188\n",
      "Iteration 191, loss = 0.00445321\n",
      "Iteration 192, loss = 0.00434036\n",
      "Iteration 193, loss = 0.00429346\n",
      "Iteration 194, loss = 0.00419619\n",
      "Iteration 195, loss = 0.00412268\n",
      "Iteration 196, loss = 0.00404100\n",
      "Iteration 197, loss = 0.00397801\n",
      "Iteration 198, loss = 0.00398544\n",
      "Iteration 199, loss = 0.00381735\n",
      "Iteration 200, loss = 0.00380740\n",
      "Iteration 201, loss = 0.00368761\n",
      "Iteration 202, loss = 0.00362480\n",
      "Iteration 203, loss = 0.00356624\n",
      "Iteration 204, loss = 0.00353619\n",
      "Iteration 205, loss = 0.00345712\n",
      "Iteration 206, loss = 0.00342193\n",
      "Iteration 207, loss = 0.00336844\n",
      "Iteration 208, loss = 0.00331154\n",
      "Iteration 209, loss = 0.00323127\n",
      "Iteration 210, loss = 0.00319800\n",
      "Iteration 211, loss = 0.00312540\n",
      "Iteration 212, loss = 0.00308768\n",
      "Iteration 213, loss = 0.00307264\n",
      "Iteration 214, loss = 0.00298975\n",
      "Iteration 215, loss = 0.00296723\n",
      "Iteration 216, loss = 0.00292758\n",
      "Iteration 217, loss = 0.00286415\n",
      "Iteration 218, loss = 0.00280117\n",
      "Iteration 219, loss = 0.00276265\n",
      "Iteration 220, loss = 0.00271937\n",
      "Iteration 221, loss = 0.00268606\n",
      "Iteration 222, loss = 0.00264265\n",
      "Iteration 223, loss = 0.00265099\n",
      "Iteration 224, loss = 0.00263821\n",
      "Iteration 225, loss = 0.00255373\n",
      "Iteration 226, loss = 0.00250232\n",
      "Iteration 227, loss = 0.00245971\n",
      "Iteration 228, loss = 0.00243962\n",
      "Iteration 229, loss = 0.00240023\n",
      "Iteration 230, loss = 0.00238738\n",
      "Iteration 231, loss = 0.00234365\n",
      "Iteration 232, loss = 0.00231335\n",
      "Iteration 233, loss = 0.00225906\n",
      "Iteration 234, loss = 0.00223020\n",
      "Iteration 235, loss = 0.00220368\n",
      "Iteration 236, loss = 0.00217481\n",
      "Iteration 237, loss = 0.00217421\n",
      "Iteration 238, loss = 0.00211365\n",
      "Iteration 239, loss = 0.00209935\n",
      "Iteration 240, loss = 0.00207480\n",
      "Iteration 241, loss = 0.00203120\n",
      "Iteration 242, loss = 0.00202101\n",
      "Iteration 243, loss = 0.00200530\n",
      "Iteration 244, loss = 0.00197089\n",
      "Iteration 245, loss = 0.00193565\n",
      "Iteration 246, loss = 0.00191974\n",
      "Iteration 247, loss = 0.00192712\n",
      "Iteration 248, loss = 0.00187445\n",
      "Iteration 249, loss = 0.00183995\n",
      "Iteration 250, loss = 0.00183245\n",
      "Iteration 251, loss = 0.00179794\n",
      "Iteration 252, loss = 0.00178306\n",
      "Iteration 253, loss = 0.00176356\n",
      "Iteration 254, loss = 0.00174071\n",
      "Iteration 255, loss = 0.00171847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, loss = 0.00169764\n",
      "Iteration 257, loss = 0.00168043\n",
      "Iteration 258, loss = 0.00167580\n",
      "Iteration 259, loss = 0.00167094\n",
      "Iteration 260, loss = 0.00162804\n",
      "Iteration 261, loss = 0.00160509\n",
      "Iteration 262, loss = 0.00160304\n",
      "Iteration 263, loss = 0.00157319\n",
      "Iteration 264, loss = 0.00155482\n",
      "Iteration 265, loss = 0.00156048\n",
      "Iteration 266, loss = 0.00152291\n",
      "Iteration 267, loss = 0.00151124\n",
      "Iteration 268, loss = 0.00148963\n",
      "Iteration 269, loss = 0.00147379\n",
      "Iteration 270, loss = 0.00145711\n",
      "Iteration 271, loss = 0.00144330\n",
      "Iteration 272, loss = 0.00144857\n",
      "Iteration 273, loss = 0.00141443\n",
      "Iteration 274, loss = 0.00140215\n",
      "Iteration 275, loss = 0.00138620\n",
      "Iteration 276, loss = 0.00136908\n",
      "Iteration 277, loss = 0.00136065\n",
      "Iteration 278, loss = 0.00134710\n",
      "Iteration 279, loss = 0.00133056\n",
      "Iteration 280, loss = 0.00131881\n",
      "Iteration 281, loss = 0.00130384\n",
      "Iteration 282, loss = 0.00129399\n",
      "Iteration 283, loss = 0.00128199\n",
      "Iteration 284, loss = 0.00126922\n",
      "Iteration 285, loss = 0.00125986\n",
      "Iteration 286, loss = 0.00125250\n",
      "Iteration 287, loss = 0.00124192\n",
      "Iteration 288, loss = 0.00122499\n",
      "Iteration 289, loss = 0.00121122\n",
      "Iteration 290, loss = 0.00120202\n",
      "Iteration 291, loss = 0.00118797\n",
      "Iteration 292, loss = 0.00118202\n",
      "Iteration 293, loss = 0.00117361\n",
      "Iteration 294, loss = 0.00115598\n",
      "Iteration 295, loss = 0.00115654\n",
      "Iteration 296, loss = 0.00113855\n",
      "Iteration 297, loss = 0.00113923\n",
      "Iteration 298, loss = 0.00112345\n",
      "Iteration 299, loss = 0.00110521\n",
      "Iteration 300, loss = 0.00110161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joe/.local/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train_imp, y_train)\n",
    "y_pred = clf.predict(X_test_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8010204081632653\n",
      "[[ 35  20]\n",
      " [ 19 122]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.65      0.64      0.64        55\n",
      "        True       0.86      0.87      0.86       141\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       196\n",
      "   macro avg       0.75      0.75      0.75       196\n",
      "weighted avg       0.80      0.80      0.80       196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e09efebcca48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;34m'head0_AU15'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 'ClassP2':'bool'})\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mvaribale_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# df['head0_pose'] = le.fit_transform(df['head0_pose'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/joe/.local/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5067\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'feature_names'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/joe/MLP/features_sample_head2.csv\")\n",
    "df = df.astype({'Class_P0': 'bool', \n",
    "                'head0_AU28': 'bool',\n",
    "                'head0_AU45': 'bool',\n",
    "                'head0_AU26': 'bool',\n",
    "                'head0_AU25': 'bool',\n",
    "                'head0_AU01': 'bool',\n",
    "                'head0_AU05': 'bool',\n",
    "                'head0_AU20': 'bool',\n",
    "                'head0_AU06': 'bool',\n",
    "                'head0_AU23': 'bool',\n",
    "                'head0_AU07': 'bool',\n",
    "                'head0_AU02': 'bool', \n",
    "                'head0_AU09': 'bool',\n",
    "                'head0_AU17': 'bool',\n",
    "                'head0_AU10': 'bool',\n",
    "                'head0_AU04': 'bool',\n",
    "                'head0_AU12': 'bool',\n",
    "                'head0_AU14': 'bool',\n",
    "                'head0_AU15': 'bool',\n",
    "                'ClassP2':'bool'})\n",
    "varibale_names = df.feature_names \n",
    "le = LabelEncoder()\n",
    "# df['head0_pose'] = le.fit_transform(df['head0_pose'])\n",
    "df['head0_pose'] = le.fit_transform(df['head0_pose'])\n",
    "# df['head2_pose'] = le.fit_transform(df['head2_pose'])\n",
    "# df['head0_emo'] = le.fit_transform(df['head0_emo'])\n",
    "df['head0_emo'] = le.fit_transform(df['head0_emo'])\n",
    "# df['head2_emo'] = le.fit_transform(df['head2_emo'])\n",
    "\n",
    "y = df['ClassP2']\n",
    "x = df.drop(['ClassP2'], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size= 0.25, random_state=27)\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imp = imp.fit(x_train)\n",
    "\n",
    "X_train_imp = imp.transform(x_train)\n",
    "X_test_imp = imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=300, alpha=0.0001, activation='tanh',\n",
    "                     solver='adam', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.66313160\n",
      "Iteration 2, loss = 0.63630756\n",
      "Iteration 3, loss = 0.60316658\n",
      "Iteration 4, loss = 0.58630414\n",
      "Iteration 5, loss = 0.56063088\n",
      "Iteration 6, loss = 0.55188695\n",
      "Iteration 7, loss = 0.52971970\n",
      "Iteration 8, loss = 0.51903886\n",
      "Iteration 9, loss = 0.50699892\n",
      "Iteration 10, loss = 0.49877028\n",
      "Iteration 11, loss = 0.48918708\n",
      "Iteration 12, loss = 0.48662481\n",
      "Iteration 13, loss = 0.47907372\n",
      "Iteration 14, loss = 0.47434132\n",
      "Iteration 15, loss = 0.46556669\n",
      "Iteration 16, loss = 0.45932368\n",
      "Iteration 17, loss = 0.45397285\n",
      "Iteration 18, loss = 0.45123703\n",
      "Iteration 19, loss = 0.44323909\n",
      "Iteration 20, loss = 0.43956243\n",
      "Iteration 21, loss = 0.43361364\n",
      "Iteration 22, loss = 0.43156074\n",
      "Iteration 23, loss = 0.42292727\n",
      "Iteration 24, loss = 0.41987173\n",
      "Iteration 25, loss = 0.41284294\n",
      "Iteration 26, loss = 0.40671693\n",
      "Iteration 27, loss = 0.40064212\n",
      "Iteration 28, loss = 0.39524330\n",
      "Iteration 29, loss = 0.38975847\n",
      "Iteration 30, loss = 0.38399987\n",
      "Iteration 31, loss = 0.37798904\n",
      "Iteration 32, loss = 0.37252712\n",
      "Iteration 33, loss = 0.36655832\n",
      "Iteration 34, loss = 0.35936664\n",
      "Iteration 35, loss = 0.35122188\n",
      "Iteration 36, loss = 0.34717730\n",
      "Iteration 37, loss = 0.34127774\n",
      "Iteration 38, loss = 0.33476387\n",
      "Iteration 39, loss = 0.33144774\n",
      "Iteration 40, loss = 0.32131971\n",
      "Iteration 41, loss = 0.31337729\n",
      "Iteration 42, loss = 0.30743687\n",
      "Iteration 43, loss = 0.30479715\n",
      "Iteration 44, loss = 0.29548250\n",
      "Iteration 45, loss = 0.28615310\n",
      "Iteration 46, loss = 0.28651117\n",
      "Iteration 47, loss = 0.27522076\n",
      "Iteration 48, loss = 0.26899777\n",
      "Iteration 49, loss = 0.26177470\n",
      "Iteration 50, loss = 0.25699525\n",
      "Iteration 51, loss = 0.25419821\n",
      "Iteration 52, loss = 0.24819847\n",
      "Iteration 53, loss = 0.24479596\n",
      "Iteration 54, loss = 0.23486340\n",
      "Iteration 55, loss = 0.22771037\n",
      "Iteration 56, loss = 0.22888730\n",
      "Iteration 57, loss = 0.21982012\n",
      "Iteration 58, loss = 0.21524771\n",
      "Iteration 59, loss = 0.20979910\n",
      "Iteration 60, loss = 0.20665802\n",
      "Iteration 61, loss = 0.20687144\n",
      "Iteration 62, loss = 0.19868704\n",
      "Iteration 63, loss = 0.19143181\n",
      "Iteration 64, loss = 0.18877141\n",
      "Iteration 65, loss = 0.18278836\n",
      "Iteration 66, loss = 0.17858907\n",
      "Iteration 67, loss = 0.17539194\n",
      "Iteration 68, loss = 0.17183973\n",
      "Iteration 69, loss = 0.16839429\n",
      "Iteration 70, loss = 0.16486813\n",
      "Iteration 71, loss = 0.16284263\n",
      "Iteration 72, loss = 0.15647525\n",
      "Iteration 73, loss = 0.15585920\n",
      "Iteration 74, loss = 0.15172328\n",
      "Iteration 75, loss = 0.15806729\n",
      "Iteration 76, loss = 0.16558377\n",
      "Iteration 77, loss = 0.15369687\n",
      "Iteration 78, loss = 0.14413272\n",
      "Iteration 79, loss = 0.13708079\n",
      "Iteration 80, loss = 0.13323378\n",
      "Iteration 81, loss = 0.13211191\n",
      "Iteration 82, loss = 0.13177617\n",
      "Iteration 83, loss = 0.12928435\n",
      "Iteration 84, loss = 0.14326619\n",
      "Iteration 85, loss = 0.13679349\n",
      "Iteration 86, loss = 0.13624694\n",
      "Iteration 87, loss = 0.12668688\n",
      "Iteration 88, loss = 0.11978093\n",
      "Iteration 89, loss = 0.11548314\n",
      "Iteration 90, loss = 0.11707833\n",
      "Iteration 91, loss = 0.12029469\n",
      "Iteration 92, loss = 0.11362446\n",
      "Iteration 93, loss = 0.11538170\n",
      "Iteration 94, loss = 0.10328195\n",
      "Iteration 95, loss = 0.10153676\n",
      "Iteration 96, loss = 0.09992045\n",
      "Iteration 97, loss = 0.09031486\n",
      "Iteration 98, loss = 0.08916876\n",
      "Iteration 99, loss = 0.08641491\n",
      "Iteration 100, loss = 0.08728313\n",
      "Iteration 101, loss = 0.08391805\n",
      "Iteration 102, loss = 0.08301692\n",
      "Iteration 103, loss = 0.08079150\n",
      "Iteration 104, loss = 0.07848207\n",
      "Iteration 105, loss = 0.07604862\n",
      "Iteration 106, loss = 0.07743323\n",
      "Iteration 107, loss = 0.07434102\n",
      "Iteration 108, loss = 0.07308877\n",
      "Iteration 109, loss = 0.07277441\n",
      "Iteration 110, loss = 0.07718848\n",
      "Iteration 111, loss = 0.07244708\n",
      "Iteration 112, loss = 0.06760799\n",
      "Iteration 113, loss = 0.06613151\n",
      "Iteration 114, loss = 0.06362436\n",
      "Iteration 115, loss = 0.05975994\n",
      "Iteration 116, loss = 0.05819883\n",
      "Iteration 117, loss = 0.05615199\n",
      "Iteration 118, loss = 0.05582011\n",
      "Iteration 119, loss = 0.05447551\n",
      "Iteration 120, loss = 0.05251327\n",
      "Iteration 121, loss = 0.05005785\n",
      "Iteration 122, loss = 0.05234912\n",
      "Iteration 123, loss = 0.04883990\n",
      "Iteration 124, loss = 0.04651848\n",
      "Iteration 125, loss = 0.04646762\n",
      "Iteration 126, loss = 0.04523227\n",
      "Iteration 127, loss = 0.04331996\n",
      "Iteration 128, loss = 0.04309540\n",
      "Iteration 129, loss = 0.04145730\n",
      "Iteration 130, loss = 0.04010531\n",
      "Iteration 131, loss = 0.03948549\n",
      "Iteration 132, loss = 0.03791529\n",
      "Iteration 133, loss = 0.03729522\n",
      "Iteration 134, loss = 0.03620144\n",
      "Iteration 135, loss = 0.03680821\n",
      "Iteration 136, loss = 0.03611964\n",
      "Iteration 137, loss = 0.03343251\n",
      "Iteration 138, loss = 0.03434245\n",
      "Iteration 139, loss = 0.03225047\n",
      "Iteration 140, loss = 0.03153420\n",
      "Iteration 141, loss = 0.03114574\n",
      "Iteration 142, loss = 0.02873734\n",
      "Iteration 143, loss = 0.02824804\n",
      "Iteration 144, loss = 0.02769329\n",
      "Iteration 145, loss = 0.02712101\n",
      "Iteration 146, loss = 0.02639285\n",
      "Iteration 147, loss = 0.02629167\n",
      "Iteration 148, loss = 0.02477469\n",
      "Iteration 149, loss = 0.02342679\n",
      "Iteration 150, loss = 0.02465475\n",
      "Iteration 151, loss = 0.02334947\n",
      "Iteration 152, loss = 0.02194497\n",
      "Iteration 153, loss = 0.02211891\n",
      "Iteration 154, loss = 0.02137078\n",
      "Iteration 155, loss = 0.02074879\n",
      "Iteration 156, loss = 0.02003248\n",
      "Iteration 157, loss = 0.01963490\n",
      "Iteration 158, loss = 0.01895928\n",
      "Iteration 159, loss = 0.01894471\n",
      "Iteration 160, loss = 0.01818288\n",
      "Iteration 161, loss = 0.01758577\n",
      "Iteration 162, loss = 0.01789284\n",
      "Iteration 163, loss = 0.01767145\n",
      "Iteration 164, loss = 0.01749390\n",
      "Iteration 165, loss = 0.01833926\n",
      "Iteration 166, loss = 0.01703693\n",
      "Iteration 167, loss = 0.01558935\n",
      "Iteration 168, loss = 0.01638481\n",
      "Iteration 169, loss = 0.01654051\n",
      "Iteration 170, loss = 0.01649932\n",
      "Iteration 171, loss = 0.01690894\n",
      "Iteration 172, loss = 0.01408816\n",
      "Iteration 173, loss = 0.01492805\n",
      "Iteration 174, loss = 0.01322033\n",
      "Iteration 175, loss = 0.01282315\n",
      "Iteration 176, loss = 0.01259047\n",
      "Iteration 177, loss = 0.01262431\n",
      "Iteration 178, loss = 0.01209995\n",
      "Iteration 179, loss = 0.01214541\n",
      "Iteration 180, loss = 0.01175442\n",
      "Iteration 181, loss = 0.01102661\n",
      "Iteration 182, loss = 0.01133060\n",
      "Iteration 183, loss = 0.01157737\n",
      "Iteration 184, loss = 0.01103108\n",
      "Iteration 185, loss = 0.01017124\n",
      "Iteration 186, loss = 0.01119107\n",
      "Iteration 187, loss = 0.01065825\n",
      "Iteration 188, loss = 0.00959497\n",
      "Iteration 189, loss = 0.00981325\n",
      "Iteration 190, loss = 0.00992596\n",
      "Iteration 191, loss = 0.00991376\n",
      "Iteration 192, loss = 0.00899633\n",
      "Iteration 193, loss = 0.00955302\n",
      "Iteration 194, loss = 0.00868329\n",
      "Iteration 195, loss = 0.00827995\n",
      "Iteration 196, loss = 0.00847705\n",
      "Iteration 197, loss = 0.00800543\n",
      "Iteration 198, loss = 0.00774511\n",
      "Iteration 199, loss = 0.00768718\n",
      "Iteration 200, loss = 0.00745971\n",
      "Iteration 201, loss = 0.00714700\n",
      "Iteration 202, loss = 0.00706114\n",
      "Iteration 203, loss = 0.00684388\n",
      "Iteration 204, loss = 0.00677711\n",
      "Iteration 205, loss = 0.00666711\n",
      "Iteration 206, loss = 0.00658029\n",
      "Iteration 207, loss = 0.00654826\n",
      "Iteration 208, loss = 0.00632392\n",
      "Iteration 209, loss = 0.00631773\n",
      "Iteration 210, loss = 0.00613324\n",
      "Iteration 211, loss = 0.00600175\n",
      "Iteration 212, loss = 0.00601864\n",
      "Iteration 213, loss = 0.00597942\n",
      "Iteration 214, loss = 0.00564525\n",
      "Iteration 215, loss = 0.00558400\n",
      "Iteration 216, loss = 0.00550830\n",
      "Iteration 217, loss = 0.00548668\n",
      "Iteration 218, loss = 0.00545194\n",
      "Iteration 219, loss = 0.00522034\n",
      "Iteration 220, loss = 0.00520119\n",
      "Iteration 221, loss = 0.00501674\n",
      "Iteration 222, loss = 0.00493127\n",
      "Iteration 223, loss = 0.00493499\n",
      "Iteration 224, loss = 0.00476038\n",
      "Iteration 225, loss = 0.00476382\n",
      "Iteration 226, loss = 0.00467517\n",
      "Iteration 227, loss = 0.00455543\n",
      "Iteration 228, loss = 0.00447890\n",
      "Iteration 229, loss = 0.00442053\n",
      "Iteration 230, loss = 0.00432417\n",
      "Iteration 231, loss = 0.00427444\n",
      "Iteration 232, loss = 0.00432471\n",
      "Iteration 233, loss = 0.00416347\n",
      "Iteration 234, loss = 0.00409831\n",
      "Iteration 235, loss = 0.00401341\n",
      "Iteration 236, loss = 0.00397709\n",
      "Iteration 237, loss = 0.00390923\n",
      "Iteration 238, loss = 0.00387012\n",
      "Iteration 239, loss = 0.00386171\n",
      "Iteration 240, loss = 0.00383288\n",
      "Iteration 241, loss = 0.00385576\n",
      "Iteration 242, loss = 0.00387056\n",
      "Iteration 243, loss = 0.00380833\n",
      "Iteration 244, loss = 0.00358941\n",
      "Iteration 245, loss = 0.00350150\n",
      "Iteration 246, loss = 0.00340124\n",
      "Iteration 247, loss = 0.00343152\n",
      "Iteration 248, loss = 0.00332760\n",
      "Iteration 249, loss = 0.00333608\n",
      "Iteration 250, loss = 0.00331491\n",
      "Iteration 251, loss = 0.00321045\n",
      "Iteration 252, loss = 0.00317168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.00314008\n",
      "Iteration 254, loss = 0.00308483\n",
      "Iteration 255, loss = 0.00302706\n",
      "Iteration 256, loss = 0.00300674\n",
      "Iteration 257, loss = 0.00297180\n",
      "Iteration 258, loss = 0.00292962\n",
      "Iteration 259, loss = 0.00290563\n",
      "Iteration 260, loss = 0.00287469\n",
      "Iteration 261, loss = 0.00283221\n",
      "Iteration 262, loss = 0.00277641\n",
      "Iteration 263, loss = 0.00279250\n",
      "Iteration 264, loss = 0.00275780\n",
      "Iteration 265, loss = 0.00279772\n",
      "Iteration 266, loss = 0.00263133\n",
      "Iteration 267, loss = 0.00270851\n",
      "Iteration 268, loss = 0.00258736\n",
      "Iteration 269, loss = 0.00259818\n",
      "Iteration 270, loss = 0.00250813\n",
      "Iteration 271, loss = 0.00252183\n",
      "Iteration 272, loss = 0.00246542\n",
      "Iteration 273, loss = 0.00242710\n",
      "Iteration 274, loss = 0.00245551\n",
      "Iteration 275, loss = 0.00238956\n",
      "Iteration 276, loss = 0.00235438\n",
      "Iteration 277, loss = 0.00233465\n",
      "Iteration 278, loss = 0.00230923\n",
      "Iteration 279, loss = 0.00226937\n",
      "Iteration 280, loss = 0.00226314\n",
      "Iteration 281, loss = 0.00221714\n",
      "Iteration 282, loss = 0.00219200\n",
      "Iteration 283, loss = 0.00216486\n",
      "Iteration 284, loss = 0.00215269\n",
      "Iteration 285, loss = 0.00211777\n",
      "Iteration 286, loss = 0.00209515\n",
      "Iteration 287, loss = 0.00208877\n",
      "Iteration 288, loss = 0.00206148\n",
      "Iteration 289, loss = 0.00203728\n",
      "Iteration 290, loss = 0.00204597\n",
      "Iteration 291, loss = 0.00201382\n",
      "Iteration 292, loss = 0.00199676\n",
      "Iteration 293, loss = 0.00195940\n",
      "Iteration 294, loss = 0.00193575\n",
      "Iteration 295, loss = 0.00191436\n",
      "Iteration 296, loss = 0.00191329\n",
      "Iteration 297, loss = 0.00188685\n",
      "Iteration 298, loss = 0.00188226\n",
      "Iteration 299, loss = 0.00184851\n",
      "Iteration 300, loss = 0.00184869\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train_imp, y_train)\n",
    "y_pred = clf.predict(X_test_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8163265306122449\n",
      "[[ 51  21]\n",
      " [ 15 109]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.77      0.71      0.74        72\n",
      "        True       0.84      0.88      0.86       124\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       196\n",
      "   macro avg       0.81      0.79      0.80       196\n",
      "weighted avg       0.81      0.82      0.81       196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
