{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.decomposition import FactorAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/joe/MLP/features_sample_head0_selection.csv\")\n",
    "df = df.astype({'Class_P0': 'bool'})\n",
    "#                 'head1_AU28': 'bool',\n",
    "#                 'head1_AU45': 'bool',\n",
    "#                 'head1_AU26': 'bool',\n",
    "#                 'head1_AU25': 'bool',\n",
    "#                 'head1_AU01': 'bool',\n",
    "#                 'head1_AU05': 'bool',\n",
    "#                 'head1_AU20': 'bool',\n",
    "#                 'head1_AU06': 'bool',\n",
    "#                 'head1_AU23': 'bool',\n",
    "#                 'head1_AU07': 'bool',\n",
    "#                 'head1_AU02': 'bool', \n",
    "#                 'head1_AU09': 'bool',\n",
    "#                 'head1_AU17': 'bool',\n",
    "#                 'head1_AU10': 'bool',\n",
    "#                 'head1_AU04': 'bool',\n",
    "#                 'head1_AU12': 'bool',\n",
    "#                 'head1_AU14': 'bool',\n",
    "#                 'head1_AU15': 'bool'})\n",
    "# le = LabelEncoder()\n",
    "# df['head0_pose'] = le.fit_transform(df['head0_pose'])\n",
    "# df['head1_pose'] = le.fit_transform(df['head1_pose'])\n",
    "# df['head2_pose'] = le.fit_transform(df['head2_pose'])\n",
    "# df['head0_emo'] = le.fit_transform(df['head0_emo'])\n",
    "# df['head1_emo'] = le.fit_transform(df['head1_emo'])\n",
    "# df['head2_emo'] = le.fit_transform(df['head2_emo'])\n",
    "\n",
    "y = df['Class_P0']\n",
    "x = df.drop(['Class_P0'], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size= 0.30, random_state=27)\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imp = imp.fit(x_train)\n",
    "\n",
    "X_train_imp = imp.transform(x_train)\n",
    "X_test_imp = imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "head1_head0_trans_x    float64\n",
       "head1_head0_trans_y    float64\n",
       "Unnamed: 2             float64\n",
       "Unnamed: 3             float64\n",
       "Unnamed: 4             float64\n",
       "Unnamed: 5             float64\n",
       "head1_head0_ori_w      float64\n",
       "head1_head1_trans_x    float64\n",
       "head1_head1_ori_x      float64\n",
       "head1_head1_ori_y      float64\n",
       "head1_head2_trans_z    float64\n",
       "head1_AU07             float64\n",
       "Unnamed: 12            float64\n",
       "head1_AU09             float64\n",
       "head1_AU12             float64\n",
       "ClassP1                   bool\n",
       "Class_P0                  bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=300, alpha=0.0001, activation='tanh',\n",
    "                     solver='adam', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection (Skip as it Takes time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfsl = sfs(clf, k_features=10,forward=True)\n",
    "# print(sfsl.fit(X_test_imp, y_test))\n",
    "# print(sfsl.subsets_)\n",
    "# #'feature_names': head1_head0_trans_x\thead1_head0_trans_y\thead1_head0_ori_w\thead1_head1_trans_x\thead1_head1_ori_x\thead1_head1_ori_y\thead1_head2_trans_z\thead1_AU07\thead1_AU09\thead1_AU12\tClassP1\tClass_P0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factore analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.388506</td>\n",
       "      <td>0.087763</td>\n",
       "      <td>0.005932</td>\n",
       "      <td>-0.396647</td>\n",
       "      <td>-0.015126</td>\n",
       "      <td>0.117161</td>\n",
       "      <td>-0.051744</td>\n",
       "      <td>-0.123371</td>\n",
       "      <td>0.011002</td>\n",
       "      <td>0.007325</td>\n",
       "      <td>-0.071395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.056144</td>\n",
       "      <td>-0.071599</td>\n",
       "      <td>-0.029599</td>\n",
       "      <td>-0.057255</td>\n",
       "      <td>0.039622</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.005869</td>\n",
       "      <td>-0.109926</td>\n",
       "      <td>-0.040776</td>\n",
       "      <td>-0.032834</td>\n",
       "      <td>-0.066033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.067947</td>\n",
       "      <td>0.011947</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.048567</td>\n",
       "      <td>0.009669</td>\n",
       "      <td>-0.013527</td>\n",
       "      <td>-0.052712</td>\n",
       "      <td>-0.017093</td>\n",
       "      <td>0.006009</td>\n",
       "      <td>-0.071062</td>\n",
       "      <td>0.043702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.029889</td>\n",
       "      <td>-0.004201</td>\n",
       "      <td>-0.001778</td>\n",
       "      <td>-0.006503</td>\n",
       "      <td>0.006608</td>\n",
       "      <td>-0.003135</td>\n",
       "      <td>-0.011832</td>\n",
       "      <td>0.056646</td>\n",
       "      <td>0.055773</td>\n",
       "      <td>0.013911</td>\n",
       "      <td>-0.050988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.025074</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>-0.001575</td>\n",
       "      <td>0.003273</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>-0.000907</td>\n",
       "      <td>0.015107</td>\n",
       "      <td>-0.020778</td>\n",
       "      <td>0.024504</td>\n",
       "      <td>0.030926</td>\n",
       "      <td>0.053669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0  -0.388506  0.087763  0.005932 -0.396647 -0.015126  0.117161 -0.051744   \n",
       "1  -0.056144 -0.071599 -0.029599 -0.057255  0.039622  0.014710  0.005869   \n",
       "2  -0.067947  0.011947  0.000442  0.048567  0.009669 -0.013527 -0.052712   \n",
       "3  -0.029889 -0.004201 -0.001778 -0.006503  0.006608 -0.003135 -0.011832   \n",
       "4  -0.025074  0.000669 -0.001575  0.003273  0.002489 -0.000907  0.015107   \n",
       "5   0.000000 -0.000000 -0.000000 -0.000000 -0.000000  0.000000  0.000000   \n",
       "6   0.000000 -0.000000 -0.000000 -0.000000 -0.000000  0.000000 -0.000000   \n",
       "7   0.000000 -0.000000  0.000000 -0.000000  0.000000  0.000000 -0.000000   \n",
       "8  -0.000000  0.000000 -0.000000  0.000000 -0.000000 -0.000000  0.000000   \n",
       "9   0.000000  0.000000 -0.000000  0.000000  0.000000  0.000000 -0.000000   \n",
       "10 -0.000000 -0.000000  0.000000  0.000000 -0.000000  0.000000  0.000000   \n",
       "\n",
       "          7         8         9         10  \n",
       "0  -0.123371  0.011002  0.007325 -0.071395  \n",
       "1  -0.109926 -0.040776 -0.032834 -0.066033  \n",
       "2  -0.017093  0.006009 -0.071062  0.043702  \n",
       "3   0.056646  0.055773  0.013911 -0.050988  \n",
       "4  -0.020778  0.024504  0.030926  0.053669  \n",
       "5   0.000000  0.000000 -0.000000  0.000000  \n",
       "6  -0.000000  0.000000  0.000000  0.000000  \n",
       "7   0.000000 -0.000000 -0.000000  0.000000  \n",
       "8   0.000000 -0.000000  0.000000  0.000000  \n",
       "9   0.000000  0.000000 -0.000000  0.000000  \n",
       "10 -0.000000 -0.000000  0.000000 -0.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factore = FactorAnalysis().fit(X_train_imp)\n",
    "pd.DataFrame(factore.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.62267323\n",
      "Iteration 2, loss = 0.59164631\n",
      "Iteration 3, loss = 0.57340501\n",
      "Iteration 4, loss = 0.56795669\n",
      "Iteration 5, loss = 0.56122511\n",
      "Iteration 6, loss = 0.55591731\n",
      "Iteration 7, loss = 0.55054527\n",
      "Iteration 8, loss = 0.54576695\n",
      "Iteration 9, loss = 0.54280397\n",
      "Iteration 10, loss = 0.53962543\n",
      "Iteration 11, loss = 0.53612381\n",
      "Iteration 12, loss = 0.53291764\n",
      "Iteration 13, loss = 0.53041010\n",
      "Iteration 14, loss = 0.52699700\n",
      "Iteration 15, loss = 0.52065848\n",
      "Iteration 16, loss = 0.51574210\n",
      "Iteration 17, loss = 0.50865140\n",
      "Iteration 18, loss = 0.50109449\n",
      "Iteration 19, loss = 0.49262554\n",
      "Iteration 20, loss = 0.48243906\n",
      "Iteration 21, loss = 0.47315107\n",
      "Iteration 22, loss = 0.46109615\n",
      "Iteration 23, loss = 0.44825852\n",
      "Iteration 24, loss = 0.43447873\n",
      "Iteration 25, loss = 0.42074495\n",
      "Iteration 26, loss = 0.40843114\n",
      "Iteration 27, loss = 0.39302677\n",
      "Iteration 28, loss = 0.38030792\n",
      "Iteration 29, loss = 0.36867777\n",
      "Iteration 30, loss = 0.35617889\n",
      "Iteration 31, loss = 0.34479597\n",
      "Iteration 32, loss = 0.33540188\n",
      "Iteration 33, loss = 0.32662472\n",
      "Iteration 34, loss = 0.32070909\n",
      "Iteration 35, loss = 0.31222708\n",
      "Iteration 36, loss = 0.30751696\n",
      "Iteration 37, loss = 0.30039099\n",
      "Iteration 38, loss = 0.30021553\n",
      "Iteration 39, loss = 0.29566042\n",
      "Iteration 40, loss = 0.28959341\n",
      "Iteration 41, loss = 0.28645770\n",
      "Iteration 42, loss = 0.28346276\n",
      "Iteration 43, loss = 0.28399793\n",
      "Iteration 44, loss = 0.27799084\n",
      "Iteration 45, loss = 0.27604345\n",
      "Iteration 46, loss = 0.27322984\n",
      "Iteration 47, loss = 0.27082936\n",
      "Iteration 48, loss = 0.26824726\n",
      "Iteration 49, loss = 0.26719368\n",
      "Iteration 50, loss = 0.26404581\n",
      "Iteration 51, loss = 0.26473584\n",
      "Iteration 52, loss = 0.25950047\n",
      "Iteration 53, loss = 0.26240052\n",
      "Iteration 54, loss = 0.25844399\n",
      "Iteration 55, loss = 0.25967917\n",
      "Iteration 56, loss = 0.25651305\n",
      "Iteration 57, loss = 0.25687444\n",
      "Iteration 58, loss = 0.25446719\n",
      "Iteration 59, loss = 0.25285536\n",
      "Iteration 60, loss = 0.25117889\n",
      "Iteration 61, loss = 0.25231091\n",
      "Iteration 62, loss = 0.25126136\n",
      "Iteration 63, loss = 0.25246168\n",
      "Iteration 64, loss = 0.24808728\n",
      "Iteration 65, loss = 0.25222348\n",
      "Iteration 66, loss = 0.24784883\n",
      "Iteration 67, loss = 0.24796979\n",
      "Iteration 68, loss = 0.24811888\n",
      "Iteration 69, loss = 0.24545897\n",
      "Iteration 70, loss = 0.24765623\n",
      "Iteration 71, loss = 0.24944242\n",
      "Iteration 72, loss = 0.24535231\n",
      "Iteration 73, loss = 0.24481847\n",
      "Iteration 74, loss = 0.24371576\n",
      "Iteration 75, loss = 0.24574860\n",
      "Iteration 76, loss = 0.24702163\n",
      "Iteration 77, loss = 0.24258020\n",
      "Iteration 78, loss = 0.24802483\n",
      "Iteration 79, loss = 0.24526806\n",
      "Iteration 80, loss = 0.24329405\n",
      "Iteration 81, loss = 0.23947666\n",
      "Iteration 82, loss = 0.24439038\n",
      "Iteration 83, loss = 0.23996587\n",
      "Iteration 84, loss = 0.24146692\n",
      "Iteration 85, loss = 0.24094140\n",
      "Iteration 86, loss = 0.23931046\n",
      "Iteration 87, loss = 0.24062866\n",
      "Iteration 88, loss = 0.23715269\n",
      "Iteration 89, loss = 0.23925425\n",
      "Iteration 90, loss = 0.23620097\n",
      "Iteration 91, loss = 0.23976300\n",
      "Iteration 92, loss = 0.23725899\n",
      "Iteration 93, loss = 0.23631606\n",
      "Iteration 94, loss = 0.23659056\n",
      "Iteration 95, loss = 0.23589093\n",
      "Iteration 96, loss = 0.23582462\n",
      "Iteration 97, loss = 0.23605954\n",
      "Iteration 98, loss = 0.23745552\n",
      "Iteration 99, loss = 0.23459176\n",
      "Iteration 100, loss = 0.23497076\n",
      "Iteration 101, loss = 0.23700367\n",
      "Iteration 102, loss = 0.23325798\n",
      "Iteration 103, loss = 0.23588367\n",
      "Iteration 104, loss = 0.23247576\n",
      "Iteration 105, loss = 0.23394116\n",
      "Iteration 106, loss = 0.23156651\n",
      "Iteration 107, loss = 0.23240833\n",
      "Iteration 108, loss = 0.23066794\n",
      "Iteration 109, loss = 0.23618929\n",
      "Iteration 110, loss = 0.23954115\n",
      "Iteration 111, loss = 0.23178866\n",
      "Iteration 112, loss = 0.23934917\n",
      "Iteration 113, loss = 0.22912614\n",
      "Iteration 114, loss = 0.23906788\n",
      "Iteration 115, loss = 0.22886898\n",
      "Iteration 116, loss = 0.23567715\n",
      "Iteration 117, loss = 0.22683952\n",
      "Iteration 118, loss = 0.23243305\n",
      "Iteration 119, loss = 0.23173947\n",
      "Iteration 120, loss = 0.22999535\n",
      "Iteration 121, loss = 0.22905102\n",
      "Iteration 122, loss = 0.22685075\n",
      "Iteration 123, loss = 0.22789727\n",
      "Iteration 124, loss = 0.22663192\n",
      "Iteration 125, loss = 0.22838167\n",
      "Iteration 126, loss = 0.22956607\n",
      "Iteration 127, loss = 0.23070608\n",
      "Iteration 128, loss = 0.22530477\n",
      "Iteration 129, loss = 0.22491435\n",
      "Iteration 130, loss = 0.22461844\n",
      "Iteration 131, loss = 0.22442997\n",
      "Iteration 132, loss = 0.22487090\n",
      "Iteration 133, loss = 0.22433023\n",
      "Iteration 134, loss = 0.22407114\n",
      "Iteration 135, loss = 0.22481310\n",
      "Iteration 136, loss = 0.22475304\n",
      "Iteration 137, loss = 0.22398585\n",
      "Iteration 138, loss = 0.22542488\n",
      "Iteration 139, loss = 0.22491445\n",
      "Iteration 140, loss = 0.22868222\n",
      "Iteration 141, loss = 0.22526116\n",
      "Iteration 142, loss = 0.22518726\n",
      "Iteration 143, loss = 0.22192030\n",
      "Iteration 144, loss = 0.22084763\n",
      "Iteration 145, loss = 0.21980539\n",
      "Iteration 146, loss = 0.21954919\n",
      "Iteration 147, loss = 0.21934916\n",
      "Iteration 148, loss = 0.21963396\n",
      "Iteration 149, loss = 0.21840235\n",
      "Iteration 150, loss = 0.22121961\n",
      "Iteration 151, loss = 0.21808169\n",
      "Iteration 152, loss = 0.22150533\n",
      "Iteration 153, loss = 0.22057262\n",
      "Iteration 154, loss = 0.21885545\n",
      "Iteration 155, loss = 0.21697037\n",
      "Iteration 156, loss = 0.22274313\n",
      "Iteration 157, loss = 0.21680732\n",
      "Iteration 158, loss = 0.21828448\n",
      "Iteration 159, loss = 0.21974638\n",
      "Iteration 160, loss = 0.21616086\n",
      "Iteration 161, loss = 0.21720969\n",
      "Iteration 162, loss = 0.21551187\n",
      "Iteration 163, loss = 0.21530115\n",
      "Iteration 164, loss = 0.21443893\n",
      "Iteration 165, loss = 0.21376021\n",
      "Iteration 166, loss = 0.21378951\n",
      "Iteration 167, loss = 0.21469530\n",
      "Iteration 168, loss = 0.21312585\n",
      "Iteration 169, loss = 0.21358646\n",
      "Iteration 170, loss = 0.21383576\n",
      "Iteration 171, loss = 0.21347998\n",
      "Iteration 172, loss = 0.21314079\n",
      "Iteration 173, loss = 0.21173020\n",
      "Iteration 174, loss = 0.21299264\n",
      "Iteration 175, loss = 0.21337737\n",
      "Iteration 176, loss = 0.21158760\n",
      "Iteration 177, loss = 0.21293085\n",
      "Iteration 178, loss = 0.21214954\n",
      "Iteration 179, loss = 0.21043017\n",
      "Iteration 180, loss = 0.20997848\n",
      "Iteration 181, loss = 0.20969655\n",
      "Iteration 182, loss = 0.20956401\n",
      "Iteration 183, loss = 0.20859333\n",
      "Iteration 184, loss = 0.20895783\n",
      "Iteration 185, loss = 0.20885685\n",
      "Iteration 186, loss = 0.20760966\n",
      "Iteration 187, loss = 0.20755950\n",
      "Iteration 188, loss = 0.20964370\n",
      "Iteration 189, loss = 0.20803965\n",
      "Iteration 190, loss = 0.21030775\n",
      "Iteration 191, loss = 0.20602557\n",
      "Iteration 192, loss = 0.20610922\n",
      "Iteration 193, loss = 0.20713490\n",
      "Iteration 194, loss = 0.20850637\n",
      "Iteration 195, loss = 0.20659658\n",
      "Iteration 196, loss = 0.20723194\n",
      "Iteration 197, loss = 0.20596195\n",
      "Iteration 198, loss = 0.20751832\n",
      "Iteration 199, loss = 0.20544250\n",
      "Iteration 200, loss = 0.20623476\n",
      "Iteration 201, loss = 0.20610616\n",
      "Iteration 202, loss = 0.20354193\n",
      "Iteration 203, loss = 0.20601986\n",
      "Iteration 204, loss = 0.20614722\n",
      "Iteration 205, loss = 0.20493941\n",
      "Iteration 206, loss = 0.20148174\n",
      "Iteration 207, loss = 0.20276937\n",
      "Iteration 208, loss = 0.20221522\n",
      "Iteration 209, loss = 0.20375388\n",
      "Iteration 210, loss = 0.20197092\n",
      "Iteration 211, loss = 0.20161979\n",
      "Iteration 212, loss = 0.20186997\n",
      "Iteration 213, loss = 0.20011343\n",
      "Iteration 214, loss = 0.19934627\n",
      "Iteration 215, loss = 0.20002207\n",
      "Iteration 216, loss = 0.20026304\n",
      "Iteration 217, loss = 0.20088283\n",
      "Iteration 218, loss = 0.19796361\n",
      "Iteration 219, loss = 0.20004177\n",
      "Iteration 220, loss = 0.19788659\n",
      "Iteration 221, loss = 0.19825955\n",
      "Iteration 222, loss = 0.19741999\n",
      "Iteration 223, loss = 0.19590978\n",
      "Iteration 224, loss = 0.19904124\n",
      "Iteration 225, loss = 0.19696463\n",
      "Iteration 226, loss = 0.20086083\n",
      "Iteration 227, loss = 0.19996349\n",
      "Iteration 228, loss = 0.19652519\n",
      "Iteration 229, loss = 0.19599116\n",
      "Iteration 230, loss = 0.19506800\n",
      "Iteration 231, loss = 0.19350272\n",
      "Iteration 232, loss = 0.19622003\n",
      "Iteration 233, loss = 0.19317913\n",
      "Iteration 234, loss = 0.19461791\n",
      "Iteration 235, loss = 0.19312453\n",
      "Iteration 236, loss = 0.19413195\n",
      "Iteration 237, loss = 0.19303606\n",
      "Iteration 238, loss = 0.19264624\n",
      "Iteration 239, loss = 0.19277912\n",
      "Iteration 240, loss = 0.19514506\n",
      "Iteration 241, loss = 0.19159324\n",
      "Iteration 242, loss = 0.19389678\n",
      "Iteration 243, loss = 0.19314092\n",
      "Iteration 244, loss = 0.19289767\n",
      "Iteration 245, loss = 0.19001578\n",
      "Iteration 246, loss = 0.19383369\n",
      "Iteration 247, loss = 0.19028801\n",
      "Iteration 248, loss = 0.19130823\n",
      "Iteration 249, loss = 0.18834451\n",
      "Iteration 250, loss = 0.19200218\n",
      "Iteration 251, loss = 0.18813362\n",
      "Iteration 252, loss = 0.18899135\n",
      "Iteration 253, loss = 0.18754250\n",
      "Iteration 254, loss = 0.18694979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 255, loss = 0.18708513\n",
      "Iteration 256, loss = 0.18658133\n",
      "Iteration 257, loss = 0.18922548\n",
      "Iteration 258, loss = 0.18883779\n",
      "Iteration 259, loss = 0.18781648\n",
      "Iteration 260, loss = 0.18675342\n",
      "Iteration 261, loss = 0.18565299\n",
      "Iteration 262, loss = 0.18533921\n",
      "Iteration 263, loss = 0.18447226\n",
      "Iteration 264, loss = 0.18445728\n",
      "Iteration 265, loss = 0.18415260\n",
      "Iteration 266, loss = 0.18385089\n",
      "Iteration 267, loss = 0.18351705\n",
      "Iteration 268, loss = 0.18322352\n",
      "Iteration 269, loss = 0.18214653\n",
      "Iteration 270, loss = 0.18300482\n",
      "Iteration 271, loss = 0.18363204\n",
      "Iteration 272, loss = 0.18192743\n",
      "Iteration 273, loss = 0.18193245\n",
      "Iteration 274, loss = 0.18092115\n",
      "Iteration 275, loss = 0.18236923\n",
      "Iteration 276, loss = 0.18116372\n",
      "Iteration 277, loss = 0.17891853\n",
      "Iteration 278, loss = 0.18016102\n",
      "Iteration 279, loss = 0.17843205\n",
      "Iteration 280, loss = 0.17851858\n",
      "Iteration 281, loss = 0.17845584\n",
      "Iteration 282, loss = 0.17815492\n",
      "Iteration 283, loss = 0.17775145\n",
      "Iteration 284, loss = 0.17641057\n",
      "Iteration 285, loss = 0.17794583\n",
      "Iteration 286, loss = 0.17698842\n",
      "Iteration 287, loss = 0.17689974\n",
      "Iteration 288, loss = 0.17592031\n",
      "Iteration 289, loss = 0.17607908\n",
      "Iteration 290, loss = 0.17664974\n",
      "Iteration 291, loss = 0.17573686\n",
      "Iteration 292, loss = 0.17452841\n",
      "Iteration 293, loss = 0.17605134\n",
      "Iteration 294, loss = 0.17742151\n",
      "Iteration 295, loss = 0.17727080\n",
      "Iteration 296, loss = 0.17317794\n",
      "Iteration 297, loss = 0.17276824\n",
      "Iteration 298, loss = 0.17165635\n",
      "Iteration 299, loss = 0.17228378\n",
      "Iteration 300, loss = 0.17325623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joe/.local/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train_imp, y_train)\n",
    "y_pred = clf.predict(X_test_imp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8425531914893617\n",
      "[[ 57  21]\n",
      " [ 16 141]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      0.73      0.75        78\n",
      "        True       0.87      0.90      0.88       157\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       235\n",
      "   macro avg       0.83      0.81      0.82       235\n",
      "weighted avg       0.84      0.84      0.84       235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/joe/MLP/features_sample_head1_selection.csv\")\n",
    "df = df.astype({'Class_P0': 'bool', \n",
    "#                 'head0_AU28': 'bool',\n",
    "#                 'head0_AU45': 'bool',\n",
    "#                 'head0_AU26': 'bool',\n",
    "#                 'head0_AU25': 'bool',\n",
    "#                 'head0_AU01': 'bool',\n",
    "#                 'head0_AU05': 'bool',\n",
    "#                 'head0_AU20': 'bool',\n",
    "#                 'head0_AU06': 'bool',\n",
    "#                 'head0_AU23': 'bool',\n",
    "#                 'head0_AU07': 'bool',\n",
    "#                 'head0_AU02': 'bool', \n",
    "#                 'head0_AU09': 'bool',\n",
    "#                 'head0_AU17': 'bool',\n",
    "#                 'head0_AU10': 'bool',\n",
    "#                 'head0_AU04': 'bool',\n",
    "#                 'head0_AU12': 'bool',\n",
    "#                 'head0_AU14': 'bool',\n",
    "#                 'head0_AU15': 'bool',\n",
    "                'ClassP1':'bool'})\n",
    "\n",
    "le = LabelEncoder()\n",
    "# df['head0_pose'] = le.fit_transform(df['head0_pose'])\n",
    "# df['head0_pose'] = le.fit_transform(df['head0_pose'])\n",
    "# df['head2_pose'] = le.fit_transform(df['head2_pose'])\n",
    "# df['head0_emo'] = le.fit_transform(df['head0_emo'])\n",
    "# df['head0_emo'] = le.fit_transform(df['head0_emo'])\n",
    "# df['head2_emo'] = le.fit_transform(df['head2_emo'])\n",
    "\n",
    "y = df['ClassP1']\n",
    "x = df.drop(['ClassP1'], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size= 0.30, random_state=27)\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imp = imp.fit(x_train)\n",
    "\n",
    "X_train_imp = imp.transform(x_train)\n",
    "X_test_imp = imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "head0_head0_trans_x    float64\n",
       "head0_head0_trans_y    float64\n",
       "head0_head0_ori_y      float64\n",
       "head0_head0_ori_w      float64\n",
       "head0_head1_trans_x    float64\n",
       "head0_head1_trans_z    float64\n",
       "head0_head1_ori_y      float64\n",
       "head0_head2_trans_x    float64\n",
       "head0_head2_ori_x      float64\n",
       "head0_AU26             float64\n",
       "Class_P0                  bool\n",
       "ClassP2                   bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=300, alpha=0.0001, activation='tanh',\n",
    "                     solver='adam', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfsl = sfs(clf, k_features=10,forward=True)\n",
    "# print(sfsl.fit(X_test_imp, y_test))\n",
    "# print(sfsl.subsets_)\n",
    "# ('0', '7', '14', '16', '18', '20', '22', '27', '28', '32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65825207\n",
      "Iteration 2, loss = 0.53385928\n",
      "Iteration 3, loss = 0.54384695\n",
      "Iteration 4, loss = 0.52560267\n",
      "Iteration 5, loss = 0.49881361\n",
      "Iteration 6, loss = 0.49570625\n",
      "Iteration 7, loss = 0.49703455\n",
      "Iteration 8, loss = 0.48516131\n",
      "Iteration 9, loss = 0.47805795\n",
      "Iteration 10, loss = 0.47698585\n",
      "Iteration 11, loss = 0.47402489\n",
      "Iteration 12, loss = 0.46805799\n",
      "Iteration 13, loss = 0.46642760\n",
      "Iteration 14, loss = 0.46523517\n",
      "Iteration 15, loss = 0.46249839\n",
      "Iteration 16, loss = 0.46047774\n",
      "Iteration 17, loss = 0.45990180\n",
      "Iteration 18, loss = 0.45779835\n",
      "Iteration 19, loss = 0.45630455\n",
      "Iteration 20, loss = 0.45400427\n",
      "Iteration 21, loss = 0.45082466\n",
      "Iteration 22, loss = 0.44902884\n",
      "Iteration 23, loss = 0.44553636\n",
      "Iteration 24, loss = 0.44103916\n",
      "Iteration 25, loss = 0.43686816\n",
      "Iteration 26, loss = 0.43298512\n",
      "Iteration 27, loss = 0.42830069\n",
      "Iteration 28, loss = 0.42214901\n",
      "Iteration 29, loss = 0.41564254\n",
      "Iteration 30, loss = 0.40896665\n",
      "Iteration 31, loss = 0.39995987\n",
      "Iteration 32, loss = 0.39191156\n",
      "Iteration 33, loss = 0.38269245\n",
      "Iteration 34, loss = 0.37331781\n",
      "Iteration 35, loss = 0.36292311\n",
      "Iteration 36, loss = 0.35424593\n",
      "Iteration 37, loss = 0.34453940\n",
      "Iteration 38, loss = 0.33451269\n",
      "Iteration 39, loss = 0.32597445\n",
      "Iteration 40, loss = 0.31793868\n",
      "Iteration 41, loss = 0.31079953\n",
      "Iteration 42, loss = 0.30267530\n",
      "Iteration 43, loss = 0.29690380\n",
      "Iteration 44, loss = 0.29052262\n",
      "Iteration 45, loss = 0.28653356\n",
      "Iteration 46, loss = 0.28103207\n",
      "Iteration 47, loss = 0.27657382\n",
      "Iteration 48, loss = 0.27432211\n",
      "Iteration 49, loss = 0.27150263\n",
      "Iteration 50, loss = 0.26698477\n",
      "Iteration 51, loss = 0.26331703\n",
      "Iteration 52, loss = 0.26115399\n",
      "Iteration 53, loss = 0.25989249\n",
      "Iteration 54, loss = 0.25661321\n",
      "Iteration 55, loss = 0.25491250\n",
      "Iteration 56, loss = 0.25309569\n",
      "Iteration 57, loss = 0.25089711\n",
      "Iteration 58, loss = 0.25132183\n",
      "Iteration 59, loss = 0.24801084\n",
      "Iteration 60, loss = 0.24736271\n",
      "Iteration 61, loss = 0.24514535\n",
      "Iteration 62, loss = 0.24383757\n",
      "Iteration 63, loss = 0.24249944\n",
      "Iteration 64, loss = 0.24120412\n",
      "Iteration 65, loss = 0.23953478\n",
      "Iteration 66, loss = 0.23795963\n",
      "Iteration 67, loss = 0.23677747\n",
      "Iteration 68, loss = 0.23534378\n",
      "Iteration 69, loss = 0.23613960\n",
      "Iteration 70, loss = 0.23377055\n",
      "Iteration 71, loss = 0.23284866\n",
      "Iteration 72, loss = 0.23201968\n",
      "Iteration 73, loss = 0.23027089\n",
      "Iteration 74, loss = 0.22850953\n",
      "Iteration 75, loss = 0.22747509\n",
      "Iteration 76, loss = 0.22684143\n",
      "Iteration 77, loss = 0.22567355\n",
      "Iteration 78, loss = 0.22417570\n",
      "Iteration 79, loss = 0.22371609\n",
      "Iteration 80, loss = 0.22331559\n",
      "Iteration 81, loss = 0.22153500\n",
      "Iteration 82, loss = 0.22084751\n",
      "Iteration 83, loss = 0.21873933\n",
      "Iteration 84, loss = 0.22037635\n",
      "Iteration 85, loss = 0.21814839\n",
      "Iteration 86, loss = 0.21652750\n",
      "Iteration 87, loss = 0.21515679\n",
      "Iteration 88, loss = 0.21360586\n",
      "Iteration 89, loss = 0.21354513\n",
      "Iteration 90, loss = 0.21162704\n",
      "Iteration 91, loss = 0.21263647\n",
      "Iteration 92, loss = 0.20997561\n",
      "Iteration 93, loss = 0.21112061\n",
      "Iteration 94, loss = 0.20900304\n",
      "Iteration 95, loss = 0.20700375\n",
      "Iteration 96, loss = 0.20604252\n",
      "Iteration 97, loss = 0.20472894\n",
      "Iteration 98, loss = 0.20288644\n",
      "Iteration 99, loss = 0.20353233\n",
      "Iteration 100, loss = 0.20078560\n",
      "Iteration 101, loss = 0.20065257\n",
      "Iteration 102, loss = 0.20187912\n",
      "Iteration 103, loss = 0.19921416\n",
      "Iteration 104, loss = 0.19740909\n",
      "Iteration 105, loss = 0.19805814\n",
      "Iteration 106, loss = 0.19738879\n",
      "Iteration 107, loss = 0.19360506\n",
      "Iteration 108, loss = 0.19419632\n",
      "Iteration 109, loss = 0.19170204\n",
      "Iteration 110, loss = 0.19049058\n",
      "Iteration 111, loss = 0.19051721\n",
      "Iteration 112, loss = 0.18838449\n",
      "Iteration 113, loss = 0.18684844\n",
      "Iteration 114, loss = 0.18670124\n",
      "Iteration 115, loss = 0.18491745\n",
      "Iteration 116, loss = 0.18276427\n",
      "Iteration 117, loss = 0.18251425\n",
      "Iteration 118, loss = 0.18140349\n",
      "Iteration 119, loss = 0.17915472\n",
      "Iteration 120, loss = 0.17887315\n",
      "Iteration 121, loss = 0.17711408\n",
      "Iteration 122, loss = 0.17675055\n",
      "Iteration 123, loss = 0.17439906\n",
      "Iteration 124, loss = 0.17320007\n",
      "Iteration 125, loss = 0.17319441\n",
      "Iteration 126, loss = 0.17057861\n",
      "Iteration 127, loss = 0.17024426\n",
      "Iteration 128, loss = 0.16877767\n",
      "Iteration 129, loss = 0.16759790\n",
      "Iteration 130, loss = 0.16763143\n",
      "Iteration 131, loss = 0.16562125\n",
      "Iteration 132, loss = 0.16465395\n",
      "Iteration 133, loss = 0.16398220\n",
      "Iteration 134, loss = 0.16212553\n",
      "Iteration 135, loss = 0.16406110\n",
      "Iteration 136, loss = 0.15856753\n",
      "Iteration 137, loss = 0.16049632\n",
      "Iteration 138, loss = 0.15743977\n",
      "Iteration 139, loss = 0.15829732\n",
      "Iteration 140, loss = 0.15721839\n",
      "Iteration 141, loss = 0.15425883\n",
      "Iteration 142, loss = 0.15299460\n",
      "Iteration 143, loss = 0.15391211\n",
      "Iteration 144, loss = 0.15049766\n",
      "Iteration 145, loss = 0.15005462\n",
      "Iteration 146, loss = 0.15042616\n",
      "Iteration 147, loss = 0.14990910\n",
      "Iteration 148, loss = 0.14711590\n",
      "Iteration 149, loss = 0.14560271\n",
      "Iteration 150, loss = 0.14564816\n",
      "Iteration 151, loss = 0.14219303\n",
      "Iteration 152, loss = 0.14356217\n",
      "Iteration 153, loss = 0.14088184\n",
      "Iteration 154, loss = 0.14407905\n",
      "Iteration 155, loss = 0.13839306\n",
      "Iteration 156, loss = 0.14125890\n",
      "Iteration 157, loss = 0.14098240\n",
      "Iteration 158, loss = 0.13914624\n",
      "Iteration 159, loss = 0.14181994\n",
      "Iteration 160, loss = 0.13881010\n",
      "Iteration 161, loss = 0.13589127\n",
      "Iteration 162, loss = 0.13663329\n",
      "Iteration 163, loss = 0.13239320\n",
      "Iteration 164, loss = 0.13066267\n",
      "Iteration 165, loss = 0.13026274\n",
      "Iteration 166, loss = 0.12725481\n",
      "Iteration 167, loss = 0.12869877\n",
      "Iteration 168, loss = 0.12682376\n",
      "Iteration 169, loss = 0.12630463\n",
      "Iteration 170, loss = 0.12607145\n",
      "Iteration 171, loss = 0.12394819\n",
      "Iteration 172, loss = 0.12301831\n",
      "Iteration 173, loss = 0.12265126\n",
      "Iteration 174, loss = 0.12213657\n",
      "Iteration 175, loss = 0.11960392\n",
      "Iteration 176, loss = 0.11885002\n",
      "Iteration 177, loss = 0.11738808\n",
      "Iteration 178, loss = 0.11762473\n",
      "Iteration 179, loss = 0.11729904\n",
      "Iteration 180, loss = 0.11609664\n",
      "Iteration 181, loss = 0.11375762\n",
      "Iteration 182, loss = 0.11478544\n",
      "Iteration 183, loss = 0.11280147\n",
      "Iteration 184, loss = 0.11073368\n",
      "Iteration 185, loss = 0.10968374\n",
      "Iteration 186, loss = 0.10944627\n",
      "Iteration 187, loss = 0.10844772\n",
      "Iteration 188, loss = 0.10829472\n",
      "Iteration 189, loss = 0.10510012\n",
      "Iteration 190, loss = 0.10751363\n",
      "Iteration 191, loss = 0.10291297\n",
      "Iteration 192, loss = 0.10634445\n",
      "Iteration 193, loss = 0.10182706\n",
      "Iteration 194, loss = 0.10471125\n",
      "Iteration 195, loss = 0.10006928\n",
      "Iteration 196, loss = 0.10262138\n",
      "Iteration 197, loss = 0.10291747\n",
      "Iteration 198, loss = 0.09959918\n",
      "Iteration 199, loss = 0.09902308\n",
      "Iteration 200, loss = 0.10020253\n",
      "Iteration 201, loss = 0.09518119\n",
      "Iteration 202, loss = 0.09589783\n",
      "Iteration 203, loss = 0.09145721\n",
      "Iteration 204, loss = 0.09380801\n",
      "Iteration 205, loss = 0.08952903\n",
      "Iteration 206, loss = 0.09146048\n",
      "Iteration 207, loss = 0.08800240\n",
      "Iteration 208, loss = 0.08752968\n",
      "Iteration 209, loss = 0.08596055\n",
      "Iteration 210, loss = 0.08579721\n",
      "Iteration 211, loss = 0.08657098\n",
      "Iteration 212, loss = 0.08219530\n",
      "Iteration 213, loss = 0.08526363\n",
      "Iteration 214, loss = 0.08195899\n",
      "Iteration 215, loss = 0.08165570\n",
      "Iteration 216, loss = 0.08207014\n",
      "Iteration 217, loss = 0.07937616\n",
      "Iteration 218, loss = 0.07752298\n",
      "Iteration 219, loss = 0.07684089\n",
      "Iteration 220, loss = 0.07574079\n",
      "Iteration 221, loss = 0.07425931\n",
      "Iteration 222, loss = 0.07576657\n",
      "Iteration 223, loss = 0.07446777\n",
      "Iteration 224, loss = 0.07298243\n",
      "Iteration 225, loss = 0.07382403\n",
      "Iteration 226, loss = 0.06991570\n",
      "Iteration 227, loss = 0.07297133\n",
      "Iteration 228, loss = 0.06912433\n",
      "Iteration 229, loss = 0.07089703\n",
      "Iteration 230, loss = 0.06818815\n",
      "Iteration 231, loss = 0.06651031\n",
      "Iteration 232, loss = 0.07025288\n",
      "Iteration 233, loss = 0.06828127\n",
      "Iteration 234, loss = 0.06900579\n",
      "Iteration 235, loss = 0.06604266\n",
      "Iteration 236, loss = 0.06818266\n",
      "Iteration 237, loss = 0.07277796\n",
      "Iteration 238, loss = 0.06404579\n",
      "Iteration 239, loss = 0.07092104\n",
      "Iteration 240, loss = 0.06501941\n",
      "Iteration 241, loss = 0.06173003\n",
      "Iteration 242, loss = 0.06176350\n",
      "Iteration 243, loss = 0.05818099\n",
      "Iteration 244, loss = 0.06069062\n",
      "Iteration 245, loss = 0.05503211\n",
      "Iteration 246, loss = 0.05833098\n",
      "Iteration 247, loss = 0.05429088\n",
      "Iteration 248, loss = 0.05797951\n",
      "Iteration 249, loss = 0.05482348\n",
      "Iteration 250, loss = 0.05474107\n",
      "Iteration 251, loss = 0.05146005\n",
      "Iteration 252, loss = 0.05357856\n",
      "Iteration 253, loss = 0.05020208\n",
      "Iteration 254, loss = 0.05099515\n",
      "Iteration 255, loss = 0.04936934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, loss = 0.05089990\n",
      "Iteration 257, loss = 0.04820587\n",
      "Iteration 258, loss = 0.04978308\n",
      "Iteration 259, loss = 0.04720463\n",
      "Iteration 260, loss = 0.04744498\n",
      "Iteration 261, loss = 0.04724125\n",
      "Iteration 262, loss = 0.04682765\n",
      "Iteration 263, loss = 0.04555938\n",
      "Iteration 264, loss = 0.04667503\n",
      "Iteration 265, loss = 0.04601109\n",
      "Iteration 266, loss = 0.04442982\n",
      "Iteration 267, loss = 0.04421355\n",
      "Iteration 268, loss = 0.04226157\n",
      "Iteration 269, loss = 0.04363595\n",
      "Iteration 270, loss = 0.04205349\n",
      "Iteration 271, loss = 0.04223604\n",
      "Iteration 272, loss = 0.04031006\n",
      "Iteration 273, loss = 0.04159741\n",
      "Iteration 274, loss = 0.04093935\n",
      "Iteration 275, loss = 0.03951966\n",
      "Iteration 276, loss = 0.03895989\n",
      "Iteration 277, loss = 0.03904886\n",
      "Iteration 278, loss = 0.03805548\n",
      "Iteration 279, loss = 0.03943738\n",
      "Iteration 280, loss = 0.03743132\n",
      "Iteration 281, loss = 0.03819468\n",
      "Iteration 282, loss = 0.03725501\n",
      "Iteration 283, loss = 0.03611392\n",
      "Iteration 284, loss = 0.03751799\n",
      "Iteration 285, loss = 0.03602859\n",
      "Iteration 286, loss = 0.03671892\n",
      "Iteration 287, loss = 0.03708820\n",
      "Iteration 288, loss = 0.03657208\n",
      "Iteration 289, loss = 0.03582010\n",
      "Iteration 290, loss = 0.03445459\n",
      "Iteration 291, loss = 0.03473975\n",
      "Iteration 292, loss = 0.03264116\n",
      "Iteration 293, loss = 0.03388754\n",
      "Iteration 294, loss = 0.03241968\n",
      "Iteration 295, loss = 0.03315902\n",
      "Iteration 296, loss = 0.03170596\n",
      "Iteration 297, loss = 0.03211079\n",
      "Iteration 298, loss = 0.03127226\n",
      "Iteration 299, loss = 0.03097895\n",
      "Iteration 300, loss = 0.03216448\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train_imp, y_train)\n",
    "y_pred = clf.predict(X_test_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8765957446808511\n",
      "[[ 51  12]\n",
      " [ 17 155]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.75      0.81      0.78        63\n",
      "        True       0.93      0.90      0.91       172\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       235\n",
      "   macro avg       0.84      0.86      0.85       235\n",
      "weighted avg       0.88      0.88      0.88       235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/joe/MLP/features_sample_head2_selection.csv\")\n",
    "df = df.astype({'Class_P0': 'bool', \n",
    "#                 'head0_AU28': 'bool',\n",
    "#                 'head0_AU45': 'bool',\n",
    "#                 'head0_AU26': 'bool',\n",
    "#                 'head0_AU25': 'bool',\n",
    "#                 'head0_AU01': 'bool',\n",
    "#                 'head0_AU05': 'bool',\n",
    "#                 'head0_AU20': 'bool',\n",
    "#                 'head0_AU06': 'bool',\n",
    "#                 'head0_AU23': 'bool',\n",
    "#                 'head0_AU07': 'bool',\n",
    "#                 'head0_AU02': 'bool', \n",
    "#                 'head0_AU09': 'bool',\n",
    "#                 'head0_AU17': 'bool',\n",
    "#                 'head0_AU10': 'bool',\n",
    "#                 'head0_AU04': 'bool',\n",
    "#                 'head0_AU12': 'bool',\n",
    "#                 'head0_AU14': 'bool',\n",
    "#                 'head0_AU15': 'bool',\n",
    "                'ClassP2':'bool'})\n",
    "le = LabelEncoder()\n",
    "# df['head0_pose'] = le.fit_transform(df['head0_pose'])\n",
    "# df['head0_pose'] = le.fit_transform(df['head0_pose'])\n",
    "# df['head2_pose'] = le.fit_transform(df['head2_pose'])\n",
    "# df['head0_emo'] = le.fit_transform(df['head0_emo'])\n",
    "# df['head0_emo'] = le.fit_transform(df['head0_emo'])\n",
    "# df['head2_emo'] = le.fit_transform(df['head2_emo'])\n",
    "\n",
    "y = df['ClassP2']\n",
    "x = df.drop(['ClassP2'], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size= 0.30, random_state=27)\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imp = imp.fit(x_train)\n",
    "\n",
    "X_train_imp = imp.transform(x_train)\n",
    "X_test_imp = imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=300, alpha=0.0001, activation='tanh',\n",
    "                     solver='adam', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfsl = sfs(clf, k_features=10,forward=True)\n",
    "# print(sfsl.fit(X_test_imp, y_test))\n",
    "# print(sfsl.subsets_)\n",
    "# feature_names': ('0', '1', '4', '6', '7', '9', '11', '14', '17', '24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67959590\n",
      "Iteration 2, loss = 0.61588374\n",
      "Iteration 3, loss = 0.58874316\n",
      "Iteration 4, loss = 0.57339714\n",
      "Iteration 5, loss = 0.56531784\n",
      "Iteration 6, loss = 0.55652568\n",
      "Iteration 7, loss = 0.54778030\n",
      "Iteration 8, loss = 0.54627189\n",
      "Iteration 9, loss = 0.54461690\n",
      "Iteration 10, loss = 0.53993356\n",
      "Iteration 11, loss = 0.53654098\n",
      "Iteration 12, loss = 0.53021298\n",
      "Iteration 13, loss = 0.52843148\n",
      "Iteration 14, loss = 0.52446390\n",
      "Iteration 15, loss = 0.51960907\n",
      "Iteration 16, loss = 0.51749485\n",
      "Iteration 17, loss = 0.51126703\n",
      "Iteration 18, loss = 0.50646531\n",
      "Iteration 19, loss = 0.50250207\n",
      "Iteration 20, loss = 0.49508840\n",
      "Iteration 21, loss = 0.48858305\n",
      "Iteration 22, loss = 0.48042729\n",
      "Iteration 23, loss = 0.47220944\n",
      "Iteration 24, loss = 0.46222033\n",
      "Iteration 25, loss = 0.45077801\n",
      "Iteration 26, loss = 0.43897341\n",
      "Iteration 27, loss = 0.42587836\n",
      "Iteration 28, loss = 0.41278359\n",
      "Iteration 29, loss = 0.39912488\n",
      "Iteration 30, loss = 0.38481436\n",
      "Iteration 31, loss = 0.37319714\n",
      "Iteration 32, loss = 0.36070026\n",
      "Iteration 33, loss = 0.34908977\n",
      "Iteration 34, loss = 0.33813850\n",
      "Iteration 35, loss = 0.32908145\n",
      "Iteration 36, loss = 0.32166402\n",
      "Iteration 37, loss = 0.31348609\n",
      "Iteration 38, loss = 0.30746279\n",
      "Iteration 39, loss = 0.30244703\n",
      "Iteration 40, loss = 0.29823337\n",
      "Iteration 41, loss = 0.29402592\n",
      "Iteration 42, loss = 0.29010502\n",
      "Iteration 43, loss = 0.28738154\n",
      "Iteration 44, loss = 0.28452967\n",
      "Iteration 45, loss = 0.28372118\n",
      "Iteration 46, loss = 0.28010971\n",
      "Iteration 47, loss = 0.28079993\n",
      "Iteration 48, loss = 0.27648248\n",
      "Iteration 49, loss = 0.27545330\n",
      "Iteration 50, loss = 0.27289886\n",
      "Iteration 51, loss = 0.27318837\n",
      "Iteration 52, loss = 0.27030418\n",
      "Iteration 53, loss = 0.26816802\n",
      "Iteration 54, loss = 0.26742271\n",
      "Iteration 55, loss = 0.26552193\n",
      "Iteration 56, loss = 0.26491508\n",
      "Iteration 57, loss = 0.26473323\n",
      "Iteration 58, loss = 0.26277323\n",
      "Iteration 59, loss = 0.26096547\n",
      "Iteration 60, loss = 0.25985954\n",
      "Iteration 61, loss = 0.25767967\n",
      "Iteration 62, loss = 0.25677524\n",
      "Iteration 63, loss = 0.25767501\n",
      "Iteration 64, loss = 0.25565778\n",
      "Iteration 65, loss = 0.25452624\n",
      "Iteration 66, loss = 0.25240665\n",
      "Iteration 67, loss = 0.25302855\n",
      "Iteration 68, loss = 0.25091805\n",
      "Iteration 69, loss = 0.25052984\n",
      "Iteration 70, loss = 0.24718781\n",
      "Iteration 71, loss = 0.24708171\n",
      "Iteration 72, loss = 0.24635994\n",
      "Iteration 73, loss = 0.24391211\n",
      "Iteration 74, loss = 0.24351753\n",
      "Iteration 75, loss = 0.24147807\n",
      "Iteration 76, loss = 0.24066883\n",
      "Iteration 77, loss = 0.24039220\n",
      "Iteration 78, loss = 0.23933254\n",
      "Iteration 79, loss = 0.23855268\n",
      "Iteration 80, loss = 0.23665361\n",
      "Iteration 81, loss = 0.23644969\n",
      "Iteration 82, loss = 0.23525848\n",
      "Iteration 83, loss = 0.23350039\n",
      "Iteration 84, loss = 0.23161558\n",
      "Iteration 85, loss = 0.23338992\n",
      "Iteration 86, loss = 0.23180857\n",
      "Iteration 87, loss = 0.23096270\n",
      "Iteration 88, loss = 0.23114309\n",
      "Iteration 89, loss = 0.22810222\n",
      "Iteration 90, loss = 0.22619962\n",
      "Iteration 91, loss = 0.22501543\n",
      "Iteration 92, loss = 0.22424846\n",
      "Iteration 93, loss = 0.22387002\n",
      "Iteration 94, loss = 0.22261589\n",
      "Iteration 95, loss = 0.22100252\n",
      "Iteration 96, loss = 0.22222651\n",
      "Iteration 97, loss = 0.22008647\n",
      "Iteration 98, loss = 0.21969268\n",
      "Iteration 99, loss = 0.21712864\n",
      "Iteration 100, loss = 0.21743321\n",
      "Iteration 101, loss = 0.21478980\n",
      "Iteration 102, loss = 0.21424068\n",
      "Iteration 103, loss = 0.21225024\n",
      "Iteration 104, loss = 0.21122660\n",
      "Iteration 105, loss = 0.21138477\n",
      "Iteration 106, loss = 0.20945603\n",
      "Iteration 107, loss = 0.21029776\n",
      "Iteration 108, loss = 0.20808410\n",
      "Iteration 109, loss = 0.20727105\n",
      "Iteration 110, loss = 0.20519267\n",
      "Iteration 111, loss = 0.20448839\n",
      "Iteration 112, loss = 0.20316041\n",
      "Iteration 113, loss = 0.20240798\n",
      "Iteration 114, loss = 0.20056253\n",
      "Iteration 115, loss = 0.20092430\n",
      "Iteration 116, loss = 0.19888372\n",
      "Iteration 117, loss = 0.19838646\n",
      "Iteration 118, loss = 0.19781596\n",
      "Iteration 119, loss = 0.19804594\n",
      "Iteration 120, loss = 0.19457850\n",
      "Iteration 121, loss = 0.19359117\n",
      "Iteration 122, loss = 0.19232947\n",
      "Iteration 123, loss = 0.19200031\n",
      "Iteration 124, loss = 0.19323002\n",
      "Iteration 125, loss = 0.19064257\n",
      "Iteration 126, loss = 0.18992761\n",
      "Iteration 127, loss = 0.18877211\n",
      "Iteration 128, loss = 0.18830442\n",
      "Iteration 129, loss = 0.18700197\n",
      "Iteration 130, loss = 0.18592335\n",
      "Iteration 131, loss = 0.18560125\n",
      "Iteration 132, loss = 0.18562729\n",
      "Iteration 133, loss = 0.18590716\n",
      "Iteration 134, loss = 0.18341409\n",
      "Iteration 135, loss = 0.18323462\n",
      "Iteration 136, loss = 0.18476637\n",
      "Iteration 137, loss = 0.18123782\n",
      "Iteration 138, loss = 0.18293669\n",
      "Iteration 139, loss = 0.17743741\n",
      "Iteration 140, loss = 0.17806227\n",
      "Iteration 141, loss = 0.17649535\n",
      "Iteration 142, loss = 0.17546412\n",
      "Iteration 143, loss = 0.17711102\n",
      "Iteration 144, loss = 0.17619996\n",
      "Iteration 145, loss = 0.17478877\n",
      "Iteration 146, loss = 0.17592728\n",
      "Iteration 147, loss = 0.17313991\n",
      "Iteration 148, loss = 0.17446867\n",
      "Iteration 149, loss = 0.17523156\n",
      "Iteration 150, loss = 0.17062490\n",
      "Iteration 151, loss = 0.17130275\n",
      "Iteration 152, loss = 0.16866645\n",
      "Iteration 153, loss = 0.16731721\n",
      "Iteration 154, loss = 0.16681827\n",
      "Iteration 155, loss = 0.16713054\n",
      "Iteration 156, loss = 0.16556352\n",
      "Iteration 157, loss = 0.16691227\n",
      "Iteration 158, loss = 0.16501625\n",
      "Iteration 159, loss = 0.16394491\n",
      "Iteration 160, loss = 0.16386073\n",
      "Iteration 161, loss = 0.16211441\n",
      "Iteration 162, loss = 0.16420859\n",
      "Iteration 163, loss = 0.16209441\n",
      "Iteration 164, loss = 0.16106138\n",
      "Iteration 165, loss = 0.16001147\n",
      "Iteration 166, loss = 0.15942139\n",
      "Iteration 167, loss = 0.15970510\n",
      "Iteration 168, loss = 0.16002255\n",
      "Iteration 169, loss = 0.15741983\n",
      "Iteration 170, loss = 0.15934571\n",
      "Iteration 171, loss = 0.15672129\n",
      "Iteration 172, loss = 0.15591337\n",
      "Iteration 173, loss = 0.15616477\n",
      "Iteration 174, loss = 0.15263485\n",
      "Iteration 175, loss = 0.15542047\n",
      "Iteration 176, loss = 0.15323897\n",
      "Iteration 177, loss = 0.15289567\n",
      "Iteration 178, loss = 0.15155350\n",
      "Iteration 179, loss = 0.15161997\n",
      "Iteration 180, loss = 0.15048760\n",
      "Iteration 181, loss = 0.14900011\n",
      "Iteration 182, loss = 0.14953978\n",
      "Iteration 183, loss = 0.14919568\n",
      "Iteration 184, loss = 0.14908472\n",
      "Iteration 185, loss = 0.15174599\n",
      "Iteration 186, loss = 0.14665176\n",
      "Iteration 187, loss = 0.14666329\n",
      "Iteration 188, loss = 0.14638187\n",
      "Iteration 189, loss = 0.14637144\n",
      "Iteration 190, loss = 0.14492035\n",
      "Iteration 191, loss = 0.14459320\n",
      "Iteration 192, loss = 0.14340610\n",
      "Iteration 193, loss = 0.14484379\n",
      "Iteration 194, loss = 0.14385490\n",
      "Iteration 195, loss = 0.14355286\n",
      "Iteration 196, loss = 0.14053930\n",
      "Iteration 197, loss = 0.14326936\n",
      "Iteration 198, loss = 0.14030432\n",
      "Iteration 199, loss = 0.14185913\n",
      "Iteration 200, loss = 0.13902088\n",
      "Iteration 201, loss = 0.14156041\n",
      "Iteration 202, loss = 0.13910359\n",
      "Iteration 203, loss = 0.13830206\n",
      "Iteration 204, loss = 0.13798461\n",
      "Iteration 205, loss = 0.14225451\n",
      "Iteration 206, loss = 0.13630536\n",
      "Iteration 207, loss = 0.13804335\n",
      "Iteration 208, loss = 0.13907666\n",
      "Iteration 209, loss = 0.13552719\n",
      "Iteration 210, loss = 0.13441313\n",
      "Iteration 211, loss = 0.13760407\n",
      "Iteration 212, loss = 0.13404397\n",
      "Iteration 213, loss = 0.13286195\n",
      "Iteration 214, loss = 0.13297099\n",
      "Iteration 215, loss = 0.13218288\n",
      "Iteration 216, loss = 0.13492312\n",
      "Iteration 217, loss = 0.12926816\n",
      "Iteration 218, loss = 0.13080284\n",
      "Iteration 219, loss = 0.12816737\n",
      "Iteration 220, loss = 0.12855845\n",
      "Iteration 221, loss = 0.12674848\n",
      "Iteration 222, loss = 0.13000998\n",
      "Iteration 223, loss = 0.12572106\n",
      "Iteration 224, loss = 0.12633178\n",
      "Iteration 225, loss = 0.12653013\n",
      "Iteration 226, loss = 0.12715867\n",
      "Iteration 227, loss = 0.12540997\n",
      "Iteration 228, loss = 0.12348650\n",
      "Iteration 229, loss = 0.12280131\n",
      "Iteration 230, loss = 0.12353721\n",
      "Iteration 231, loss = 0.12244359\n",
      "Iteration 232, loss = 0.12375543\n",
      "Iteration 233, loss = 0.12159621\n",
      "Iteration 234, loss = 0.12169399\n",
      "Iteration 235, loss = 0.12159742\n",
      "Iteration 236, loss = 0.11875029\n",
      "Iteration 237, loss = 0.11978639\n",
      "Iteration 238, loss = 0.11881877\n",
      "Iteration 239, loss = 0.11787736\n",
      "Iteration 240, loss = 0.11799401\n",
      "Iteration 241, loss = 0.11837355\n",
      "Iteration 242, loss = 0.11747437\n",
      "Iteration 243, loss = 0.11654263\n",
      "Iteration 244, loss = 0.11490362\n",
      "Iteration 245, loss = 0.11567071\n",
      "Iteration 246, loss = 0.11457046\n",
      "Iteration 247, loss = 0.11366830\n",
      "Iteration 248, loss = 0.11302191\n",
      "Iteration 249, loss = 0.11288486\n",
      "Iteration 250, loss = 0.11352206\n",
      "Iteration 251, loss = 0.11221413\n",
      "Iteration 252, loss = 0.11142664\n",
      "Iteration 253, loss = 0.11185121\n",
      "Iteration 254, loss = 0.10997618\n",
      "Iteration 255, loss = 0.11267946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, loss = 0.10973927\n",
      "Iteration 257, loss = 0.10940797\n",
      "Iteration 258, loss = 0.10950328\n",
      "Iteration 259, loss = 0.10820543\n",
      "Iteration 260, loss = 0.10850635\n",
      "Iteration 261, loss = 0.10985594\n",
      "Iteration 262, loss = 0.10878644\n",
      "Iteration 263, loss = 0.10908478\n",
      "Iteration 264, loss = 0.10539557\n",
      "Iteration 265, loss = 0.10695903\n",
      "Iteration 266, loss = 0.10574307\n",
      "Iteration 267, loss = 0.10439510\n",
      "Iteration 268, loss = 0.10413687\n",
      "Iteration 269, loss = 0.10293603\n",
      "Iteration 270, loss = 0.10277483\n",
      "Iteration 271, loss = 0.10307655\n",
      "Iteration 272, loss = 0.10140726\n",
      "Iteration 273, loss = 0.10145571\n",
      "Iteration 274, loss = 0.10202656\n",
      "Iteration 275, loss = 0.10148281\n",
      "Iteration 276, loss = 0.10196156\n",
      "Iteration 277, loss = 0.10221751\n",
      "Iteration 278, loss = 0.10005190\n",
      "Iteration 279, loss = 0.10072133\n",
      "Iteration 280, loss = 0.09955611\n",
      "Iteration 281, loss = 0.09783463\n",
      "Iteration 282, loss = 0.09784285\n",
      "Iteration 283, loss = 0.09796485\n",
      "Iteration 284, loss = 0.09836463\n",
      "Iteration 285, loss = 0.09744745\n",
      "Iteration 286, loss = 0.09965242\n",
      "Iteration 287, loss = 0.09615910\n",
      "Iteration 288, loss = 0.09793856\n",
      "Iteration 289, loss = 0.09576787\n",
      "Iteration 290, loss = 0.09677534\n",
      "Iteration 291, loss = 0.09565281\n",
      "Iteration 292, loss = 0.09450403\n",
      "Iteration 293, loss = 0.09308649\n",
      "Iteration 294, loss = 0.09328781\n",
      "Iteration 295, loss = 0.09273725\n",
      "Iteration 296, loss = 0.09293186\n",
      "Iteration 297, loss = 0.09284428\n",
      "Iteration 298, loss = 0.09175480\n",
      "Iteration 299, loss = 0.09108136\n",
      "Iteration 300, loss = 0.09239591\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train_imp, y_train)\n",
    "y_pred = clf.predict(X_test_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9234042553191489\n",
      "[[ 70  12]\n",
      " [  6 147]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.85      0.89        82\n",
      "        True       0.92      0.96      0.94       153\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       235\n",
      "   macro avg       0.92      0.91      0.91       235\n",
      "weighted avg       0.92      0.92      0.92       235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
