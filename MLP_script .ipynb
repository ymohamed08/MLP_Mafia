{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.decomposition import FactorAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/joe/MLP/features_sample_head0.csv\")\n",
    "df = df.astype({'Class_P0': 'bool',\n",
    "                'head1_AU28': 'bool',\n",
    "                'head1_AU45': 'bool',\n",
    "                'head1_AU26': 'bool',\n",
    "                'head1_AU25': 'bool',\n",
    "                'head1_AU01': 'bool',\n",
    "                'head1_AU05': 'bool',\n",
    "                'head1_AU20': 'bool',\n",
    "                'head1_AU06': 'bool',\n",
    "                'head1_AU23': 'bool',\n",
    "                'head1_AU07': 'bool',\n",
    "                'head1_AU02': 'bool', \n",
    "                'head1_AU09': 'bool',\n",
    "                'head1_AU17': 'bool',\n",
    "                'head1_AU10': 'bool',\n",
    "                'head1_AU04': 'bool',\n",
    "                'head1_AU12': 'bool',\n",
    "                'head1_AU14': 'bool',\n",
    "                'head1_AU15': 'bool'})\n",
    "le = LabelEncoder()\n",
    "# df['head0_pose'] = le.fit_transform(df['head0_pose'])\n",
    "df['head1_pose'] = le.fit_transform(df['head1_pose'])\n",
    "# df['head2_pose'] = le.fit_transform(df['head2_pose'])\n",
    "# df['head0_emo'] = le.fit_transform(df['head0_emo'])\n",
    "df['head1_emo'] = le.fit_transform(df['head1_emo'])\n",
    "# df['head2_emo'] = le.fit_transform(df['head2_emo'])\n",
    "\n",
    "y = df['Class_P0']\n",
    "x = df.drop(['Class_P0'], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size= 0.30, random_state=27)\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imp = imp.fit(x_train)\n",
    "\n",
    "X_train_imp = imp.transform(x_train)\n",
    "X_test_imp = imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "head1_head0_trans_x    float64\n",
       "head1_head0_trans_y    float64\n",
       "head1_head0_trans_z    float64\n",
       "head1_head0_ori_x      float64\n",
       "head1_head0_ori_y      float64\n",
       "head1_head0_ori_z      float64\n",
       "head1_head0_ori_w      float64\n",
       "head1_head1_trans_x    float64\n",
       "head1_head1_trans_y    float64\n",
       "head1_head1_trans_z    float64\n",
       "head1_head1_ori_x      float64\n",
       "head1_head1_ori_y      float64\n",
       "head1_head1_ori_z      float64\n",
       "head1_head1_ori_w      float64\n",
       "head1_head2_trans_x    float64\n",
       "head1_head2_trans_y    float64\n",
       "head1_head2_trans_z    float64\n",
       "head1_head2_ori_x      float64\n",
       "head1_head2_ori_y      float64\n",
       "head1_head2_ori_z      float64\n",
       "head1_head2_ori_w      float64\n",
       "head1_emo                int64\n",
       "head1_AU28                bool\n",
       "head1_AU45                bool\n",
       "head1_AU26                bool\n",
       "head1_AU25                bool\n",
       "head1_AU01                bool\n",
       "head1_AU05                bool\n",
       "head1_AU20                bool\n",
       "head1_AU06                bool\n",
       "head1_AU23                bool\n",
       "head1_AU07                bool\n",
       "head1_AU02                bool\n",
       "head1_AU09                bool\n",
       "head1_AU17                bool\n",
       "head1_AU10                bool\n",
       "head1_AU04                bool\n",
       "head1_AU12                bool\n",
       "head1_AU14                bool\n",
       "head1_AU15                bool\n",
       "head1_pose               int64\n",
       "ClassP1                   bool\n",
       "Class_P0                  bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=300, alpha=0.0001, activation='tanh',\n",
    "                     solver='adam', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection (Skip as it Takes time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfsl = sfs(clf, k_features=10,forward=True)\n",
    "# print(sfsl.fit(X_test_imp, y_test))\n",
    "# print(sfsl.subsets_)\n",
    "# #'feature_names': head1_head0_trans_x\thead1_head0_trans_y\thead1_head0_ori_w\thead1_head1_trans_x\thead1_head1_ori_x\thead1_head1_ori_y\thead1_head2_trans_z\thead1_AU07\thead1_AU09\thead1_AU12\tClassP1\tClass_P0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factore analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.250149</td>\n",
       "      <td>0.045421</td>\n",
       "      <td>-0.048971</td>\n",
       "      <td>-0.001805</td>\n",
       "      <td>0.057151</td>\n",
       "      <td>0.009619</td>\n",
       "      <td>-0.002008</td>\n",
       "      <td>-0.197865</td>\n",
       "      <td>0.035695</td>\n",
       "      <td>0.023018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119543</td>\n",
       "      <td>0.346680</td>\n",
       "      <td>0.278890</td>\n",
       "      <td>0.244760</td>\n",
       "      <td>0.289871</td>\n",
       "      <td>0.207581</td>\n",
       "      <td>0.223799</td>\n",
       "      <td>0.117088</td>\n",
       "      <td>0.139196</td>\n",
       "      <td>-0.017376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.403650</td>\n",
       "      <td>0.084069</td>\n",
       "      <td>-0.054023</td>\n",
       "      <td>-0.027858</td>\n",
       "      <td>0.139051</td>\n",
       "      <td>0.012316</td>\n",
       "      <td>-0.001168</td>\n",
       "      <td>-0.213966</td>\n",
       "      <td>0.049680</td>\n",
       "      <td>0.013963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150266</td>\n",
       "      <td>-0.079707</td>\n",
       "      <td>0.029973</td>\n",
       "      <td>-0.090913</td>\n",
       "      <td>0.062688</td>\n",
       "      <td>-0.197779</td>\n",
       "      <td>-0.219705</td>\n",
       "      <td>-0.060826</td>\n",
       "      <td>0.158575</td>\n",
       "      <td>-0.064839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.009745</td>\n",
       "      <td>-0.034948</td>\n",
       "      <td>0.028491</td>\n",
       "      <td>0.042814</td>\n",
       "      <td>-0.008411</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>-0.009573</td>\n",
       "      <td>-0.011465</td>\n",
       "      <td>-0.007674</td>\n",
       "      <td>-0.008132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036865</td>\n",
       "      <td>0.008204</td>\n",
       "      <td>0.010181</td>\n",
       "      <td>-0.023712</td>\n",
       "      <td>0.028237</td>\n",
       "      <td>-0.031855</td>\n",
       "      <td>-0.005872</td>\n",
       "      <td>0.004513</td>\n",
       "      <td>1.197419</td>\n",
       "      <td>0.024751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.008378</td>\n",
       "      <td>-0.053277</td>\n",
       "      <td>0.074923</td>\n",
       "      <td>0.079193</td>\n",
       "      <td>0.015027</td>\n",
       "      <td>0.042446</td>\n",
       "      <td>-0.011862</td>\n",
       "      <td>0.085330</td>\n",
       "      <td>-0.039875</td>\n",
       "      <td>0.063165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114232</td>\n",
       "      <td>0.002575</td>\n",
       "      <td>0.054535</td>\n",
       "      <td>-0.058629</td>\n",
       "      <td>0.129016</td>\n",
       "      <td>-0.108544</td>\n",
       "      <td>-0.054319</td>\n",
       "      <td>0.025066</td>\n",
       "      <td>-0.462774</td>\n",
       "      <td>0.017957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.040211</td>\n",
       "      <td>-0.002376</td>\n",
       "      <td>0.042580</td>\n",
       "      <td>-0.019596</td>\n",
       "      <td>-0.034220</td>\n",
       "      <td>-0.046193</td>\n",
       "      <td>-0.020101</td>\n",
       "      <td>-0.116239</td>\n",
       "      <td>0.079771</td>\n",
       "      <td>-0.038731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007448</td>\n",
       "      <td>0.017506</td>\n",
       "      <td>0.047645</td>\n",
       "      <td>0.012699</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.012242</td>\n",
       "      <td>0.012426</td>\n",
       "      <td>-0.018585</td>\n",
       "      <td>-0.256412</td>\n",
       "      <td>-0.054098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.068629</td>\n",
       "      <td>-0.027333</td>\n",
       "      <td>0.075058</td>\n",
       "      <td>0.013803</td>\n",
       "      <td>-0.012672</td>\n",
       "      <td>-0.018640</td>\n",
       "      <td>-0.013929</td>\n",
       "      <td>0.188865</td>\n",
       "      <td>-0.030733</td>\n",
       "      <td>0.098193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006589</td>\n",
       "      <td>-0.015460</td>\n",
       "      <td>-0.039864</td>\n",
       "      <td>0.003249</td>\n",
       "      <td>-0.037561</td>\n",
       "      <td>0.024469</td>\n",
       "      <td>0.011687</td>\n",
       "      <td>-0.069697</td>\n",
       "      <td>0.035507</td>\n",
       "      <td>-0.123216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.105171</td>\n",
       "      <td>-0.019156</td>\n",
       "      <td>-0.018987</td>\n",
       "      <td>-0.017155</td>\n",
       "      <td>-0.047788</td>\n",
       "      <td>-0.045729</td>\n",
       "      <td>-0.011505</td>\n",
       "      <td>0.097982</td>\n",
       "      <td>0.021891</td>\n",
       "      <td>0.022950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097675</td>\n",
       "      <td>-0.006527</td>\n",
       "      <td>0.045668</td>\n",
       "      <td>-0.077805</td>\n",
       "      <td>0.048215</td>\n",
       "      <td>-0.066110</td>\n",
       "      <td>-0.055975</td>\n",
       "      <td>-0.007699</td>\n",
       "      <td>0.105934</td>\n",
       "      <td>0.029315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.019369</td>\n",
       "      <td>-0.012836</td>\n",
       "      <td>0.120861</td>\n",
       "      <td>0.016124</td>\n",
       "      <td>0.012460</td>\n",
       "      <td>-0.011762</td>\n",
       "      <td>-0.005276</td>\n",
       "      <td>-0.161650</td>\n",
       "      <td>0.029595</td>\n",
       "      <td>0.084262</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063484</td>\n",
       "      <td>-0.023360</td>\n",
       "      <td>-0.059391</td>\n",
       "      <td>0.054830</td>\n",
       "      <td>-0.014986</td>\n",
       "      <td>0.037235</td>\n",
       "      <td>0.036042</td>\n",
       "      <td>-0.007677</td>\n",
       "      <td>0.042849</td>\n",
       "      <td>-0.120660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.070164</td>\n",
       "      <td>-0.057212</td>\n",
       "      <td>-0.066154</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>-0.003102</td>\n",
       "      <td>-0.005361</td>\n",
       "      <td>-0.014976</td>\n",
       "      <td>-0.056403</td>\n",
       "      <td>0.003441</td>\n",
       "      <td>-0.101263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025059</td>\n",
       "      <td>0.005399</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>0.048329</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>-0.011255</td>\n",
       "      <td>0.037787</td>\n",
       "      <td>0.014929</td>\n",
       "      <td>-0.005276</td>\n",
       "      <td>-0.004662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.123443</td>\n",
       "      <td>0.004343</td>\n",
       "      <td>0.024232</td>\n",
       "      <td>-0.004408</td>\n",
       "      <td>-0.033212</td>\n",
       "      <td>0.017566</td>\n",
       "      <td>0.006555</td>\n",
       "      <td>-0.020290</td>\n",
       "      <td>0.006903</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031929</td>\n",
       "      <td>-0.046597</td>\n",
       "      <td>0.074956</td>\n",
       "      <td>-0.052603</td>\n",
       "      <td>-0.010232</td>\n",
       "      <td>-0.038071</td>\n",
       "      <td>-0.034174</td>\n",
       "      <td>0.014077</td>\n",
       "      <td>0.057784</td>\n",
       "      <td>-0.026218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.008304</td>\n",
       "      <td>-0.043799</td>\n",
       "      <td>-0.045952</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>-0.004583</td>\n",
       "      <td>-0.071278</td>\n",
       "      <td>-0.035308</td>\n",
       "      <td>-0.046794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012037</td>\n",
       "      <td>-0.022469</td>\n",
       "      <td>0.038756</td>\n",
       "      <td>-0.045935</td>\n",
       "      <td>-0.016719</td>\n",
       "      <td>-0.025898</td>\n",
       "      <td>0.007257</td>\n",
       "      <td>-0.007529</td>\n",
       "      <td>-0.031917</td>\n",
       "      <td>-0.043810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.038952</td>\n",
       "      <td>-0.000479</td>\n",
       "      <td>0.032611</td>\n",
       "      <td>-0.013318</td>\n",
       "      <td>0.026070</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>-0.010850</td>\n",
       "      <td>0.030235</td>\n",
       "      <td>-0.007596</td>\n",
       "      <td>0.008972</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035707</td>\n",
       "      <td>-0.008063</td>\n",
       "      <td>0.018858</td>\n",
       "      <td>-0.017527</td>\n",
       "      <td>0.014747</td>\n",
       "      <td>-0.009656</td>\n",
       "      <td>-0.027054</td>\n",
       "      <td>0.020109</td>\n",
       "      <td>-0.000358</td>\n",
       "      <td>0.046934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.006056</td>\n",
       "      <td>-0.003407</td>\n",
       "      <td>0.021305</td>\n",
       "      <td>-0.004925</td>\n",
       "      <td>-0.009138</td>\n",
       "      <td>-0.003411</td>\n",
       "      <td>-0.001695</td>\n",
       "      <td>-0.017890</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>-0.015128</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003412</td>\n",
       "      <td>-0.007432</td>\n",
       "      <td>-0.001030</td>\n",
       "      <td>-0.030409</td>\n",
       "      <td>0.038745</td>\n",
       "      <td>0.014448</td>\n",
       "      <td>0.011051</td>\n",
       "      <td>-0.072301</td>\n",
       "      <td>0.004397</td>\n",
       "      <td>-0.045736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.003695</td>\n",
       "      <td>0.004451</td>\n",
       "      <td>-0.002810</td>\n",
       "      <td>0.007074</td>\n",
       "      <td>-0.000307</td>\n",
       "      <td>-0.005085</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>-0.003368</td>\n",
       "      <td>0.006814</td>\n",
       "      <td>0.004881</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003617</td>\n",
       "      <td>-0.004876</td>\n",
       "      <td>0.007067</td>\n",
       "      <td>-0.003982</td>\n",
       "      <td>0.008950</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.015575</td>\n",
       "      <td>-0.001673</td>\n",
       "      <td>-0.015750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0  -0.250149  0.045421 -0.048971 -0.001805  0.057151  0.009619 -0.002008   \n",
       "1  -0.403650  0.084069 -0.054023 -0.027858  0.139051  0.012316 -0.001168   \n",
       "2   0.009745 -0.034948  0.028491  0.042814 -0.008411  0.003409 -0.009573   \n",
       "3  -0.008378 -0.053277  0.074923  0.079193  0.015027  0.042446 -0.011862   \n",
       "4   0.040211 -0.002376  0.042580 -0.019596 -0.034220 -0.046193 -0.020101   \n",
       "5   0.068629 -0.027333  0.075058  0.013803 -0.012672 -0.018640 -0.013929   \n",
       "6   0.105171 -0.019156 -0.018987 -0.017155 -0.047788 -0.045729 -0.011505   \n",
       "7  -0.019369 -0.012836  0.120861  0.016124  0.012460 -0.011762 -0.005276   \n",
       "8  -0.070164 -0.057212 -0.066154  0.009259 -0.003102 -0.005361 -0.014976   \n",
       "9   0.123443  0.004343  0.024232 -0.004408 -0.033212  0.017566  0.006555   \n",
       "10  0.008304 -0.043799 -0.045952  0.002361  0.001540  0.000400 -0.004583   \n",
       "11 -0.038952 -0.000479  0.032611 -0.013318  0.026070  0.001114 -0.010850   \n",
       "12  0.006056 -0.003407  0.021305 -0.004925 -0.009138 -0.003411 -0.001695   \n",
       "13 -0.003695  0.004451 -0.002810  0.007074 -0.000307 -0.005085  0.002968   \n",
       "14 -0.000000 -0.000000 -0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "15  0.000000 -0.000000  0.000000 -0.000000 -0.000000  0.000000 -0.000000   \n",
       "16 -0.000000  0.000000 -0.000000 -0.000000 -0.000000 -0.000000  0.000000   \n",
       "17 -0.000000 -0.000000 -0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "18  0.000000  0.000000  0.000000  0.000000 -0.000000  0.000000 -0.000000   \n",
       "19 -0.000000  0.000000  0.000000  0.000000  0.000000 -0.000000 -0.000000   \n",
       "20  0.000000 -0.000000  0.000000 -0.000000 -0.000000 -0.000000  0.000000   \n",
       "21  0.000000  0.000000 -0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "22 -0.000000  0.000000 -0.000000  0.000000 -0.000000 -0.000000 -0.000000   \n",
       "23 -0.000000  0.000000 -0.000000  0.000000 -0.000000  0.000000 -0.000000   \n",
       "24  0.000000  0.000000 -0.000000  0.000000 -0.000000 -0.000000  0.000000   \n",
       "25 -0.000000  0.000000  0.000000 -0.000000 -0.000000  0.000000 -0.000000   \n",
       "26  0.000000  0.000000 -0.000000 -0.000000  0.000000 -0.000000  0.000000   \n",
       "27  0.000000  0.000000 -0.000000  0.000000 -0.000000  0.000000  0.000000   \n",
       "28  0.000000  0.000000 -0.000000 -0.000000 -0.000000 -0.000000 -0.000000   \n",
       "29 -0.000000  0.000000 -0.000000 -0.000000 -0.000000  0.000000 -0.000000   \n",
       "30  0.000000  0.000000 -0.000000  0.000000 -0.000000  0.000000 -0.000000   \n",
       "31  0.000000  0.000000 -0.000000  0.000000  0.000000  0.000000 -0.000000   \n",
       "32  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000 -0.000000   \n",
       "33 -0.000000 -0.000000 -0.000000 -0.000000 -0.000000 -0.000000  0.000000   \n",
       "34  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000 -0.000000   \n",
       "35  0.000000  0.000000 -0.000000 -0.000000 -0.000000  0.000000 -0.000000   \n",
       "36 -0.000000 -0.000000  0.000000 -0.000000 -0.000000  0.000000  0.000000   \n",
       "37  0.000000 -0.000000 -0.000000 -0.000000  0.000000  0.000000  0.000000   \n",
       "38 -0.000000  0.000000  0.000000  0.000000 -0.000000  0.000000  0.000000   \n",
       "39 -0.000000 -0.000000 -0.000000  0.000000  0.000000 -0.000000  0.000000   \n",
       "40 -0.000000 -0.000000  0.000000 -0.000000  0.000000  0.000000  0.000000   \n",
       "41 -0.000000 -0.000000 -0.000000  0.000000 -0.000000 -0.000000 -0.000000   \n",
       "\n",
       "          7         8         9   ...        32        33        34        35  \\\n",
       "0  -0.197865  0.035695  0.023018  ...  0.119543  0.346680  0.278890  0.244760   \n",
       "1  -0.213966  0.049680  0.013963  ...  0.150266 -0.079707  0.029973 -0.090913   \n",
       "2  -0.011465 -0.007674 -0.008132  ...  0.036865  0.008204  0.010181 -0.023712   \n",
       "3   0.085330 -0.039875  0.063165  ...  0.114232  0.002575  0.054535 -0.058629   \n",
       "4  -0.116239  0.079771 -0.038731  ...  0.007448  0.017506  0.047645  0.012699   \n",
       "5   0.188865 -0.030733  0.098193  ... -0.006589 -0.015460 -0.039864  0.003249   \n",
       "6   0.097982  0.021891  0.022950  ...  0.097675 -0.006527  0.045668 -0.077805   \n",
       "7  -0.161650  0.029595  0.084262  ... -0.063484 -0.023360 -0.059391  0.054830   \n",
       "8  -0.056403  0.003441 -0.101263  ... -0.025059  0.005399  0.001950  0.048329   \n",
       "9  -0.020290  0.006903  0.002364  ...  0.031929 -0.046597  0.074956 -0.052603   \n",
       "10 -0.071278 -0.035308 -0.046794  ... -0.012037 -0.022469  0.038756 -0.045935   \n",
       "11  0.030235 -0.007596  0.008972  ... -0.035707 -0.008063  0.018858 -0.017527   \n",
       "12 -0.017890  0.001577 -0.015128  ... -0.003412 -0.007432 -0.001030 -0.030409   \n",
       "13 -0.003368  0.006814  0.004881  ... -0.003617 -0.004876  0.007067 -0.003982   \n",
       "14  0.000000 -0.000000  0.000000  ... -0.000000  0.000000 -0.000000  0.000000   \n",
       "15 -0.000000  0.000000 -0.000000  ...  0.000000  0.000000 -0.000000 -0.000000   \n",
       "16  0.000000 -0.000000 -0.000000  ...  0.000000  0.000000 -0.000000 -0.000000   \n",
       "17 -0.000000  0.000000  0.000000  ...  0.000000  0.000000 -0.000000 -0.000000   \n",
       "18 -0.000000  0.000000  0.000000  ... -0.000000 -0.000000 -0.000000 -0.000000   \n",
       "19 -0.000000  0.000000 -0.000000  ...  0.000000  0.000000  0.000000 -0.000000   \n",
       "20  0.000000 -0.000000  0.000000  ...  0.000000  0.000000 -0.000000  0.000000   \n",
       "21  0.000000 -0.000000 -0.000000  ... -0.000000  0.000000 -0.000000  0.000000   \n",
       "22 -0.000000 -0.000000  0.000000  ...  0.000000 -0.000000  0.000000 -0.000000   \n",
       "23  0.000000  0.000000  0.000000  ... -0.000000  0.000000  0.000000  0.000000   \n",
       "24 -0.000000 -0.000000  0.000000  ... -0.000000  0.000000  0.000000  0.000000   \n",
       "25 -0.000000  0.000000  0.000000  ... -0.000000  0.000000  0.000000 -0.000000   \n",
       "26  0.000000 -0.000000  0.000000  ...  0.000000 -0.000000  0.000000 -0.000000   \n",
       "27 -0.000000  0.000000  0.000000  ... -0.000000  0.000000 -0.000000 -0.000000   \n",
       "28 -0.000000 -0.000000 -0.000000  ... -0.000000 -0.000000 -0.000000  0.000000   \n",
       "29  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000 -0.000000   \n",
       "30 -0.000000  0.000000  0.000000  ...  0.000000  0.000000 -0.000000  0.000000   \n",
       "31  0.000000  0.000000 -0.000000  ... -0.000000  0.000000 -0.000000 -0.000000   \n",
       "32  0.000000 -0.000000 -0.000000  ...  0.000000  0.000000 -0.000000  0.000000   \n",
       "33 -0.000000 -0.000000  0.000000  ...  0.000000  0.000000  0.000000 -0.000000   \n",
       "34  0.000000  0.000000 -0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "35  0.000000  0.000000  0.000000  ...  0.000000 -0.000000 -0.000000  0.000000   \n",
       "36 -0.000000 -0.000000  0.000000  ... -0.000000  0.000000 -0.000000  0.000000   \n",
       "37  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000 -0.000000   \n",
       "38  0.000000 -0.000000 -0.000000  ...  0.000000  0.000000 -0.000000  0.000000   \n",
       "39  0.000000  0.000000 -0.000000  ...  0.000000  0.000000  0.000000 -0.000000   \n",
       "40  0.000000  0.000000 -0.000000  ... -0.000000  0.000000  0.000000 -0.000000   \n",
       "41  0.000000 -0.000000  0.000000  ... -0.000000 -0.000000  0.000000  0.000000   \n",
       "\n",
       "          36        37        38        39        40        41  \n",
       "0   0.289871  0.207581  0.223799  0.117088  0.139196 -0.017376  \n",
       "1   0.062688 -0.197779 -0.219705 -0.060826  0.158575 -0.064839  \n",
       "2   0.028237 -0.031855 -0.005872  0.004513  1.197419  0.024751  \n",
       "3   0.129016 -0.108544 -0.054319  0.025066 -0.462774  0.017957  \n",
       "4   0.001334  0.012242  0.012426 -0.018585 -0.256412 -0.054098  \n",
       "5  -0.037561  0.024469  0.011687 -0.069697  0.035507 -0.123216  \n",
       "6   0.048215 -0.066110 -0.055975 -0.007699  0.105934  0.029315  \n",
       "7  -0.014986  0.037235  0.036042 -0.007677  0.042849 -0.120660  \n",
       "8   0.005396 -0.011255  0.037787  0.014929 -0.005276 -0.004662  \n",
       "9  -0.010232 -0.038071 -0.034174  0.014077  0.057784 -0.026218  \n",
       "10 -0.016719 -0.025898  0.007257 -0.007529 -0.031917 -0.043810  \n",
       "11  0.014747 -0.009656 -0.027054  0.020109 -0.000358  0.046934  \n",
       "12  0.038745  0.014448  0.011051 -0.072301  0.004397 -0.045736  \n",
       "13  0.008950 -0.008295  0.000326  0.015575 -0.001673 -0.015750  \n",
       "14 -0.000000  0.000000 -0.000000 -0.000000 -0.000000  0.000000  \n",
       "15 -0.000000 -0.000000 -0.000000  0.000000 -0.000000 -0.000000  \n",
       "16  0.000000 -0.000000 -0.000000  0.000000  0.000000 -0.000000  \n",
       "17  0.000000 -0.000000 -0.000000 -0.000000 -0.000000 -0.000000  \n",
       "18  0.000000 -0.000000  0.000000 -0.000000 -0.000000  0.000000  \n",
       "19 -0.000000 -0.000000  0.000000 -0.000000 -0.000000  0.000000  \n",
       "20  0.000000 -0.000000  0.000000 -0.000000 -0.000000  0.000000  \n",
       "21  0.000000 -0.000000  0.000000 -0.000000  0.000000  0.000000  \n",
       "22 -0.000000  0.000000  0.000000  0.000000 -0.000000  0.000000  \n",
       "23 -0.000000 -0.000000  0.000000 -0.000000  0.000000 -0.000000  \n",
       "24  0.000000 -0.000000 -0.000000 -0.000000 -0.000000 -0.000000  \n",
       "25  0.000000 -0.000000 -0.000000  0.000000  0.000000  0.000000  \n",
       "26 -0.000000 -0.000000  0.000000 -0.000000 -0.000000 -0.000000  \n",
       "27 -0.000000 -0.000000 -0.000000 -0.000000 -0.000000  0.000000  \n",
       "28  0.000000 -0.000000 -0.000000  0.000000 -0.000000  0.000000  \n",
       "29  0.000000 -0.000000  0.000000 -0.000000  0.000000  0.000000  \n",
       "30 -0.000000  0.000000 -0.000000 -0.000000 -0.000000  0.000000  \n",
       "31 -0.000000 -0.000000  0.000000 -0.000000  0.000000 -0.000000  \n",
       "32 -0.000000 -0.000000  0.000000 -0.000000 -0.000000  0.000000  \n",
       "33 -0.000000 -0.000000 -0.000000 -0.000000 -0.000000  0.000000  \n",
       "34  0.000000  0.000000 -0.000000 -0.000000 -0.000000 -0.000000  \n",
       "35  0.000000 -0.000000  0.000000 -0.000000  0.000000  0.000000  \n",
       "36  0.000000  0.000000 -0.000000 -0.000000  0.000000  0.000000  \n",
       "37 -0.000000  0.000000  0.000000  0.000000  0.000000 -0.000000  \n",
       "38  0.000000  0.000000 -0.000000 -0.000000 -0.000000  0.000000  \n",
       "39 -0.000000  0.000000 -0.000000  0.000000 -0.000000  0.000000  \n",
       "40 -0.000000 -0.000000 -0.000000 -0.000000  0.000000 -0.000000  \n",
       "41  0.000000  0.000000 -0.000000  0.000000  0.000000 -0.000000  \n",
       "\n",
       "[42 rows x 42 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factore = FactorAnalysis().fit(X_train_imp)\n",
    "pd.DataFrame(factore.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65248305\n",
      "Iteration 2, loss = 0.60631126\n",
      "Iteration 3, loss = 0.56062488\n",
      "Iteration 4, loss = 0.54649652\n",
      "Iteration 5, loss = 0.53261418\n",
      "Iteration 6, loss = 0.51267103\n",
      "Iteration 7, loss = 0.51056811\n",
      "Iteration 8, loss = 0.49573525\n",
      "Iteration 9, loss = 0.49083525\n",
      "Iteration 10, loss = 0.47870955\n",
      "Iteration 11, loss = 0.47149653\n",
      "Iteration 12, loss = 0.46531734\n",
      "Iteration 13, loss = 0.45160268\n",
      "Iteration 14, loss = 0.44734334\n",
      "Iteration 15, loss = 0.43663668\n",
      "Iteration 16, loss = 0.42927804\n",
      "Iteration 17, loss = 0.42069367\n",
      "Iteration 18, loss = 0.41270979\n",
      "Iteration 19, loss = 0.40554774\n",
      "Iteration 20, loss = 0.39966331\n",
      "Iteration 21, loss = 0.39048309\n",
      "Iteration 22, loss = 0.38294880\n",
      "Iteration 23, loss = 0.37684216\n",
      "Iteration 24, loss = 0.36751361\n",
      "Iteration 25, loss = 0.36185842\n",
      "Iteration 26, loss = 0.35353531\n",
      "Iteration 27, loss = 0.34610053\n",
      "Iteration 28, loss = 0.34017974\n",
      "Iteration 29, loss = 0.33225471\n",
      "Iteration 30, loss = 0.32731819\n",
      "Iteration 31, loss = 0.31835854\n",
      "Iteration 32, loss = 0.31439979\n",
      "Iteration 33, loss = 0.30822298\n",
      "Iteration 34, loss = 0.30813561\n",
      "Iteration 35, loss = 0.29521286\n",
      "Iteration 36, loss = 0.29436623\n",
      "Iteration 37, loss = 0.28479519\n",
      "Iteration 38, loss = 0.28004889\n",
      "Iteration 39, loss = 0.27495587\n",
      "Iteration 40, loss = 0.27445540\n",
      "Iteration 41, loss = 0.26357560\n",
      "Iteration 42, loss = 0.26123688\n",
      "Iteration 43, loss = 0.25723800\n",
      "Iteration 44, loss = 0.25027174\n",
      "Iteration 45, loss = 0.24767958\n",
      "Iteration 46, loss = 0.24317297\n",
      "Iteration 47, loss = 0.23993264\n",
      "Iteration 48, loss = 0.24256750\n",
      "Iteration 49, loss = 0.23718805\n",
      "Iteration 50, loss = 0.23851370\n",
      "Iteration 51, loss = 0.22940443\n",
      "Iteration 52, loss = 0.22440342\n",
      "Iteration 53, loss = 0.22276774\n",
      "Iteration 54, loss = 0.21686355\n",
      "Iteration 55, loss = 0.21234280\n",
      "Iteration 56, loss = 0.20940367\n",
      "Iteration 57, loss = 0.20702066\n",
      "Iteration 58, loss = 0.20376760\n",
      "Iteration 59, loss = 0.20446298\n",
      "Iteration 60, loss = 0.19862105\n",
      "Iteration 61, loss = 0.19387339\n",
      "Iteration 62, loss = 0.19387927\n",
      "Iteration 63, loss = 0.19125316\n",
      "Iteration 64, loss = 0.18770975\n",
      "Iteration 65, loss = 0.18687187\n",
      "Iteration 66, loss = 0.18901164\n",
      "Iteration 67, loss = 0.18093977\n",
      "Iteration 68, loss = 0.18225254\n",
      "Iteration 69, loss = 0.17538384\n",
      "Iteration 70, loss = 0.17464845\n",
      "Iteration 71, loss = 0.17335973\n",
      "Iteration 72, loss = 0.16716482\n",
      "Iteration 73, loss = 0.16378759\n",
      "Iteration 74, loss = 0.15845421\n",
      "Iteration 75, loss = 0.15772205\n",
      "Iteration 76, loss = 0.15652429\n",
      "Iteration 77, loss = 0.15295740\n",
      "Iteration 78, loss = 0.14957095\n",
      "Iteration 79, loss = 0.14920096\n",
      "Iteration 80, loss = 0.14949923\n",
      "Iteration 81, loss = 0.15075708\n",
      "Iteration 82, loss = 0.14021470\n",
      "Iteration 83, loss = 0.13551704\n",
      "Iteration 84, loss = 0.13805726\n",
      "Iteration 85, loss = 0.13472266\n",
      "Iteration 86, loss = 0.13354695\n",
      "Iteration 87, loss = 0.12805743\n",
      "Iteration 88, loss = 0.12986533\n",
      "Iteration 89, loss = 0.12608356\n",
      "Iteration 90, loss = 0.12120053\n",
      "Iteration 91, loss = 0.11964882\n",
      "Iteration 92, loss = 0.12012288\n",
      "Iteration 93, loss = 0.11972946\n",
      "Iteration 94, loss = 0.11165500\n",
      "Iteration 95, loss = 0.11136387\n",
      "Iteration 96, loss = 0.10405140\n",
      "Iteration 97, loss = 0.10944796\n",
      "Iteration 98, loss = 0.09915852\n",
      "Iteration 99, loss = 0.10082208\n",
      "Iteration 100, loss = 0.10302402\n",
      "Iteration 101, loss = 0.10147534\n",
      "Iteration 102, loss = 0.09639594\n",
      "Iteration 103, loss = 0.08922361\n",
      "Iteration 104, loss = 0.08710875\n",
      "Iteration 105, loss = 0.08595211\n",
      "Iteration 106, loss = 0.08492015\n",
      "Iteration 107, loss = 0.08197103\n",
      "Iteration 108, loss = 0.08014181\n",
      "Iteration 109, loss = 0.07816044\n",
      "Iteration 110, loss = 0.07877935\n",
      "Iteration 111, loss = 0.07285552\n",
      "Iteration 112, loss = 0.06848055\n",
      "Iteration 113, loss = 0.06996226\n",
      "Iteration 114, loss = 0.06644886\n",
      "Iteration 115, loss = 0.06397176\n",
      "Iteration 116, loss = 0.06276622\n",
      "Iteration 117, loss = 0.06222634\n",
      "Iteration 118, loss = 0.06190815\n",
      "Iteration 119, loss = 0.06012254\n",
      "Iteration 120, loss = 0.05948286\n",
      "Iteration 121, loss = 0.05808775\n",
      "Iteration 122, loss = 0.05393399\n",
      "Iteration 123, loss = 0.05088836\n",
      "Iteration 124, loss = 0.04951692\n",
      "Iteration 125, loss = 0.04766120\n",
      "Iteration 126, loss = 0.04655634\n",
      "Iteration 127, loss = 0.04665824\n",
      "Iteration 128, loss = 0.04679934\n",
      "Iteration 129, loss = 0.04304137\n",
      "Iteration 130, loss = 0.04228968\n",
      "Iteration 131, loss = 0.04012712\n",
      "Iteration 132, loss = 0.04261389\n",
      "Iteration 133, loss = 0.04132452\n",
      "Iteration 134, loss = 0.04354535\n",
      "Iteration 135, loss = 0.04108408\n",
      "Iteration 136, loss = 0.03533599\n",
      "Iteration 137, loss = 0.03870311\n",
      "Iteration 138, loss = 0.03932846\n",
      "Iteration 139, loss = 0.03920204\n",
      "Iteration 140, loss = 0.03401628\n",
      "Iteration 141, loss = 0.03397062\n",
      "Iteration 142, loss = 0.03215065\n",
      "Iteration 143, loss = 0.03625939\n",
      "Iteration 144, loss = 0.03129184\n",
      "Iteration 145, loss = 0.02866289\n",
      "Iteration 146, loss = 0.02635554\n",
      "Iteration 147, loss = 0.02698237\n",
      "Iteration 148, loss = 0.02489468\n",
      "Iteration 149, loss = 0.02447554\n",
      "Iteration 150, loss = 0.02648118\n",
      "Iteration 151, loss = 0.02278785\n",
      "Iteration 152, loss = 0.02207949\n",
      "Iteration 153, loss = 0.02272043\n",
      "Iteration 154, loss = 0.02218702\n",
      "Iteration 155, loss = 0.02076778\n",
      "Iteration 156, loss = 0.02207566\n",
      "Iteration 157, loss = 0.02295003\n",
      "Iteration 158, loss = 0.02021659\n",
      "Iteration 159, loss = 0.01944663\n",
      "Iteration 160, loss = 0.01881401\n",
      "Iteration 161, loss = 0.01911856\n",
      "Iteration 162, loss = 0.01685646\n",
      "Iteration 163, loss = 0.01680217\n",
      "Iteration 164, loss = 0.01480483\n",
      "Iteration 165, loss = 0.01570866\n",
      "Iteration 166, loss = 0.01556421\n",
      "Iteration 167, loss = 0.01433970\n",
      "Iteration 168, loss = 0.01355784\n",
      "Iteration 169, loss = 0.01378271\n",
      "Iteration 170, loss = 0.01313776\n",
      "Iteration 171, loss = 0.01223244\n",
      "Iteration 172, loss = 0.01183457\n",
      "Iteration 173, loss = 0.01150926\n",
      "Iteration 174, loss = 0.01120129\n",
      "Iteration 175, loss = 0.01117384\n",
      "Iteration 176, loss = 0.01108688\n",
      "Iteration 177, loss = 0.01071249\n",
      "Iteration 178, loss = 0.01047425\n",
      "Iteration 179, loss = 0.01020953\n",
      "Iteration 180, loss = 0.01002091\n",
      "Iteration 181, loss = 0.00986012\n",
      "Iteration 182, loss = 0.00927278\n",
      "Iteration 183, loss = 0.00904150\n",
      "Iteration 184, loss = 0.00876350\n",
      "Iteration 185, loss = 0.00892681\n",
      "Iteration 186, loss = 0.00935778\n",
      "Iteration 187, loss = 0.00875352\n",
      "Iteration 188, loss = 0.00836873\n",
      "Iteration 189, loss = 0.00857959\n",
      "Iteration 190, loss = 0.00795511\n",
      "Iteration 191, loss = 0.00749636\n",
      "Iteration 192, loss = 0.00782656\n",
      "Iteration 193, loss = 0.00722295\n",
      "Iteration 194, loss = 0.00709267\n",
      "Iteration 195, loss = 0.00705070\n",
      "Iteration 196, loss = 0.00696094\n",
      "Iteration 197, loss = 0.00652968\n",
      "Iteration 198, loss = 0.00700362\n",
      "Iteration 199, loss = 0.00646275\n",
      "Iteration 200, loss = 0.00623513\n",
      "Iteration 201, loss = 0.00605636\n",
      "Iteration 202, loss = 0.00597042\n",
      "Iteration 203, loss = 0.00576008\n",
      "Iteration 204, loss = 0.00570406\n",
      "Iteration 205, loss = 0.00568224\n",
      "Iteration 206, loss = 0.00571765\n",
      "Iteration 207, loss = 0.00555158\n",
      "Iteration 208, loss = 0.00532523\n",
      "Iteration 209, loss = 0.00558160\n",
      "Iteration 210, loss = 0.00557049\n",
      "Iteration 211, loss = 0.00529822\n",
      "Iteration 212, loss = 0.00511837\n",
      "Iteration 213, loss = 0.00483390\n",
      "Iteration 214, loss = 0.00473965\n",
      "Iteration 215, loss = 0.00462869\n",
      "Iteration 216, loss = 0.00450954\n",
      "Iteration 217, loss = 0.00456670\n",
      "Iteration 218, loss = 0.00439205\n",
      "Iteration 219, loss = 0.00442252\n",
      "Iteration 220, loss = 0.00428047\n",
      "Iteration 221, loss = 0.00414946\n",
      "Iteration 222, loss = 0.00410364\n",
      "Iteration 223, loss = 0.00401892\n",
      "Iteration 224, loss = 0.00399083\n",
      "Iteration 225, loss = 0.00393843\n",
      "Iteration 226, loss = 0.00387297\n",
      "Iteration 227, loss = 0.00379644\n",
      "Iteration 228, loss = 0.00374183\n",
      "Iteration 229, loss = 0.00374470\n",
      "Iteration 230, loss = 0.00359801\n",
      "Iteration 231, loss = 0.00357237\n",
      "Iteration 232, loss = 0.00349283\n",
      "Iteration 233, loss = 0.00346319\n",
      "Iteration 234, loss = 0.00339524\n",
      "Iteration 235, loss = 0.00334326\n",
      "Iteration 236, loss = 0.00330607\n",
      "Iteration 237, loss = 0.00328600\n",
      "Iteration 238, loss = 0.00319536\n",
      "Iteration 239, loss = 0.00319013\n",
      "Iteration 240, loss = 0.00309067\n",
      "Iteration 241, loss = 0.00310323\n",
      "Iteration 242, loss = 0.00302185\n",
      "Iteration 243, loss = 0.00299951\n",
      "Iteration 244, loss = 0.00292668\n",
      "Iteration 245, loss = 0.00292237\n",
      "Iteration 246, loss = 0.00286856\n",
      "Iteration 247, loss = 0.00279787\n",
      "Iteration 248, loss = 0.00283881\n",
      "Iteration 249, loss = 0.00273872\n",
      "Iteration 250, loss = 0.00271669\n",
      "Iteration 251, loss = 0.00266732\n",
      "Iteration 252, loss = 0.00267466\n",
      "Iteration 253, loss = 0.00276280\n",
      "Iteration 254, loss = 0.00267509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 255, loss = 0.00260732\n",
      "Iteration 256, loss = 0.00252923\n",
      "Iteration 257, loss = 0.00251550\n",
      "Iteration 258, loss = 0.00256233\n",
      "Iteration 259, loss = 0.00247125\n",
      "Iteration 260, loss = 0.00244887\n",
      "Iteration 261, loss = 0.00242107\n",
      "Iteration 262, loss = 0.00228627\n",
      "Iteration 263, loss = 0.00236164\n",
      "Iteration 264, loss = 0.00225837\n",
      "Iteration 265, loss = 0.00227998\n",
      "Iteration 266, loss = 0.00222907\n",
      "Iteration 267, loss = 0.00223565\n",
      "Iteration 268, loss = 0.00216258\n",
      "Iteration 269, loss = 0.00217774\n",
      "Iteration 270, loss = 0.00209951\n",
      "Iteration 271, loss = 0.00210247\n",
      "Iteration 272, loss = 0.00204979\n",
      "Iteration 273, loss = 0.00204574\n",
      "Iteration 274, loss = 0.00203973\n",
      "Iteration 275, loss = 0.00200398\n",
      "Iteration 276, loss = 0.00200595\n",
      "Iteration 277, loss = 0.00194703\n",
      "Iteration 278, loss = 0.00195356\n",
      "Iteration 279, loss = 0.00194344\n",
      "Iteration 280, loss = 0.00189400\n",
      "Iteration 281, loss = 0.00185884\n",
      "Iteration 282, loss = 0.00182436\n",
      "Iteration 283, loss = 0.00181723\n",
      "Iteration 284, loss = 0.00179490\n",
      "Iteration 285, loss = 0.00179037\n",
      "Iteration 286, loss = 0.00179240\n",
      "Iteration 287, loss = 0.00174220\n",
      "Iteration 288, loss = 0.00175071\n",
      "Iteration 289, loss = 0.00170904\n",
      "Iteration 290, loss = 0.00170132\n",
      "Iteration 291, loss = 0.00166290\n",
      "Iteration 292, loss = 0.00166052\n",
      "Iteration 293, loss = 0.00164305\n",
      "Iteration 294, loss = 0.00161960\n",
      "Iteration 295, loss = 0.00160320\n",
      "Iteration 296, loss = 0.00158352\n",
      "Iteration 297, loss = 0.00157420\n",
      "Iteration 298, loss = 0.00154602\n",
      "Iteration 299, loss = 0.00154978\n",
      "Iteration 300, loss = 0.00152612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joe/.local/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train_imp, y_train)\n",
    "y_pred = clf.predict(X_test_imp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8425531914893617\n",
      "[[ 55  23]\n",
      " [ 14 143]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.71      0.75        78\n",
      "        True       0.86      0.91      0.89       157\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       235\n",
      "   macro avg       0.83      0.81      0.82       235\n",
      "weighted avg       0.84      0.84      0.84       235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/joe/MLP/features_sample_head1.csv\")\n",
    "df = df.astype({'Class_P0': 'bool', \n",
    "                'head0_AU28': 'bool',\n",
    "                'head0_AU45': 'bool',\n",
    "                'head0_AU26': 'bool',\n",
    "                'head0_AU25': 'bool',\n",
    "                'head0_AU01': 'bool',\n",
    "                'head0_AU05': 'bool',\n",
    "                'head0_AU20': 'bool',\n",
    "                'head0_AU06': 'bool',\n",
    "                'head0_AU23': 'bool',\n",
    "                'head0_AU07': 'bool',\n",
    "                'head0_AU02': 'bool', \n",
    "                'head0_AU09': 'bool',\n",
    "                'head0_AU17': 'bool',\n",
    "                'head0_AU10': 'bool',\n",
    "                'head0_AU04': 'bool',\n",
    "                'head0_AU12': 'bool',\n",
    "                'head0_AU14': 'bool',\n",
    "                'head0_AU15': 'bool',\n",
    "                'ClassP1':'bool'})\n",
    "\n",
    "le = LabelEncoder()\n",
    "# df['head0_pose'] = le.fit_transform(df['head0_pose'])\n",
    "df['head0_pose'] = le.fit_transform(df['head0_pose'])\n",
    "# df['head2_pose'] = le.fit_transform(df['head2_pose'])\n",
    "# df['head0_emo'] = le.fit_transform(df['head0_emo'])\n",
    "df['head0_emo'] = le.fit_transform(df['head0_emo'])\n",
    "# df['head2_emo'] = le.fit_transform(df['head2_emo'])\n",
    "\n",
    "y = df['ClassP1']\n",
    "x = df.drop(['ClassP1'], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size= 0.30, random_state=27)\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imp = imp.fit(x_train)\n",
    "\n",
    "X_train_imp = imp.transform(x_train)\n",
    "X_test_imp = imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=300, alpha=0.0001, activation='tanh',\n",
    "                     solver='adam', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfsl = sfs(clf, k_features=10,forward=True)\n",
    "# print(sfsl.fit(X_test_imp, y_test))\n",
    "# print(sfsl.subsets_)\n",
    "# ('0', '7', '14', '16', '18', '20', '22', '27', '28', '32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64385078\n",
      "Iteration 2, loss = 0.55884261\n",
      "Iteration 3, loss = 0.53296318\n",
      "Iteration 4, loss = 0.48760836\n",
      "Iteration 5, loss = 0.48454543\n",
      "Iteration 6, loss = 0.46044767\n",
      "Iteration 7, loss = 0.44323377\n",
      "Iteration 8, loss = 0.43024875\n",
      "Iteration 9, loss = 0.40665050\n",
      "Iteration 10, loss = 0.39852028\n",
      "Iteration 11, loss = 0.38321475\n",
      "Iteration 12, loss = 0.37294857\n",
      "Iteration 13, loss = 0.36278577\n",
      "Iteration 14, loss = 0.35322136\n",
      "Iteration 15, loss = 0.34465623\n",
      "Iteration 16, loss = 0.34054942\n",
      "Iteration 17, loss = 0.33063967\n",
      "Iteration 18, loss = 0.32986518\n",
      "Iteration 19, loss = 0.31841655\n",
      "Iteration 20, loss = 0.31900384\n",
      "Iteration 21, loss = 0.31187754\n",
      "Iteration 22, loss = 0.30786121\n",
      "Iteration 23, loss = 0.30636648\n",
      "Iteration 24, loss = 0.29866940\n",
      "Iteration 25, loss = 0.29442986\n",
      "Iteration 26, loss = 0.28949793\n",
      "Iteration 27, loss = 0.28494474\n",
      "Iteration 28, loss = 0.27989482\n",
      "Iteration 29, loss = 0.27602405\n",
      "Iteration 30, loss = 0.27139262\n",
      "Iteration 31, loss = 0.26757130\n",
      "Iteration 32, loss = 0.26217055\n",
      "Iteration 33, loss = 0.25807127\n",
      "Iteration 34, loss = 0.25401731\n",
      "Iteration 35, loss = 0.24895466\n",
      "Iteration 36, loss = 0.24555165\n",
      "Iteration 37, loss = 0.24065818\n",
      "Iteration 38, loss = 0.23841568\n",
      "Iteration 39, loss = 0.23197266\n",
      "Iteration 40, loss = 0.22924767\n",
      "Iteration 41, loss = 0.22340716\n",
      "Iteration 42, loss = 0.21929546\n",
      "Iteration 43, loss = 0.21797219\n",
      "Iteration 44, loss = 0.21140220\n",
      "Iteration 45, loss = 0.20750475\n",
      "Iteration 46, loss = 0.20422667\n",
      "Iteration 47, loss = 0.20087003\n",
      "Iteration 48, loss = 0.19854150\n",
      "Iteration 49, loss = 0.19203955\n",
      "Iteration 50, loss = 0.19031989\n",
      "Iteration 51, loss = 0.18378337\n",
      "Iteration 52, loss = 0.18103353\n",
      "Iteration 53, loss = 0.17807396\n",
      "Iteration 54, loss = 0.17225627\n",
      "Iteration 55, loss = 0.17274755\n",
      "Iteration 56, loss = 0.17339234\n",
      "Iteration 57, loss = 0.17156186\n",
      "Iteration 58, loss = 0.16470208\n",
      "Iteration 59, loss = 0.15773815\n",
      "Iteration 60, loss = 0.16111756\n",
      "Iteration 61, loss = 0.16442350\n",
      "Iteration 62, loss = 0.15647254\n",
      "Iteration 63, loss = 0.14649469\n",
      "Iteration 64, loss = 0.14496652\n",
      "Iteration 65, loss = 0.14024514\n",
      "Iteration 66, loss = 0.13120052\n",
      "Iteration 67, loss = 0.13180217\n",
      "Iteration 68, loss = 0.12615045\n",
      "Iteration 69, loss = 0.12324768\n",
      "Iteration 70, loss = 0.12322784\n",
      "Iteration 71, loss = 0.11671226\n",
      "Iteration 72, loss = 0.11394592\n",
      "Iteration 73, loss = 0.11279555\n",
      "Iteration 74, loss = 0.11084649\n",
      "Iteration 75, loss = 0.10561180\n",
      "Iteration 76, loss = 0.10598854\n",
      "Iteration 77, loss = 0.09955002\n",
      "Iteration 78, loss = 0.09904618\n",
      "Iteration 79, loss = 0.09446521\n",
      "Iteration 80, loss = 0.09004340\n",
      "Iteration 81, loss = 0.09067672\n",
      "Iteration 82, loss = 0.08567262\n",
      "Iteration 83, loss = 0.08181564\n",
      "Iteration 84, loss = 0.08014119\n",
      "Iteration 85, loss = 0.07690149\n",
      "Iteration 86, loss = 0.07423156\n",
      "Iteration 87, loss = 0.07189189\n",
      "Iteration 88, loss = 0.07088003\n",
      "Iteration 89, loss = 0.06779734\n",
      "Iteration 90, loss = 0.06464661\n",
      "Iteration 91, loss = 0.06345052\n",
      "Iteration 92, loss = 0.06254710\n",
      "Iteration 93, loss = 0.06173496\n",
      "Iteration 94, loss = 0.05832812\n",
      "Iteration 95, loss = 0.05437991\n",
      "Iteration 96, loss = 0.05354904\n",
      "Iteration 97, loss = 0.04957619\n",
      "Iteration 98, loss = 0.04871069\n",
      "Iteration 99, loss = 0.05127144\n",
      "Iteration 100, loss = 0.05167326\n",
      "Iteration 101, loss = 0.04715854\n",
      "Iteration 102, loss = 0.04381193\n",
      "Iteration 103, loss = 0.04204637\n",
      "Iteration 104, loss = 0.04249057\n",
      "Iteration 105, loss = 0.04129565\n",
      "Iteration 106, loss = 0.04488193\n",
      "Iteration 107, loss = 0.04266494\n",
      "Iteration 108, loss = 0.03815284\n",
      "Iteration 109, loss = 0.03478288\n",
      "Iteration 110, loss = 0.03259539\n",
      "Iteration 111, loss = 0.03480725\n",
      "Iteration 112, loss = 0.03323759\n",
      "Iteration 113, loss = 0.03245452\n",
      "Iteration 114, loss = 0.02892705\n",
      "Iteration 115, loss = 0.02859928\n",
      "Iteration 116, loss = 0.02616297\n",
      "Iteration 117, loss = 0.02574041\n",
      "Iteration 118, loss = 0.02378929\n",
      "Iteration 119, loss = 0.02251608\n",
      "Iteration 120, loss = 0.02167514\n",
      "Iteration 121, loss = 0.02133869\n",
      "Iteration 122, loss = 0.02193978\n",
      "Iteration 123, loss = 0.02142621\n",
      "Iteration 124, loss = 0.01982741\n",
      "Iteration 125, loss = 0.01781867\n",
      "Iteration 126, loss = 0.01735583\n",
      "Iteration 127, loss = 0.01542327\n",
      "Iteration 128, loss = 0.01562079\n",
      "Iteration 129, loss = 0.01521920\n",
      "Iteration 130, loss = 0.01487806\n",
      "Iteration 131, loss = 0.01406001\n",
      "Iteration 132, loss = 0.01353193\n",
      "Iteration 133, loss = 0.01258968\n",
      "Iteration 134, loss = 0.01244146\n",
      "Iteration 135, loss = 0.01222458\n",
      "Iteration 136, loss = 0.01222505\n",
      "Iteration 137, loss = 0.01138329\n",
      "Iteration 138, loss = 0.01108487\n",
      "Iteration 139, loss = 0.01034498\n",
      "Iteration 140, loss = 0.01015040\n",
      "Iteration 141, loss = 0.00983843\n",
      "Iteration 142, loss = 0.00956779\n",
      "Iteration 143, loss = 0.00927332\n",
      "Iteration 144, loss = 0.00917157\n",
      "Iteration 145, loss = 0.00887378\n",
      "Iteration 146, loss = 0.00859327\n",
      "Iteration 147, loss = 0.00822527\n",
      "Iteration 148, loss = 0.00799660\n",
      "Iteration 149, loss = 0.00783462\n",
      "Iteration 150, loss = 0.00759572\n",
      "Iteration 151, loss = 0.00742426\n",
      "Iteration 152, loss = 0.00722852\n",
      "Iteration 153, loss = 0.00720866\n",
      "Iteration 154, loss = 0.00684962\n",
      "Iteration 155, loss = 0.00675461\n",
      "Iteration 156, loss = 0.00649910\n",
      "Iteration 157, loss = 0.00639823\n",
      "Iteration 158, loss = 0.00625782\n",
      "Iteration 159, loss = 0.00612112\n",
      "Iteration 160, loss = 0.00602703\n",
      "Iteration 161, loss = 0.00589631\n",
      "Iteration 162, loss = 0.00567659\n",
      "Iteration 163, loss = 0.00580775\n",
      "Iteration 164, loss = 0.00543450\n",
      "Iteration 165, loss = 0.00543563\n",
      "Iteration 166, loss = 0.00516307\n",
      "Iteration 167, loss = 0.00507139\n",
      "Iteration 168, loss = 0.00496138\n",
      "Iteration 169, loss = 0.00491099\n",
      "Iteration 170, loss = 0.00472334\n",
      "Iteration 171, loss = 0.00463136\n",
      "Iteration 172, loss = 0.00449413\n",
      "Iteration 173, loss = 0.00440229\n",
      "Iteration 174, loss = 0.00431075\n",
      "Iteration 175, loss = 0.00423380\n",
      "Iteration 176, loss = 0.00414466\n",
      "Iteration 177, loss = 0.00405316\n",
      "Iteration 178, loss = 0.00400800\n",
      "Iteration 179, loss = 0.00390253\n",
      "Iteration 180, loss = 0.00382043\n",
      "Iteration 181, loss = 0.00380700\n",
      "Iteration 182, loss = 0.00376045\n",
      "Iteration 183, loss = 0.00364487\n",
      "Iteration 184, loss = 0.00355076\n",
      "Iteration 185, loss = 0.00352397\n",
      "Iteration 186, loss = 0.00356052\n",
      "Iteration 187, loss = 0.00340921\n",
      "Iteration 188, loss = 0.00333459\n",
      "Iteration 189, loss = 0.00325864\n",
      "Iteration 190, loss = 0.00319718\n",
      "Iteration 191, loss = 0.00314034\n",
      "Iteration 192, loss = 0.00309874\n",
      "Iteration 193, loss = 0.00302535\n",
      "Iteration 194, loss = 0.00296567\n",
      "Iteration 195, loss = 0.00295467\n",
      "Iteration 196, loss = 0.00287649\n",
      "Iteration 197, loss = 0.00283833\n",
      "Iteration 198, loss = 0.00280790\n",
      "Iteration 199, loss = 0.00274522\n",
      "Iteration 200, loss = 0.00271885\n",
      "Iteration 201, loss = 0.00265903\n",
      "Iteration 202, loss = 0.00262841\n",
      "Iteration 203, loss = 0.00257357\n",
      "Iteration 204, loss = 0.00257288\n",
      "Iteration 205, loss = 0.00250442\n",
      "Iteration 206, loss = 0.00247516\n",
      "Iteration 207, loss = 0.00247475\n",
      "Iteration 208, loss = 0.00238347\n",
      "Iteration 209, loss = 0.00236714\n",
      "Iteration 210, loss = 0.00231128\n",
      "Iteration 211, loss = 0.00229090\n",
      "Iteration 212, loss = 0.00227885\n",
      "Iteration 213, loss = 0.00223942\n",
      "Iteration 214, loss = 0.00218905\n",
      "Iteration 215, loss = 0.00215220\n",
      "Iteration 216, loss = 0.00212497\n",
      "Iteration 217, loss = 0.00209322\n",
      "Iteration 218, loss = 0.00206724\n",
      "Iteration 219, loss = 0.00204208\n",
      "Iteration 220, loss = 0.00200923\n",
      "Iteration 221, loss = 0.00199972\n",
      "Iteration 222, loss = 0.00196917\n",
      "Iteration 223, loss = 0.00194098\n",
      "Iteration 224, loss = 0.00190676\n",
      "Iteration 225, loss = 0.00189124\n",
      "Iteration 226, loss = 0.00186727\n",
      "Iteration 227, loss = 0.00184647\n",
      "Iteration 228, loss = 0.00181882\n",
      "Iteration 229, loss = 0.00179944\n",
      "Iteration 230, loss = 0.00177377\n",
      "Iteration 231, loss = 0.00174279\n",
      "Iteration 232, loss = 0.00173109\n",
      "Iteration 233, loss = 0.00170829\n",
      "Iteration 234, loss = 0.00169425\n",
      "Iteration 235, loss = 0.00166939\n",
      "Iteration 236, loss = 0.00163679\n",
      "Iteration 237, loss = 0.00162838\n",
      "Iteration 238, loss = 0.00160787\n",
      "Iteration 239, loss = 0.00158185\n",
      "Iteration 240, loss = 0.00157517\n",
      "Iteration 241, loss = 0.00155593\n",
      "Iteration 242, loss = 0.00153461\n",
      "Iteration 243, loss = 0.00151754\n",
      "Iteration 244, loss = 0.00150094\n",
      "Iteration 245, loss = 0.00148071\n",
      "Iteration 246, loss = 0.00146558\n",
      "Iteration 247, loss = 0.00145570\n",
      "Iteration 248, loss = 0.00143444\n",
      "Iteration 249, loss = 0.00142004\n",
      "Iteration 250, loss = 0.00141557\n",
      "Iteration 251, loss = 0.00139232\n",
      "Iteration 252, loss = 0.00137394\n",
      "Iteration 253, loss = 0.00136227\n",
      "Iteration 254, loss = 0.00135005\n",
      "Iteration 255, loss = 0.00133430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, loss = 0.00131841\n",
      "Iteration 257, loss = 0.00131222\n",
      "Iteration 258, loss = 0.00128319\n",
      "Iteration 259, loss = 0.00127663\n",
      "Iteration 260, loss = 0.00127537\n",
      "Iteration 261, loss = 0.00124706\n",
      "Iteration 262, loss = 0.00123629\n",
      "Iteration 263, loss = 0.00122712\n",
      "Iteration 264, loss = 0.00121070\n",
      "Iteration 265, loss = 0.00120792\n",
      "Iteration 266, loss = 0.00119162\n",
      "Iteration 267, loss = 0.00118137\n",
      "Iteration 268, loss = 0.00116687\n",
      "Iteration 269, loss = 0.00115268\n",
      "Iteration 270, loss = 0.00114394\n",
      "Iteration 271, loss = 0.00113402\n",
      "Iteration 272, loss = 0.00112201\n",
      "Iteration 273, loss = 0.00111124\n",
      "Iteration 274, loss = 0.00110268\n",
      "Iteration 275, loss = 0.00109211\n",
      "Iteration 276, loss = 0.00108292\n",
      "Iteration 277, loss = 0.00107170\n",
      "Iteration 278, loss = 0.00106689\n",
      "Iteration 279, loss = 0.00105735\n",
      "Iteration 280, loss = 0.00104204\n",
      "Iteration 281, loss = 0.00103247\n",
      "Iteration 282, loss = 0.00102573\n",
      "Iteration 283, loss = 0.00101452\n",
      "Iteration 284, loss = 0.00100574\n",
      "Iteration 285, loss = 0.00099815\n",
      "Iteration 286, loss = 0.00099057\n",
      "Iteration 287, loss = 0.00098142\n",
      "Iteration 288, loss = 0.00097239\n",
      "Iteration 289, loss = 0.00096300\n",
      "Iteration 290, loss = 0.00095523\n",
      "Iteration 291, loss = 0.00094973\n",
      "Iteration 292, loss = 0.00093969\n",
      "Iteration 293, loss = 0.00093327\n",
      "Iteration 294, loss = 0.00092450\n",
      "Iteration 295, loss = 0.00091673\n",
      "Iteration 296, loss = 0.00090846\n",
      "Iteration 297, loss = 0.00090049\n",
      "Iteration 298, loss = 0.00089511\n",
      "Iteration 299, loss = 0.00088758\n",
      "Iteration 300, loss = 0.00088471\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train_imp, y_train)\n",
    "y_pred = clf.predict(X_test_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8127659574468085\n",
      "[[ 41  22]\n",
      " [ 22 150]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.65      0.65      0.65        63\n",
      "        True       0.87      0.87      0.87       172\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       235\n",
      "   macro avg       0.76      0.76      0.76       235\n",
      "weighted avg       0.81      0.81      0.81       235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/joe/MLP/features_sample_head2.csv\")\n",
    "df = df.astype({'Class_P0': 'bool', \n",
    "                'head0_AU28': 'bool',\n",
    "                'head0_AU45': 'bool',\n",
    "                'head0_AU26': 'bool',\n",
    "                'head0_AU25': 'bool',\n",
    "                'head0_AU01': 'bool',\n",
    "                'head0_AU05': 'bool',\n",
    "                'head0_AU20': 'bool',\n",
    "                'head0_AU06': 'bool',\n",
    "                'head0_AU23': 'bool',\n",
    "                'head0_AU07': 'bool',\n",
    "                'head0_AU02': 'bool', \n",
    "                'head0_AU09': 'bool',\n",
    "                'head0_AU17': 'bool',\n",
    "                'head0_AU10': 'bool',\n",
    "                'head0_AU04': 'bool',\n",
    "                'head0_AU12': 'bool',\n",
    "                'head0_AU14': 'bool',\n",
    "                'head0_AU15': 'bool',\n",
    "                'ClassP2':'bool'})\n",
    "le = LabelEncoder()\n",
    "# df['head0_pose'] = le.fit_transform(df['head0_pose'])\n",
    "df['head0_pose'] = le.fit_transform(df['head0_pose'])\n",
    "# df['head2_pose'] = le.fit_transform(df['head2_pose'])\n",
    "# df['head0_emo'] = le.fit_transform(df['head0_emo'])\n",
    "df['head0_emo'] = le.fit_transform(df['head0_emo'])\n",
    "# df['head2_emo'] = le.fit_transform(df['head2_emo'])\n",
    "\n",
    "y = df['ClassP2']\n",
    "x = df.drop(['ClassP2'], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size= 0.30, random_state=27)\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imp = imp.fit(x_train)\n",
    "\n",
    "X_train_imp = imp.transform(x_train)\n",
    "X_test_imp = imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=300, alpha=0.0001, activation='tanh',\n",
    "                     solver='adam', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfsl = sfs(clf, k_features=10,forward=True)\n",
    "# print(sfsl.fit(X_test_imp, y_test))\n",
    "# print(sfsl.subsets_)\n",
    "# feature_names': ('0', '1', '4', '6', '7', '9', '11', '14', '17', '24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68182416\n",
      "Iteration 2, loss = 0.65145412\n",
      "Iteration 3, loss = 0.60671259\n",
      "Iteration 4, loss = 0.59271150\n",
      "Iteration 5, loss = 0.56944706\n",
      "Iteration 6, loss = 0.55421862\n",
      "Iteration 7, loss = 0.53561078\n",
      "Iteration 8, loss = 0.52308774\n",
      "Iteration 9, loss = 0.51188750\n",
      "Iteration 10, loss = 0.50063329\n",
      "Iteration 11, loss = 0.49132970\n",
      "Iteration 12, loss = 0.48579162\n",
      "Iteration 13, loss = 0.47897710\n",
      "Iteration 14, loss = 0.47067063\n",
      "Iteration 15, loss = 0.46392396\n",
      "Iteration 16, loss = 0.45749690\n",
      "Iteration 17, loss = 0.45037560\n",
      "Iteration 18, loss = 0.44579289\n",
      "Iteration 19, loss = 0.43856665\n",
      "Iteration 20, loss = 0.43569633\n",
      "Iteration 21, loss = 0.42545287\n",
      "Iteration 22, loss = 0.42277612\n",
      "Iteration 23, loss = 0.41614972\n",
      "Iteration 24, loss = 0.40921340\n",
      "Iteration 25, loss = 0.40212157\n",
      "Iteration 26, loss = 0.39772553\n",
      "Iteration 27, loss = 0.39075436\n",
      "Iteration 28, loss = 0.38378375\n",
      "Iteration 29, loss = 0.38112326\n",
      "Iteration 30, loss = 0.37535939\n",
      "Iteration 31, loss = 0.36972052\n",
      "Iteration 32, loss = 0.36574270\n",
      "Iteration 33, loss = 0.35210121\n",
      "Iteration 34, loss = 0.34952124\n",
      "Iteration 35, loss = 0.34101313\n",
      "Iteration 36, loss = 0.33601534\n",
      "Iteration 37, loss = 0.32653352\n",
      "Iteration 38, loss = 0.32398772\n",
      "Iteration 39, loss = 0.32311075\n",
      "Iteration 40, loss = 0.31247449\n",
      "Iteration 41, loss = 0.30327736\n",
      "Iteration 42, loss = 0.30714397\n",
      "Iteration 43, loss = 0.29763219\n",
      "Iteration 44, loss = 0.29124967\n",
      "Iteration 45, loss = 0.28089035\n",
      "Iteration 46, loss = 0.27838376\n",
      "Iteration 47, loss = 0.26707281\n",
      "Iteration 48, loss = 0.26951033\n",
      "Iteration 49, loss = 0.26012143\n",
      "Iteration 50, loss = 0.25084893\n",
      "Iteration 51, loss = 0.24928862\n",
      "Iteration 52, loss = 0.24495163\n",
      "Iteration 53, loss = 0.23781143\n",
      "Iteration 54, loss = 0.23142361\n",
      "Iteration 55, loss = 0.23097159\n",
      "Iteration 56, loss = 0.22882748\n",
      "Iteration 57, loss = 0.21722419\n",
      "Iteration 58, loss = 0.21578562\n",
      "Iteration 59, loss = 0.21324053\n",
      "Iteration 60, loss = 0.20423099\n",
      "Iteration 61, loss = 0.19757977\n",
      "Iteration 62, loss = 0.19653155\n",
      "Iteration 63, loss = 0.18935702\n",
      "Iteration 64, loss = 0.18682011\n",
      "Iteration 65, loss = 0.18275332\n",
      "Iteration 66, loss = 0.17888491\n",
      "Iteration 67, loss = 0.17582394\n",
      "Iteration 68, loss = 0.16990112\n",
      "Iteration 69, loss = 0.16648246\n",
      "Iteration 70, loss = 0.16309335\n",
      "Iteration 71, loss = 0.16575340\n",
      "Iteration 72, loss = 0.16249887\n",
      "Iteration 73, loss = 0.15751951\n",
      "Iteration 74, loss = 0.14835419\n",
      "Iteration 75, loss = 0.15323157\n",
      "Iteration 76, loss = 0.14584598\n",
      "Iteration 77, loss = 0.13850170\n",
      "Iteration 78, loss = 0.13547917\n",
      "Iteration 79, loss = 0.13374946\n",
      "Iteration 80, loss = 0.12927197\n",
      "Iteration 81, loss = 0.12506047\n",
      "Iteration 82, loss = 0.12307339\n",
      "Iteration 83, loss = 0.12205450\n",
      "Iteration 84, loss = 0.13207281\n",
      "Iteration 85, loss = 0.12068036\n",
      "Iteration 86, loss = 0.11418117\n",
      "Iteration 87, loss = 0.10851872\n",
      "Iteration 88, loss = 0.10518005\n",
      "Iteration 89, loss = 0.10599875\n",
      "Iteration 90, loss = 0.09899543\n",
      "Iteration 91, loss = 0.09835629\n",
      "Iteration 92, loss = 0.10051892\n",
      "Iteration 93, loss = 0.09795466\n",
      "Iteration 94, loss = 0.09130483\n",
      "Iteration 95, loss = 0.08502014\n",
      "Iteration 96, loss = 0.08650238\n",
      "Iteration 97, loss = 0.08238903\n",
      "Iteration 98, loss = 0.07817986\n",
      "Iteration 99, loss = 0.07809176\n",
      "Iteration 100, loss = 0.07510425\n",
      "Iteration 101, loss = 0.07221268\n",
      "Iteration 102, loss = 0.07065107\n",
      "Iteration 103, loss = 0.07025987\n",
      "Iteration 104, loss = 0.06873718\n",
      "Iteration 105, loss = 0.07022337\n",
      "Iteration 106, loss = 0.06735779\n",
      "Iteration 107, loss = 0.06221170\n",
      "Iteration 108, loss = 0.05674099\n",
      "Iteration 109, loss = 0.05877539\n",
      "Iteration 110, loss = 0.05658243\n",
      "Iteration 111, loss = 0.05387306\n",
      "Iteration 112, loss = 0.05252397\n",
      "Iteration 113, loss = 0.04915532\n",
      "Iteration 114, loss = 0.04939388\n",
      "Iteration 115, loss = 0.04746963\n",
      "Iteration 116, loss = 0.04671578\n",
      "Iteration 117, loss = 0.04387107\n",
      "Iteration 118, loss = 0.04194072\n",
      "Iteration 119, loss = 0.04111503\n",
      "Iteration 120, loss = 0.04007883\n",
      "Iteration 121, loss = 0.04307201\n",
      "Iteration 122, loss = 0.05130689\n",
      "Iteration 123, loss = 0.05880813\n",
      "Iteration 124, loss = 0.03990408\n",
      "Iteration 125, loss = 0.03685593\n",
      "Iteration 126, loss = 0.03838552\n",
      "Iteration 127, loss = 0.04137335\n",
      "Iteration 128, loss = 0.04416593\n",
      "Iteration 129, loss = 0.03394991\n",
      "Iteration 130, loss = 0.03266088\n",
      "Iteration 131, loss = 0.03438244\n",
      "Iteration 132, loss = 0.03028062\n",
      "Iteration 133, loss = 0.02874240\n",
      "Iteration 134, loss = 0.02747186\n",
      "Iteration 135, loss = 0.02518390\n",
      "Iteration 136, loss = 0.02529800\n",
      "Iteration 137, loss = 0.02717305\n",
      "Iteration 138, loss = 0.02391999\n",
      "Iteration 139, loss = 0.02204175\n",
      "Iteration 140, loss = 0.02390481\n",
      "Iteration 141, loss = 0.02240420\n",
      "Iteration 142, loss = 0.02044401\n",
      "Iteration 143, loss = 0.02048542\n",
      "Iteration 144, loss = 0.02115261\n",
      "Iteration 145, loss = 0.01976393\n",
      "Iteration 146, loss = 0.01967262\n",
      "Iteration 147, loss = 0.01814347\n",
      "Iteration 148, loss = 0.01789953\n",
      "Iteration 149, loss = 0.01737289\n",
      "Iteration 150, loss = 0.01631108\n",
      "Iteration 151, loss = 0.01675532\n",
      "Iteration 152, loss = 0.01599940\n",
      "Iteration 153, loss = 0.01582637\n",
      "Iteration 154, loss = 0.01445414\n",
      "Iteration 155, loss = 0.01415484\n",
      "Iteration 156, loss = 0.01387533\n",
      "Iteration 157, loss = 0.01339516\n",
      "Iteration 158, loss = 0.01311593\n",
      "Iteration 159, loss = 0.01265945\n",
      "Iteration 160, loss = 0.01243851\n",
      "Iteration 161, loss = 0.01313279\n",
      "Iteration 162, loss = 0.01207217\n",
      "Iteration 163, loss = 0.01192005\n",
      "Iteration 164, loss = 0.01163575\n",
      "Iteration 165, loss = 0.01082123\n",
      "Iteration 166, loss = 0.01095452\n",
      "Iteration 167, loss = 0.01146849\n",
      "Iteration 168, loss = 0.01039934\n",
      "Iteration 169, loss = 0.01044932\n",
      "Iteration 170, loss = 0.01027364\n",
      "Iteration 171, loss = 0.00957978\n",
      "Iteration 172, loss = 0.01001898\n",
      "Iteration 173, loss = 0.00908598\n",
      "Iteration 174, loss = 0.00891368\n",
      "Iteration 175, loss = 0.00864743\n",
      "Iteration 176, loss = 0.00851770\n",
      "Iteration 177, loss = 0.00820360\n",
      "Iteration 178, loss = 0.00830654\n",
      "Iteration 179, loss = 0.00803869\n",
      "Iteration 180, loss = 0.00778974\n",
      "Iteration 181, loss = 0.00778439\n",
      "Iteration 182, loss = 0.00761224\n",
      "Iteration 183, loss = 0.00733011\n",
      "Iteration 184, loss = 0.00716249\n",
      "Iteration 185, loss = 0.00697375\n",
      "Iteration 186, loss = 0.00701226\n",
      "Iteration 187, loss = 0.00701690\n",
      "Iteration 188, loss = 0.00675231\n",
      "Iteration 189, loss = 0.00661014\n",
      "Iteration 190, loss = 0.00644256\n",
      "Iteration 191, loss = 0.00614067\n",
      "Iteration 192, loss = 0.00616161\n",
      "Iteration 193, loss = 0.00600346\n",
      "Iteration 194, loss = 0.00582439\n",
      "Iteration 195, loss = 0.00593730\n",
      "Iteration 196, loss = 0.00572548\n",
      "Iteration 197, loss = 0.00564203\n",
      "Iteration 198, loss = 0.00552991\n",
      "Iteration 199, loss = 0.00539585\n",
      "Iteration 200, loss = 0.00538832\n",
      "Iteration 201, loss = 0.00521956\n",
      "Iteration 202, loss = 0.00522345\n",
      "Iteration 203, loss = 0.00505929\n",
      "Iteration 204, loss = 0.00490633\n",
      "Iteration 205, loss = 0.00480713\n",
      "Iteration 206, loss = 0.00471650\n",
      "Iteration 207, loss = 0.00470889\n",
      "Iteration 208, loss = 0.00449498\n",
      "Iteration 209, loss = 0.00457280\n",
      "Iteration 210, loss = 0.00438104\n",
      "Iteration 211, loss = 0.00437251\n",
      "Iteration 212, loss = 0.00436243\n",
      "Iteration 213, loss = 0.00419167\n",
      "Iteration 214, loss = 0.00418237\n",
      "Iteration 215, loss = 0.00408967\n",
      "Iteration 216, loss = 0.00398868\n",
      "Iteration 217, loss = 0.00392910\n",
      "Iteration 218, loss = 0.00387881\n",
      "Iteration 219, loss = 0.00378500\n",
      "Iteration 220, loss = 0.00386605\n",
      "Iteration 221, loss = 0.00381245\n",
      "Iteration 222, loss = 0.00367275\n",
      "Iteration 223, loss = 0.00356978\n",
      "Iteration 224, loss = 0.00355644\n",
      "Iteration 225, loss = 0.00346094\n",
      "Iteration 226, loss = 0.00340269\n",
      "Iteration 227, loss = 0.00334066\n",
      "Iteration 228, loss = 0.00332416\n",
      "Iteration 229, loss = 0.00329608\n",
      "Iteration 230, loss = 0.00325959\n",
      "Iteration 231, loss = 0.00322886\n",
      "Iteration 232, loss = 0.00317670\n",
      "Iteration 233, loss = 0.00313082\n",
      "Iteration 234, loss = 0.00305786\n",
      "Iteration 235, loss = 0.00300151\n",
      "Iteration 236, loss = 0.00296975\n",
      "Iteration 237, loss = 0.00291599\n",
      "Iteration 238, loss = 0.00288652\n",
      "Iteration 239, loss = 0.00282944\n",
      "Iteration 240, loss = 0.00280974\n",
      "Iteration 241, loss = 0.00277651\n",
      "Iteration 242, loss = 0.00272845\n",
      "Iteration 243, loss = 0.00269262\n",
      "Iteration 244, loss = 0.00265357\n",
      "Iteration 245, loss = 0.00266986\n",
      "Iteration 246, loss = 0.00260141\n",
      "Iteration 247, loss = 0.00255747\n",
      "Iteration 248, loss = 0.00254700\n",
      "Iteration 249, loss = 0.00251318\n",
      "Iteration 250, loss = 0.00248426\n",
      "Iteration 251, loss = 0.00244706\n",
      "Iteration 252, loss = 0.00248032\n",
      "Iteration 253, loss = 0.00240621\n",
      "Iteration 254, loss = 0.00236804\n",
      "Iteration 255, loss = 0.00232181\n",
      "Iteration 256, loss = 0.00230968\n",
      "Iteration 257, loss = 0.00229336\n",
      "Iteration 258, loss = 0.00224400\n",
      "Iteration 259, loss = 0.00222314\n",
      "Iteration 260, loss = 0.00219840\n",
      "Iteration 261, loss = 0.00216687\n",
      "Iteration 262, loss = 0.00214559\n",
      "Iteration 263, loss = 0.00213083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 264, loss = 0.00210959\n",
      "Iteration 265, loss = 0.00211523\n",
      "Iteration 266, loss = 0.00205065\n",
      "Iteration 267, loss = 0.00208681\n",
      "Iteration 268, loss = 0.00199731\n",
      "Iteration 269, loss = 0.00199677\n",
      "Iteration 270, loss = 0.00197078\n",
      "Iteration 271, loss = 0.00194874\n",
      "Iteration 272, loss = 0.00193892\n",
      "Iteration 273, loss = 0.00189870\n",
      "Iteration 274, loss = 0.00188137\n",
      "Iteration 275, loss = 0.00186349\n",
      "Iteration 276, loss = 0.00184303\n",
      "Iteration 277, loss = 0.00182359\n",
      "Iteration 278, loss = 0.00181965\n",
      "Iteration 279, loss = 0.00179886\n",
      "Iteration 280, loss = 0.00177167\n",
      "Iteration 281, loss = 0.00174766\n",
      "Iteration 282, loss = 0.00172677\n",
      "Iteration 283, loss = 0.00172219\n",
      "Iteration 284, loss = 0.00170923\n",
      "Iteration 285, loss = 0.00170179\n",
      "Iteration 286, loss = 0.00166525\n",
      "Iteration 287, loss = 0.00163994\n",
      "Iteration 288, loss = 0.00164642\n",
      "Iteration 289, loss = 0.00162927\n",
      "Iteration 290, loss = 0.00160171\n",
      "Iteration 291, loss = 0.00159108\n",
      "Iteration 292, loss = 0.00157507\n",
      "Iteration 293, loss = 0.00157433\n",
      "Iteration 294, loss = 0.00154901\n",
      "Iteration 295, loss = 0.00152514\n",
      "Iteration 296, loss = 0.00151804\n",
      "Iteration 297, loss = 0.00150088\n",
      "Iteration 298, loss = 0.00149347\n",
      "Iteration 299, loss = 0.00146325\n",
      "Iteration 300, loss = 0.00146567\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train_imp, y_train)\n",
    "y_pred = clf.predict(X_test_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8170212765957446\n",
      "[[ 59  23]\n",
      " [ 20 133]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.75      0.72      0.73        82\n",
      "        True       0.85      0.87      0.86       153\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       235\n",
      "   macro avg       0.80      0.79      0.80       235\n",
      "weighted avg       0.82      0.82      0.82       235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
